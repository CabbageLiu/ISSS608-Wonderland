[
  {
    "objectID": "MC1/MC1_InClass.html",
    "href": "MC1/MC1_InClass.html",
    "title": "MC1",
    "section": "",
    "text": "pacman::p_load(tidyverse, jsonlite, SmartEDA, tidygraph, ggraph)",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "MC1/MC1_InClass.html#import",
    "href": "MC1/MC1_InClass.html#import",
    "title": "MC1",
    "section": "Import",
    "text": "Import\n\nkg &lt;- fromJSON(\"data/MC1_release/MC1_graph.json\")",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "MC1/MC1_InClass.html#inspect-structure",
    "href": "MC1/MC1_InClass.html#inspect-structure",
    "title": "MC1",
    "section": "Inspect Structure",
    "text": "Inspect Structure\n\nstr(kg, max.level=1)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     :List of 2\n $ nodes     :'data.frame': 17412 obs. of  10 variables:\n $ links     :'data.frame': 37857 obs. of  4 variables:",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "MC1/MC1_InClass.html#extract-and-inspect",
    "href": "MC1/MC1_InClass.html#extract-and-inspect",
    "title": "MC1",
    "section": "Extract and Inspect",
    "text": "Extract and Inspect\nthis is like extracting part of the data\nwe used to have to go into kg -&gt; nodes -&gt; table, now we can just click on edges_tbl\n\nnodes_tbl &lt;- as_tibble(kg$nodes)\nedges_tbl &lt;- as_tibble(kg$links)",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "MC1/MC1_InClass.html#initial-eda",
    "href": "MC1/MC1_InClass.html#initial-eda",
    "title": "MC1",
    "section": "Initial EDA",
    "text": "Initial EDA\n\nggplot(data = edges_tbl,\n       aes(y = `Edge Type`)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(data = nodes_tbl,\n       aes(y = `Node Type`)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "MC1/MC1_InClass.html#creating-knowledge-graph",
    "href": "MC1/MC1_InClass.html#creating-knowledge-graph",
    "title": "MC1",
    "section": "Creating knowledge graph",
    "text": "Creating knowledge graph\n\nStep 1. Mapping from node id to row index\n\nid_map &lt;- tibble(id = nodes_tbl$id,\n                 index = seq_len(\n                   nrow(nodes_tbl)))\n\nThis ensures each id from your node list is mapped to the correct row number\n\n\nStep 2. Map source and target IDs to row indices\n\nedges_tbl &lt;- edges_tbl %&gt;%\n  left_join(id_map, by = c(\"source\" = \"id\")) %&gt;%\n  rename(from = index) %&gt;%\n  left_join(id_map, by = c(\"target\" = \"id\")) %&gt;%\n  rename(to = index)\n\n\n\nStep 3. Filter out any unmatched (invalid) edges\n\nedges_tbl &lt;- edges_tbl %&gt;%\n  filter(!is.na(from), !is.na(to))\n\n\n\nStep 4. Creating the graph\nLastly, tbl_graph() is used to create tidygraph graph object by using the code chunk below\nwe can also tweak directed as True\n\ngraph &lt;- tbl_graph(nodes = nodes_tbl,\n                   edges = edges_tbl,\n                   directed = kg$directed)",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "MC1/MC1_InClass.html#visualizing-the-knowledge-graph",
    "href": "MC1/MC1_InClass.html#visualizing-the-knowledge-graph",
    "title": "MC1",
    "section": "Visualizing the knowledge graph",
    "text": "Visualizing the knowledge graph\n\nset.seed(1234)\n\n\n2.1 Visualizing the whole graph\n\nggraph(graph, layout = \"fr\") +\n  geom_edge_link(alpha = 0.3,\n                 colour = \"gray\") +\n  geom_node_point(aes(color = `Node Type`),\n                  size = 4) +\n  geom_node_text(aes(label = name),\n                 repel = TRUE,\n                 size = 2.5) +\n  theme_void()\n\nDoesn’t really show meanings, so we move on to subgraph\n\n\n2.2 Step 1 Filter edges to only “Memberof”\n\ngraph_memberof &lt;- graph %&gt;%\n  activate(edges) %&gt;%\n  filter(`Edge Type` == \"Memberof\")\n\n\n\n2.2 Step 2 Extract only connected nodes (eg. used in these edges)\n\nused_node_indicies &lt;- graph_memberof %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  select(from, to) %&gt;%\n  unlist() %&gt;%\n  unique()\n\n\n\n2.3 Step 3 Keep only the nodes\n\ngraph_memberof &lt;- graph_memberof %&gt;%\n  activate(nodes) %&gt;%\n  mutate(row_id = row_number()) %&gt;%\n  filter(row_id %in% used_node_indicies) %&gt;%\n  select(-row_id)\n\n\n\n2.4 Plot sub-graph\n\nggraph(graph_memberof,\n       layout = \"fr\") +\n  geom_edge_link(alpha = 0.5,\n                 colour = \"gray\") +\n  geom_node_point(aes(color = `Node Type`),\n                  size = 1) +\n  geom_node_text(aes(label = name),\n                 repel = TRUE,\n                 size = 2.25) +\n  theme_void()",
    "crumbs": [
      "Home",
      "In Class Exercise",
      "Week 5 In Class Exercise"
    ]
  },
  {
    "objectID": "testing_qmd/testing.html",
    "href": "testing_qmd/testing.html",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "",
    "text": "This take home exercise is based on the VAST Challenge Mini Case 3\nOver the past decade, the community of Oceanus has faced numerous transformations and challenges evolving from its fishing-centric origins. Following major crackdowns on illegal fishing activities, suspects have shifted investments into more regulated sectors such as the ocean tourism industry, resulting in growing tensions. This increased tourism has recently attracted the likes of international pop star Sailor Shift, who announced plans to film a music video on the island.\nClepper Jessen, a former analyst at FishEye and now a seasoned journalist for the Hacklee Herald, has been keenly observing these rising tensions. Recently, he turned his attention towards the temporary closure of Nemo Reef. By listening to radio communications and utilizing his investigative tools, Clepper uncovered a complex web of expedited approvals and secretive logistics. These efforts revealed a story involving high-level Oceanus officials, Sailor Shift’s team, local influential families, and local conservationist group The Green Guardians, pointing towards a story of corruption and manipulation.\nYour task is to develop new and novel visualizations and visual analytics approaches to help Clepper get to the bottom of this story"
  },
  {
    "objectID": "testing_qmd/testing.html#initial-eda",
    "href": "testing_qmd/testing.html#initial-eda",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "2.1 Initial EDA",
    "text": "2.1 Initial EDA\n\n\nShow code\nExpCatViz(data=mc3_nodes,\n          col=\"pink\")\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]"
  },
  {
    "objectID": "testing_qmd/testing.html#relationship-between-entities-and-events",
    "href": "testing_qmd/testing.html#relationship-between-entities-and-events",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.1 Relationship between entities and events",
    "text": "5.1 Relationship between entities and events\n\n\nShow code\nggraph(mc3_graph, \n       layout = \"fr\") +\n  geom_edge_link(alpha = 0.3, \n                 colour = \"gray\") +\n  geom_node_point(aes(color = `type`), \n                  size = 2) +\n  geom_node_text(aes(label = type), \n                 repel = TRUE, \n                 size = 2.5) +\n  theme_void()"
  },
  {
    "objectID": "testing_qmd/testing.html#entity-distribution",
    "href": "testing_qmd/testing.html#entity-distribution",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.2 Entity distribution",
    "text": "5.2 Entity distribution\n\n\nShow code\n# Define color mapping\nsubtype_colors &lt;- c(\n  \"Person\" = \"#2ca5ff\",\n  \"Organization\" = \"#f5ee15\",\n  \"Vessel\" = \"#FB7E81\",\n  \"Group\" = \"#25e158\",\n  \"Location\" = \"#ec4bff\"\n)\n\nmc3_nodes_final %&gt;%\n  filter(type == \"Entity\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Entity Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "testing_qmd/testing.html#event-type-distribution",
    "href": "testing_qmd/testing.html#event-type-distribution",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.3 Event type distribution",
    "text": "5.3 Event type distribution\n\n\nShow code\nmc3_nodes_final %&gt;%\n  filter(type == \"Event\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Event Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "testing_qmd/testing.html#list-of-communication-participants",
    "href": "testing_qmd/testing.html#list-of-communication-participants",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.4 List of communication participants",
    "text": "5.4 List of communication participants\n\n\nShow code\nlibrary(DT)\n\n# Step 1: Get all Communication Event IDs\ncomm_event_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  pull(id)\n\n# Step 2: Extract 'sent' edges for communication events\ncomm_sent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\", to_id %in% comm_event_ids) %&gt;%\n  select(comm_id = to_id, sender_id = from_id)\n\n# Step 3: Extract 'received' edges for same communication events\ncomm_received_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\", from_id %in% comm_event_ids) %&gt;%\n  select(comm_id = from_id, receiver_id = to_id)\n\n# Step 4: Join sent and received edges by communication ID\ncomm_pairs &lt;- comm_sent_edges %&gt;%\n  inner_join(comm_received_edges, by = \"comm_id\")\n\n# Step 5: Add sender and receiver labels\nparticipants_named &lt;- comm_pairs %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n\n\n# Step7: Interactive summary of top sender–receiver pairs\nparticipants_named %&gt;%\n  count(sender_label, receiver_label, sort = TRUE) %&gt;%\n  datatable(\n    caption = \"Top Communication Pairs (Sender → Receiver)\",\n    colnames = c(\"Sender\", \"Receiver\", \"Message Count\"),\n    options = list(pageLength = 10, autoWidth = TRUE),\n    rownames = FALSE\n  )"
  },
  {
    "objectID": "testing_qmd/testing.html#visualization-of-communication-participants-network",
    "href": "testing_qmd/testing.html#visualization-of-communication-participants-network",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.4.1 Visualization of communication participants network",
    "text": "5.4.1 Visualization of communication participants network\nThis code creates an interactive communication network graph using visNetwork, where:\n\nEach node represents a person or entity, node size is based on total messages sent by that participant.\nEach edge (arrow) represents a communication sent from one participant to another, the thicker the edge, the more message sent to that particular receiver.\n\nVer 1: Layout_in_circle\n\n\nShow code\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per node\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes, merge with message count and add color/shape\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    sent_count = replace_na(sent_count, 0),\n    size = rescale(sent_count, to = c(10, 40)),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    ),\n  ) %&gt;%\n  arrange(desc(size))\n\n# Step 4: Format visNetwork edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Define legend items\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render network with legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"1000px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape are picked up from nodes_vis columns automatically\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_in_circle\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\nVer 2: Layout_on_sphere\nFrom this plot, it reveals that some pairs (e.g., Miranda Jordan and Clepper Jensen) mainly communicate with each other, suggesting isolated or private channels outside the broader network.\n\n\nShow code\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per person (by sender)\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes with label, subtype, color, shape, and scaled size\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    size = if_else(\n      sub_type == \"Person\",\n      rescale(sent_count, to = c(10, 40), na.rm = TRUE),\n      15\n    ),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Step 4: Format edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Legend mapping\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render the network with layout_on_sphere and legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"900px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape columns are automatically used\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_on_sphere\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)"
  },
  {
    "objectID": "testing_qmd/testing.html#vast-challenge-task-question-1a-and-1b",
    "href": "testing_qmd/testing.html#vast-challenge-task-question-1a-and-1b",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "VAST Challenge Task & Question 1a and 1b",
    "text": "VAST Challenge Task & Question 1a and 1b\nClepper found that messages frequently came in at around the same time each day.\n\nDevelop a graph-based visual analytics approach to identify any daily temporal patterns in communications.\nHow do these patterns shift over the two weeks of observations?\n\nObjective\n\nIdentify when communications happen most often during each day.\nDetect shifts in these patterns over the 2-week period.\nLater: Focus on a specific entity (e.g., Nadia Conti) and explore who influences them.\n\n\nStep 1: Extract & Parse Communication Event Timestamps\nExtract the Communication Timestamps from mc3_nodes_final and filter for communication events.\n\n\nShow code\n# Filter for Communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    day = as.Date(timestamp),\n    hour = hour(timestamp)\n  )\n\n\nParse the Communication Timestamp into the format “dd/mm/yyy (ddd)” for ease of reference.\n\n\nShow code\n# Communication events with parsed date and time\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")  # e.g., \"19/03/2040 (Tue)\"\n  )\n\n\n\n\nStep 2: Visualize the Communication Volume for Analysis\n\n4.1 - Bar Plot of daily communication volume over the 2 weeks period:\n\n\nShow code\n# Step 1: Prepare daily message volume data\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date)\n\n# Step 2: Compute average and total message count\navg_msg_count &lt;- mean(daily_message_volume$message_count)\ntotal_msg_count &lt;- sum(daily_message_volume$message_count)\n\n# Step 3: Plot bar chart with average + total labels\nggplot(daily_message_volume, aes(x = date_label, y = message_count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(\n    aes(label = message_count),\n    vjust = -0.3,\n    size = 2.5,\n    color = \"grey40\"\n  ) +\n  geom_hline(yintercept = avg_msg_count, color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\n    \"label\", x = 1, y = avg_msg_count + 2,\n    label = paste(\"Average =\", round(avg_msg_count, 1)),\n    color = \"red\", fill = \"grey90\",\n    label.size = 0, hjust = -0.2, vjust = 3\n  ) +\n  annotate(\n    \"label\", x = nrow(daily_message_volume), y = max(daily_message_volume$message_count) + 5,\n    label = paste(\"Total =\", total_msg_count),\n    color = \"black\", fill = \"lightgrey\",\n    label.size = 0.3, hjust = 1.1, vjust = 1\n  ) +\n  labs(\n    title = \"Daily Radio Communication Volume\",\n    x = \"Date\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n4.2 - Interactive Table of daily communication volume variation(message count)\n\n\nShow code\nlibrary(DT)\n\n# Daily message volume with comparisons\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    change_from_prev = message_count - lag(message_count),\n    pct_change_from_prev = round((message_count - lag(message_count)) / lag(message_count) * 100, 2)\n  )\n\ndatatable(\n  daily_message_volume %&gt;% select(-date),  # remove raw date if not needed\n  caption = \"Daily Message Volume with Day-over-Day Change\",\n  options = list(pageLength = 14, order = list(list(0, 'asc'))),\n  rownames = FALSE\n)\n\n\n\n\n\n\n\n\n4.3a - Heat Map of hourly message volume for each day over the 2 weeks period:\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count\n\n\nShow code\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Reconstruct sender–receiver–timestamp structure\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp),\n         hour = hour(timestamp),\n         date_label = format(timestamp, \"%d/%m/%Y (%a)\"))\n\n# Step 2: Get sender (sent) and receiver (received) links\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)\n\n# Step 3: Join all together into sender–receiver–timestamp\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(sender_id = id, sender_label = label), by = \"sender_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(receiver_id = id, receiver_label = label), by = \"receiver_id\")\n\n# Step 4: Aggregate total messages per hour/day\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, hour) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"📅 Date: \", date_label,\n      \"&lt;br&gt;⏰ Hour: \", sprintf(\"%02d:00\", hour),\n      \"&lt;br&gt;📨 Messages: \", count,\n      \"&lt;br&gt;🔴 Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;🟢 Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: Static ggplot\np &lt;- ggplot(comm_heatmap, aes(\n  x = hour,\n  y = fct_rev(factor(date_label)),\n  fill = count,\n  text = tooltip\n)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(option = \"inferno\", direction = -1, name = \"Message Count\") +\n  scale_x_continuous(\n    breaks = 0:23,\n    labels = function(x) sprintf(\"%02d:00\", x)\n  ) +\n  labs(\n    title = \"Hourly Heatmap of Radio Communications by Day\",\n    x = \"Hour of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Make interactive\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\nWe will increase the resolution to half-hour time slots.\n\n\n4.4b - Heat Map of half-hourly message volume for each day over the 2 weeks period:\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count.\n\n\nShow code\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Fix sender and receiver edges\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)  # ✅ fixed receiver_id\n\n# Step 2: Reconstruct sender–receiver–event linkage\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    time_label = sprintf(\"%02d:%02d\", floor(time_bin), ifelse(time_bin %% 1 == 0, 0, 30))\n  )\n\n# Step 3: Join to get sender/receiver labels\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n# Step 4: Aggregate by half-hour + label top actors\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, time_bin, time_label) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"📅 Date: \", date_label,\n      \"&lt;br&gt;🕒 Time: \", time_label,\n      \"&lt;br&gt;📨 Messages: \", count,\n      \"&lt;br&gt;🔴 Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;🟢 Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: ggplot\np &lt;- ggplot(comm_heatmap, aes(x = time_bin, y = fct_rev(factor(date_label)), fill = count, text = tooltip)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(\n    option = \"inferno\",\n    direction = -1,\n    name = \"Message Count\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  labs(\n    title = \"Half-Hourly Heatmap of Radio Communications by Day\",\n    x = \"Time of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Convert to interactive Plotly plot\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\n4.4c - Density plot of Daily half-hourly message volume over the 2 weeks period:\nThe faceted density plot that shows the distribution of communication events by time of day, broken down for each day in the dataset. It helps to visually detect temporal communication patterns, intensity, and consistency over multiple days.\n\nOverview of the 2 week periodDay 1 - 01/10/2040Day 2 - 02/10/2040Day 3 - 03/10/2040Day 4 - 04/10/2040Day 5 - 05/10/2040Day 6 - 06/10/2040Day 7 - 07/10/2040Day 8 - 08/10/2040Day 9 - 09/10/2040Day 10 - 10/10/2040Day 11 - 11/10/2040Day 12 - 12/10/2040Day 13 - 13/10/2040Day 14 - 14/10/2040\n\n\n\n\nShow code\n# Step 1: Preprocess communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5)\n  )\n\n# Step 2: Summarise daily medians and counts\ndaily_stats &lt;- comm_events %&gt;%\n  group_by(date_label) %&gt;%\n  summarise(\n    median_time = median(time_bin),\n    msg_count = n(),\n    .groups = \"drop\"\n  )\n\n# Step 3: Plot\nggplot(comm_events, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = daily_stats, aes(xintercept = median_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(\n    data = daily_stats,\n    aes(x = 20.5, y = 0.25, label = paste(\"Total:\", msg_count)),\n    inherit.aes = FALSE,\n    size = 3,\n    color = \"grey20\",\n    hjust = 1\n  ) +\n  facet_wrap(~ date_label, ncol = 4) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = NULL  # suppress all x-axis labels\n  ) +\n  labs(\n    title = \"Daily Communication Patterns (Half-Hourly)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"01/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"02/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"03/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"04/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"05/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"06/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"07/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"08/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"09/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"10/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"11/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"12/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"13/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"14/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n📈 Insights This Visualization Offers\n\n\n\nStep 3: Plot Combined Hourly and Half-hourly Communication Volume\nBar Plot of combined hourly message volume over the 2 weeks period:\n\n\nShow code\n# Prepare data\ncomm_hourly &lt;- comm_events %&gt;%\n  count(hour) %&gt;%\n  mutate(\n    hour_label = sprintf(\"%02d:00\", hour),  # Format to hh:mm\n    percent = n / sum(n)\n  )\n\n# Plot\nggplot(comm_hourly, aes(x = hour_label, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nBar Plot of combined half-hourly message volume in the 2 weeks period.\n\n\nShow code\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = sprintf(\"%02d:%02d\", hour, ifelse(minute &lt; 30, 0, 30))\n  )\n\ncomm_halfhour &lt;- comm_events %&gt;%\n  count(time_bin) %&gt;%\n  mutate(percent = n / sum(n))\n\nggplot(comm_halfhour, aes(x = time_bin, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Half-Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1a. What are the identifiable daily temporal patterns in communications?\n\n\n\n\nThe daily communication volume fluctuates slightly between 34 and 49 messages, with an average of approximately 42 messages per day, highlighting a stable overall activity level. Notably, the highest volume occurs on 11th October (49 messages), immediately following the lowest volume the day before on 10th October (34 messages)—a sharp rebound that may signal a response to specific events or operational needs. Despite these fluctuations, the system maintains a consistent tempo across the two weeks.\nThe temporal analysis using both the heat map and time series plots reveals a pronounced morning-centric communication rhythm. The vast majority of radio traffic is concentrated between 9:00 AM and 11:30 AM, with the most intense peaks typically occurring between 10:00 and 11:00 AM. With reference to the Density plot of Daily half-hourly message volume, of the 14 days, we see message density peaks at 10:30 AM on 9 days, while on 3 days, it peaks at 12:30 PM.\nFor instance if we were to based in on the hourly plot, 5th October (Fri) and 11th October (Thu) both register their highest single-hour counts at 10:00 AM at 24 and 21 messages respectively. Communication activity drops off steeply after lunchtime, with more than 90% of the days showing little to no activity after 2:30 PM. This pattern suggests a highly structured daily workflow, where key decisions and coordination are front-loaded in the day. Importantly, the hourly heat map also indicates that this routine holds across both weekdays and weekends—communication volumes and peak hours remain similar, underlining the operational regularity of the group regardless of the day of week.\n\n\n\n\n\n\n\n\n\n1b. How do these patterns shift over the two weeks of observations?\n\n\n\n\nOver the two-week period, while the timing and structure of communication peaks remain broadly consistent, there are subtle shifts in both intensity and timing. Some days, such as 3rd, 5th, 11th and 12th October, see particularly high spikes in the mid-morning, which may correspond to critical events, decision points, or heightened urgency. The sharp dip on October 8th and 13th, immediately after a period of “surge” (3rd - 7th and 9th to 12th October), points to possible responses to interruptions, lulls, or triggering incidents. Overall, although the daily messaging routine is remarkably stable, these bursts and brief lulls provide clues to changing circumstances or stress points in the operation—an analytical signal that warrants closer inspection of event logs or external triggers for those dates.\nAnother notable change in the communication pattern is observed during the weekends. In the first week, weekend communication peaks occurred earlier, typically between 10:00 AM and 11:30 AM, closely mirroring the weekday rhythm. However, in the second week, the weekend peaks shifted noticeably later, with the highest message volumes concentrated around 12:00 PM and 1:00 PM. This shift not only marks a departure from the otherwise stable early-morning communication structure but also suggests an adaptive or reactive operational schedule—potentially in response to evolving events, increased coordination needs, or changing priorities as the observation period progressed. The contrast between the two weekends is clear in the heatmap, underscoring the importance of monitoring such shifts as possible indicators of underlying changes in group behavior or external pressures."
  },
  {
    "objectID": "testing_qmd/testing.html#task-1c-focus-on-a-particular-entity---nadia-conti",
    "href": "testing_qmd/testing.html#task-1c-focus-on-a-particular-entity---nadia-conti",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5 - Task 1c: Focus on a Particular Entity - “Nadia Conti”",
    "text": "5 - Task 1c: Focus on a Particular Entity - “Nadia Conti”"
  },
  {
    "objectID": "testing_qmd/testing.html#vast-challenge-task-question-1c",
    "href": "testing_qmd/testing.html#vast-challenge-task-question-1c",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "VAST Challenge Task & Question 1c",
    "text": "VAST Challenge Task & Question 1c\nClepper found that messages frequently came in at around the same time each day.\n\nFocus on a specific entity and use this information to determine who has influence over them.\n\n\n5.1 - Data Preparation for “Nadia Conti” Influence Analysis\nWe first extracted the relevant communication edges from the dataset, pairing “sent” and “received” communication events to form entity-to-entity links. We retained only those edges where both nodes represent real-world entities (Person, Organization, Vessel, Group, or Location), ensuring that our analysis focuses on the meaningful actors in the Oceanus network.\n\n\nShow code\n# Extract sent and received communication event edges\nsent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(source_entity = from_id, event = to_id)\n\nreceived_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event = from_id, target_entity = to_id)\n\n# Pair sent and received to form communication edges\npaired_edges &lt;- sent_edges %&gt;%\n  inner_join(received_edges, by = \"event\") %&gt;%\n  select(from = source_entity, to = target_entity)\n\n# Add unmatched sent and received edges (optional, for completeness)\nsingle_sent_edges &lt;- sent_edges %&gt;%\n  select(from = source_entity, to = event)\nsingle_received_edges &lt;- received_edges %&gt;%\n  select(from = event, to = target_entity)\n\nall_edges &lt;- bind_rows(paired_edges, single_sent_edges, single_received_edges) %&gt;%\n  distinct()\n\n# Identify entity nodes (Person, Organization, Vessel, Group, Location)\nentity_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  pull(id) %&gt;% as.character()\n\nentity_edges &lt;- all_edges %&gt;%\n  filter(as.character(from) %in% entity_ids, as.character(to) %in% entity_ids)\n\nentity_nodes &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  select(id, label, sub_type)\n\n\n\n\n5.2 - Build the Global Network and Compute Centrality\nUsing these cleaned and filtered edges and nodes, we built a global directed graph representing the Oceanus community. We then computed key network centrality metrics for each node—PageRank, betweenness, and degree—quantifying the influence and connectivity of every entity in the overall network.\n\n\nShow code\nlibrary(igraph)\n\ng &lt;- graph_from_data_frame(d = entity_edges, vertices = entity_nodes, directed = TRUE)\n\n# Compute centralities\nV(g)$pagerank &lt;- page_rank(g)$vector\nV(g)$betweenness &lt;- betweenness(g)\nV(g)$degree &lt;- degree(g)\n\n\n\n\n5.3 - Extract “Nadia Conti” Ego Network (2-hop Neighbourhood)\nFocusing on “Nadia Conti”, we identified her node and extracted her two-step ego network, capturing both direct and indirect connections within the broader network. This local subgraph reveals Nadia’s immediate sphere of influence and the key players connected to her.\n\n\nShow code\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\nego_graph &lt;- make_ego_graph(g, order = 2, nodes = target_index, mode = \"all\")[[1]]\n\n\n\n\n5.4 - Visualize Nadia Conti’s Ego Network (Interactive)\nWe visualized Nadia’s ego network using node size, shape, and color to represent centrality and entity type. We also summarized centrality metrics in clear tables, ranking all ego network members by PageRank, Betweenness, and Degree. This allows for direct identification of the most influential, best-connected, and most strategic actors in Nadia Conti’s communication environment.\n\n\nShow code\nnodes_df &lt;- data.frame(\n  id = V(ego_graph)$name,\n  label = V(ego_graph)$label,\n  group = V(ego_graph)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_graph)$label, \"&lt;/b&gt;&lt;br&gt;\",\n                 \"Degree: \", round(V(ego_graph)$degree, 2), \"&lt;br&gt;\",\n                 \"Betweenness: \", round(V(ego_graph)$betweenness, 2), \"&lt;br&gt;\",\n                 \"PageRank: \", round(V(ego_graph)$pagerank, 4)),\n  shape = ifelse(V(ego_graph)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_graph)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_graph)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_graph)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  value = V(ego_graph)$pagerank * 30 + 5\n)\n\nedges_df &lt;- as_data_frame(ego_graph, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\nlibrary(visNetwork)\nvisNetwork(nodes_df, edges_df, width = \"100%\", height = \"700px\") %&gt;%\n  visNodes(scaling = list(min = 5, max = 30)) %&gt;%\n  visEdges(\n    arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)),\n    color = list(color = \"gray\")\n  ) %&gt;%\n  visOptions(\n    highlightNearest = TRUE,\n    nodesIdSelection = TRUE,\n    manipulation = FALSE\n  ) %&gt;%\n  visInteraction(\n    dragNodes = FALSE,\n    dragView = FALSE,\n    zoomView = FALSE\n  ) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)"
  },
  {
    "objectID": "testing_qmd/testing.html#global-and-ego-network-structure",
    "href": "testing_qmd/testing.html#global-and-ego-network-structure",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Global and Ego-Network Structure",
    "text": "Global and Ego-Network Structure\nThe overview network visualization reveals that Nadia Conti is centrally embedded in the Oceanus communication web, maintaining direct and indirect connections with major actors such as Neptune (Vessel), V. Miesel Shipping (Organization), Elise (Person), and others. The use of color and shape coding in the network allows for quick identification of the different types of entities in Nadia’s influence neighborhood.\n\n5.5 - Centrality Tables for Nadia’s Ego Network\nOn both the global and Nadia-focused ego networks, we computed standard network centrality metrics for all nodes:\n\nPageRank (overall influence),\nBetweenness (information brokerage/intermediary role), and\nDegree (number of direct connections).\n\nThese measures quantify the importance and structural roles of each entity relative to Nadia and the broader community.\n\n\nShow code\n# PageRank table\npagerank_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  pagerank = round(V(ego_graph)$pagerank, 4)\n) %&gt;% arrange(desc(pagerank))\n\n# Betweenness table\nbetweenness_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  betweenness = round(V(ego_graph)$betweenness, 2)\n) %&gt;% arrange(desc(betweenness))\n\n# Degree table\ndegree_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  degree = V(ego_graph)$degree\n) %&gt;% arrange(desc(degree))\n\n\n\n\nShow code\nknitr::kable(pagerank_df, caption = \"PageRank Centrality (Nadia's Ego Network)\")\n\n\n\nPageRank Centrality (Nadia’s Ego Network)\n\n\nlabel\nsub_type\npagerank\n\n\n\n\nMako\nVessel\n0.0687\n\n\nOceanus City Council\nOrganization\n0.0530\n\n\nReef Guardian\nVessel\n0.0454\n\n\nNadia Conti\nPerson\n0.0432\n\n\nRemora\nVessel\n0.0409\n\n\nV. Miesel Shipping\nOrganization\n0.0394\n\n\nNeptune\nVessel\n0.0358\n\n\nHimark Harbor\nLocation\n0.0358\n\n\nLiam Thorne\nPerson\n0.0275\n\n\nBoss\nPerson\n0.0272\n\n\nSentinel\nVessel\n0.0250\n\n\nPaackland Harbor\nLocation\n0.0244\n\n\nDavis\nPerson\n0.0239\n\n\nMarlin\nVessel\n0.0235\n\n\nEcoVigil\nVessel\n0.0233\n\n\nGreen Guardians\nOrganization\n0.0224\n\n\nMrs. Money\nPerson\n0.0192\n\n\nSailor Shifts Team\nOrganization\n0.0186\n\n\nSeawatch\nVessel\n0.0186\n\n\nElise\nPerson\n0.0182\n\n\nSerenity\nVessel\n0.0170\n\n\nHorizon\nVessel\n0.0152\n\n\nThe Middleman\nPerson\n0.0142\n\n\nNorthern Light\nVessel\n0.0135\n\n\nRodriguez\nPerson\n0.0122\n\n\nSamantha Blake\nPerson\n0.0114\n\n\nHaacklee Harbor\nLocation\n0.0111\n\n\nOsprey\nVessel\n0.0088\n\n\nCity Officials\nGroup\n0.0066\n\n\nThe Lookout\nPerson\n0.0062\n\n\nKnowles\nVessel\n0.0051\n\n\nSmall Fry\nPerson\n0.0035\n\n\nGlitters Team\nOrganization\n0.0035\n\n\n\n\n\n\n\nShow code\nknitr::kable(betweenness_df, caption = \"Betweenness Centrality (Nadia's Ego Network)\")\n\n\n\nBetweenness Centrality (Nadia’s Ego Network)\n\n\nlabel\nsub_type\nbetweenness\n\n\n\n\nMako\nVessel\n368.50\n\n\nMrs. Money\nPerson\n167.18\n\n\nReef Guardian\nVessel\n139.69\n\n\nBoss\nPerson\n136.18\n\n\nV. Miesel Shipping\nOrganization\n118.70\n\n\nNadia Conti\nPerson\n117.87\n\n\nOceanus City Council\nOrganization\n116.11\n\n\nRemora\nVessel\n90.45\n\n\nNeptune\nVessel\n82.59\n\n\nThe Lookout\nPerson\n80.51\n\n\nHimark Harbor\nLocation\n52.61\n\n\nThe Middleman\nPerson\n50.78\n\n\nLiam Thorne\nPerson\n41.81\n\n\nHaacklee Harbor\nLocation\n41.30\n\n\nSentinel\nVessel\n34.54\n\n\nGreen Guardians\nOrganization\n27.51\n\n\nPaackland Harbor\nLocation\n27.08\n\n\nDavis\nPerson\n22.36\n\n\nEcoVigil\nVessel\n12.63\n\n\nRodriguez\nPerson\n11.75\n\n\nNorthern Light\nVessel\n9.76\n\n\nSailor Shifts Team\nOrganization\n7.34\n\n\nHorizon\nVessel\n6.72\n\n\nMarlin\nVessel\n6.23\n\n\nSeawatch\nVessel\n5.20\n\n\nElise\nPerson\n4.60\n\n\nSamantha Blake\nPerson\n4.49\n\n\nSerenity\nVessel\n0.81\n\n\nKnowles\nVessel\n0.50\n\n\nSmall Fry\nPerson\n0.00\n\n\nGlitters Team\nOrganization\n0.00\n\n\nOsprey\nVessel\n0.00\n\n\nCity Officials\nGroup\n0.00\n\n\n\n\n\n\n\nShow code\nknitr::kable(degree_df, caption = \"Degree Centrality (Nadia's Ego Network)\")\n\n\n\nDegree Centrality (Nadia’s Ego Network)\n\n\nlabel\nsub_type\ndegree\n\n\n\n\nMako\nVessel\n37\n\n\nOceanus City Council\nOrganization\n28\n\n\nReef Guardian\nVessel\n27\n\n\nRemora\nVessel\n21\n\n\nV. Miesel Shipping\nOrganization\n19\n\n\nNeptune\nVessel\n19\n\n\nNadia Conti\nPerson\n17\n\n\nGreen Guardians\nOrganization\n17\n\n\nHimark Harbor\nLocation\n17\n\n\nDavis\nPerson\n16\n\n\nSentinel\nVessel\n16\n\n\nBoss\nPerson\n13\n\n\nEcoVigil\nVessel\n13\n\n\nPaackland Harbor\nLocation\n13\n\n\nMrs. Money\nPerson\n12\n\n\nHorizon\nVessel\n12\n\n\nLiam Thorne\nPerson\n11\n\n\nRodriguez\nPerson\n10\n\n\nMarlin\nVessel\n10\n\n\nSeawatch\nVessel\n9\n\n\nThe Middleman\nPerson\n8\n\n\nSerenity\nVessel\n8\n\n\nNorthern Light\nVessel\n8\n\n\nHaacklee Harbor\nLocation\n8\n\n\nElise\nPerson\n7\n\n\nThe Lookout\nPerson\n7\n\n\nSailor Shifts Team\nOrganization\n7\n\n\nSamantha Blake\nPerson\n6\n\n\nGlitters Team\nOrganization\n4\n\n\nKnowles\nVessel\n4\n\n\nSmall Fry\nPerson\n3\n\n\nOsprey\nVessel\n3\n\n\nCity Officials\nGroup\n1"
  },
  {
    "objectID": "testing_qmd/testing.html#centrality-metrics-and-direct-indirect-influences",
    "href": "testing_qmd/testing.html#centrality-metrics-and-direct-indirect-influences",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Centrality Metrics and Direct & Indirect Influences",
    "text": "Centrality Metrics and Direct & Indirect Influences\nBy calculating centrality metrics within Nadia’s two-hop ego network, we observe that the most influential nodes in her environment—by PageRank, betweenness, and degree—are Neptune, V. Miesel Shipping, and Elise. Nadia herself consistently ranks among the top nodes by these measures, highlighting her role as both an influencer and an information bridge. Entities such as Neptune and V. Miesel Shipping, which also score highly in centrality, exert considerable influence over Nadia’s information flow and access to other parts of the network.\nDegree centrality analysis shows Nadia maintains multiple direct connections, particularly with other highly active nodes, ensuring she is closely linked to key hubs in the network. Betweenness centrality further reveals that Nadia is not only well-connected but also acts as an important intermediary, facilitating communication between otherwise distant parts of the network. PageRank confirms that her immediate environment is composed of actors with significant structural power, increasing the likelihood that Nadia is both influenced by, and exerts influence upon, the most pivotal players in Oceanus.\n\n5.5.1 - PageRank for Nadia Conti\n\n\nShow code\nlibrary(igraph)\nlibrary(visNetwork)\n\n# -- Build the global network g as in your earlier code (using your entity_nodes/entity_edges) --\n\ng &lt;- graph_from_data_frame(\n  d = entity_edges, \n  vertices = entity_nodes, \n  directed = TRUE\n)\n\n# -- Get Nadia's index in g --\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\n# -- Extract Nadia's 1-hop ego network (all direct neighbors) --\nego_1 &lt;- make_ego_graph(g, order = 1, nodes = target_index, mode = \"all\")[[1]]\n\n\n# 1. Compute PageRank for the ego network\nV(ego_1)$pagerank &lt;- page_rank(ego_1)$vector\n\n# 2. Prepare node data frame with your consistent color scheme\nnodes_df_pagerank &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;PageRank: \", round(V(ego_1)$pagerank, 4)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$pagerank * 30 + 5\n)\n\n# 3. Prepare edges\nedges_df &lt;- as_data_frame(ego_1, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\n# 4. Plot with visNetwork\nvisNetwork(nodes_df_pagerank, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_pagerank$color, border = \"black\"),\n    shape = nodes_df_pagerank$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n5.5.2 - Betweenness for Nadia Conti\n\n\nShow code\n# 1. Compute Betweenness for the ego network\nV(ego_1)$betweenness &lt;- betweenness(ego_1, directed = TRUE)\n\n# 2. Prepare node data frame\nnodes_df_betweenness &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Betweenness: \", round(V(ego_1)$betweenness, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$betweenness * 2 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_betweenness, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_betweenness$color, border = \"black\"),\n    shape = nodes_df_betweenness$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n5.5.3 - Degree for Nadia Conti\n\n\nShow code\n# 1. Compute Degree for the ego network\nV(ego_1)$degree &lt;- degree(ego_1, mode = \"all\")\n\n# 2. Prepare node data frame\nnodes_df_degree &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Degree: \", round(V(ego_1)$degree, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$degree * 5 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_degree, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_degree$color, border = \"black\"),\n    shape = nodes_df_degree$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\n1c. With a focus on “Nadia Conti”, the visuals above could determine who has influence over this person.\n\n\n\n\nDegree centrality reveals that Nadia Conti is well-connected within her local network, with a degree of 17. However, she is not the most connected node; vessels such as Mako (37), Reef Guardian (27), and Remora (21), as well as organizations like Oceanus City Council (28) and V. Miesel Shipping (19), have even higher degrees. This indicates that while Nadia is an important hub, her sphere of direct interaction is embedded within a dense mesh of other highly connected entities.\nSeveral other individuals (e.g., Davis with 16, Boss with 13, Mrs. Money with 12) and vessels (e.g., Neptune with 19, Sentinel with 16) also play significant roles in Nadia’s network. The presence of organizations (e.g., Green Guardians, Sailor Shifts Team), multiple vessels, and key persons shows that Nadia’s environment is both diverse and robust.\nDirect Connections\nThese direct connections are clearly shown as nodes that have edges (arrows) going into or out of Nadia Conti’s node in the network diagrams. Nadia Conti directly connects to several core entities across different types:\n\nPeople: Elise, Liam Thorne, Davis, Rodriguez\nOrganization: V. Miesel Shipping, Oceanus City Council, Sailor Shifts Team\nVessel: Neptune, Marlin, Remora, Sentinel\nLocation: Haacklee Harbor\n\nInterpretation: The PageRank, Betweenness, and Degree centrality plots all consistently show Nadia Conti as a major hub, with a large node size reflecting her high centrality. Her immediate network includes influential vessels (Neptune, Remora), organizations (V. Miesel Shipping, Oceanus City Council), and several persons (Elise, Davis, Rodriguez).\nNadia’s position suggests she is a key connector and influencer but is herself surrounded by even larger hubs, particularly among vessels and organizations. Her ability to influence—and be influenced—is amplified by these connections, as these high-degree entities are likely sources and conduits of critical information and operational coordination. This structure points to a tightly interwoven community, where central actors such as Mako, Oceanus City Council, and V. Miesel Shipping may exert the most substantial influence over Nadia’s access to information, resources, and strategic decisions."
  },
  {
    "objectID": "testing_qmd/testing.html#extracting-nadias-data",
    "href": "testing_qmd/testing.html#extracting-nadias-data",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.1 Extracting Nadia’s data",
    "text": "5.1 Extracting Nadia’s data\n\n\nShow code\nnodes &lt;- MC3$nodes\nedges &lt;- MC3$edges\n\n# Extract communication events\ncomms &lt;- nodes %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(id, content)\n\n# Link to sender & receiver\nsent_edges &lt;- edges %&gt;% filter(type == \"sent\") %&gt;%\n  select(source = source, comm_id = target)\n\nrecv_edges &lt;- edges %&gt;% filter(type == \"received\") %&gt;%\n  select(comm_id = source, target = target)\n\n# Merge\ncomms_data &lt;- comms %&gt;%\n  left_join(sent_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(sender = source) %&gt;%\n\n  left_join(recv_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(receiver = target)\n\n# Add sender/receiver names\nmc3_nodes_cleaned &lt;- nodes %&gt;%\n  mutate(id = as.character(id)) %&gt;%\n  filter(!is.na(id)) %&gt;%\n  distinct(id, .keep_all = TRUE)\n\ncomms_data &lt;- comms_data %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver\" = \"id\"))\n\n# Count Nadia's messages\nnadia_counts &lt;- comms_data %&gt;%\n  summarise(\n    Sent = sum(sender_label == \"Nadia Conti\", na.rm = TRUE),\n    Received = sum(receiver_label == \"Nadia Conti\", na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Type\", values_to = \"Count\") %&gt;%\n  mutate(\n    Percent = Count / sum(Count),\n    Label = paste0(round(Percent * 100), \"%\\n(\", Count, \" msgs)\")\n  )"
  },
  {
    "objectID": "testing_qmd/testing.html#message-count-of-nadia",
    "href": "testing_qmd/testing.html#message-count-of-nadia",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.2 Message count of Nadia",
    "text": "5.2 Message count of Nadia\n\n\nShow code\nggplot(nadia_counts, aes(x = Count, y = reorder(Type, Count), fill = Type)) +\n  geom_col(color = \"white\") +\n  geom_text(aes(label = paste0(Count, \" msgs (\", round(Percent * 100), \"%)\")),\n            hjust = -0.1, size = 4) +\n  scale_fill_manual(values = c(\"Sent\" = \"deepskyblue3\", \"Received\" = \"cyan\")) +\n  labs(title = paste0(\"Nadia Conti's Messages (Total: \", sum(nadia_counts$Count), \")\"),\n       x = \"Message Count\", y = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\")) +\n  xlim(0, max(nadia_counts$Count) * 1.2)"
  },
  {
    "objectID": "testing_qmd/testing.html#message-frequency-of-nadia",
    "href": "testing_qmd/testing.html#message-frequency-of-nadia",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.3 Message frequency of Nadia",
    "text": "5.3 Message frequency of Nadia\n\n\nShow code\n# Make sure nadia_data is created\nnadia_data &lt;- comms_data %&gt;%\n  filter(sender_label == \"Nadia Conti\" | receiver_label == \"Nadia Conti\") %&gt;%\n  left_join(nodes %&gt;% select(id, timestamp), by = c(\"id\" = \"id\")) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(date = as.Date(timestamp), hour = hour(timestamp))\n\n# Create daily_freq\ndaily_freq &lt;- nadia_data %&gt;%\n  group_by(date) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n# Create hourly_freq\nhourly_freq &lt;- nadia_data %&gt;%\n  group_by(date, hour) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n\n\n5.3.1 Daily\n\n\nShow code\nggplot(daily_freq, aes(x = date, y = count)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = count), vjust = -0.5, size = 3) +\n  labs(\n    title = \"Nadia Conti's Daily Message Frequency\",\n    x = \"Date\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Hourly\n\n\nShow code\nlibrary(plotly)\n\nplot_ly(\n  data = hourly_freq,\n  x = ~hour,\n  y = ~count,\n  color = ~as.factor(date),\n  type = 'bar',\n  text = ~paste(\"Date:\", date, \"&lt;br&gt;Hour:\", hour, \"&lt;br&gt;Messages:\", count),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    barmode = 'dodge',  # use 'stack' if you prefer stacked bars\n    title = \"Nadia Conti's Hourly Message Frequency\",\n    xaxis = list(title = \"Hour of Day\"),\n    yaxis = list(title = \"Message Count\"),\n    legend = list(title = list(text = \"Date\"))\n  )"
  },
  {
    "objectID": "testing_qmd/testing.html#nadias-relationship-pattern",
    "href": "testing_qmd/testing.html#nadias-relationship-pattern",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.4 Nadia’s relationship pattern",
    "text": "5.4 Nadia’s relationship pattern\n\n\nShow code\nlibrary(ggplot2)\n\n# Count relationships by type\nrelationship_counts &lt;- mc3_edges_cleaned %&gt;%\n  filter(type != \"sent\", type != \"received\") %&gt;%  # Focus on relationships, not communication\n  count(type, sort = TRUE)\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(visNetwork)\n\n# Summarise Nadia's communication edges\nnadia_edges &lt;- nadia_data %&gt;%\n  count(sender_label, receiver_label) %&gt;%\n  filter(!is.na(sender_label), !is.na(receiver_label)) %&gt;%\n  rename(from = sender_label, to = receiver_label, value = n)\n\n# Get sender + receiver entity info\n# Get type info for sender and receiver\nentity_info &lt;- bind_rows(\n  nadia_data %&gt;%\n    left_join(mc3_nodes_cleaned %&gt;% select(id, name = label, type = sub_type),\n              by = c(\"sender\" = \"id\")) %&gt;%\n    select(name, type),\n  nadia_data %&gt;%\n    left_join(mc3_nodes_cleaned %&gt;% select(id, name = label, type = sub_type),\n              by = c(\"receiver\" = \"id\")) %&gt;%\n    select(name, type)\n) %&gt;%\n  distinct()\n\n# Build node table\nnadia_nodes &lt;- tibble(name = unique(c(nadia_edges$from, nadia_edges$to))) %&gt;%\n  left_join(entity_info, by = \"name\") %&gt;%\n  mutate(\n    group = ifelse(name == \"Nadia Conti\", \"Nadia Conti\", type),\n    id = name,\n    label = name,\n    color = case_when(\n      group == \"Person\" ~ \"#fc8d62\",       \n      group == \"Organization\" ~ \"#6baed6\",\n      group == \"Vessel\" ~ \"#66c2a2\",      \n      group == \"Location\" ~ \"#c6dbef\",    \n      group == \"Nadia Conti\" ~ \"#ffd92f\", \n      TRUE ~ \"#d9d9d9\"\n    ),\n    shape = case_when(\n      group == \"Person\" ~ \"dot\",\n      group == \"Organization\" ~ \"square\",\n      group == \"Vessel\" ~ \"triangle\",\n      group == \"Location\" ~ \"diamond\",\n      group == \"Nadia Conti\" ~ \"star\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Render network\nvisNetwork(nodes = nadia_nodes, edges = nadia_edges) %&gt;%\n  visEdges(arrows = \"to\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visLayout(randomSeed = 123) %&gt;%\n  visPhysics(\n    solver = \"forceAtlas2Based\",\n    forceAtlas2Based = list(gravitationalConstant = -25, centralGravity = 0.01, springLength = 50, springConstant = 0.02),\n    stabilization = list(enabled = TRUE, iterations = 100)\n  ) %&gt;%\n  visInteraction(navigationButtons = TRUE) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = list(\n      list(label = \"Person\", shape = \"dot\", color = \"#fc8d62\"),\n      list(label = \"Organization\", shape = \"square\", color = \"#6baed6\"),\n      list(label = \"Vessel\", shape = \"triangle\", color = \"#66c2a2\"),\n      list(label = \"Location\", shape = \"diamond\", color = \"#c6dbef\"),\n      list(label = \"Nadia Conti\", shape = \"star\", color = \"#ffd92f\")\n    ),\n    width = 0.2,\n    position = \"left\",\n    stepY = 80,\n    ncol = 1\n  )"
  },
  {
    "objectID": "testing_qmd/testing.html#nadias-most-frequent-commuter",
    "href": "testing_qmd/testing.html#nadias-most-frequent-commuter",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.5 Nadia’s most frequent commuter",
    "text": "5.5 Nadia’s most frequent commuter\n\n\nShow code\n# Get communication events linked to Nadia\nnadia_comm_ids &lt;- edges %&gt;%\n  filter(type == \"sent\" | type == \"received\") %&gt;%\n  filter(source == mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"] |\n         target == mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"]) %&gt;%\n  mutate(comm_id = ifelse(type == \"sent\", target, source)) %&gt;%\n  pull(comm_id) %&gt;%\n  unique()\n\n# Get edges related to these communications\nnadia_related_edges &lt;- edges %&gt;%\n  filter(source %in% nadia_comm_ids | target %in% nadia_comm_ids)\n\n# Get people connected (excluding comm events + Nadia herself)\nnadia_id &lt;- mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"]\n\nnadia_contacts_ids &lt;- nadia_related_edges %&gt;%\n  mutate(person_id = ifelse(source %in% nadia_comm_ids, target, source)) %&gt;%\n  filter(!person_id %in% nadia_comm_ids, person_id != nadia_id) %&gt;%\n  count(person_id, sort = TRUE)\n\n# Join with node labels\ntop_contacts_named &lt;- nadia_contacts_ids %&gt;%\n  left_join(nodes %&gt;% filter(sub_type == \"Person\") %&gt;% select(id, name = label),\n            by = c(\"person_id\" = \"id\")) %&gt;%\n  filter(!is.na(name))\n\n\n\n\nShow code\ntop_contacts_named %&gt;%\n  slice_max(n, n = 3) %&gt;%\n  ggplot(aes(x = reorder(name, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 3 Contacts Communicating with Nadia Conti\",\n    x = \"Contact Person\",\n    y = \"Number of Messages\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(DT)\n\nnadia_id &lt;- mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"]\n\n# Nadia's communication event IDs\nnadia_comm_ids &lt;- edges %&gt;%\n  filter(type == \"sent\" | type == \"received\") %&gt;%\n  filter(source == nadia_id | target == nadia_id) %&gt;%\n  mutate(comm_id = ifelse(type == \"sent\", target, source)) %&gt;%\n  pull(comm_id) %&gt;%\n  unique()\n\n# Top contact comm IDs\ntop_contact_comm_ids &lt;- edges %&gt;%\n  filter(\n    (source %in% nadia_comm_ids & target %in% top_contacts_named$person_id) |\n    (target %in% nadia_comm_ids & source %in% top_contacts_named$person_id)\n  ) %&gt;%\n  mutate(comm_id = ifelse(source %in% nadia_comm_ids, source, target)) %&gt;%\n  pull(comm_id) %&gt;%\n  unique()\n\n# Get comm event details\nnadia_messages &lt;- nodes %&gt;%\n  filter(id %in% top_contact_comm_ids) %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(id, timestamp, content) %&gt;%\n  left_join(edges %&gt;% filter(type == \"sent\") %&gt;% select(id = target, sender = source),\n            by = \"id\") %&gt;%\n  left_join(edges %&gt;% filter(type == \"received\") %&gt;% select(id = source, receiver = target),\n            by = \"id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_name = label), by = c(\"sender\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_name = label), by = c(\"receiver\" = \"id\")) %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    sender_receiver = paste(sender_name, \"→\", receiver_name)\n  ) %&gt;%\n  arrange(timestamp) %&gt;%\n  select(timestamp, sender_receiver, content)\n\n# Display\nDT::datatable(\n  nadia_messages,\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    scrollX = TRUE,\n    initComplete = htmlwidgets::JS(\n      \"function(settings, json) {\",\n      \"$(this.api().table().header()).css({'background-color': '#f8f9fa', 'color': '#333'});\",\n      \"}\"\n    )\n  ),\n  rownames = FALSE,\n  class = 'stripe hover compact',\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-size:16px; color:#444;',\n    'Messages'\n  )\n)"
  },
  {
    "objectID": "testing_qmd/testing.html#temporal-suspicious-event-alignment",
    "href": "testing_qmd/testing.html#temporal-suspicious-event-alignment",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.6 Temporal + suspicious event alignment",
    "text": "5.6 Temporal + suspicious event alignment\n\n5.6.1 Showing Nadia’s unusually active days\n\n\nShow code\n# Compute mean + SD of daily messages\ndaily_summary &lt;- daily_freq %&gt;%\n  summarise(mean_count = mean(count), sd_count = sd(count))\n\n# Flag days with unusually high message counts\nspike_days &lt;- daily_freq %&gt;%\n  filter(count &gt; daily_summary$mean_count + 2 * daily_summary$sd_count)\n\n# Show spike days\nprint(spike_days)\n\n\n# A tibble: 1 × 2\n  date       count\n  &lt;date&gt;     &lt;int&gt;\n1 2040-10-08     9\n\n\n\n\n5.6.2 Suspicious dates\n\n\nShow code\nsuspicious_dates &lt;- as.Date(c(\"2040-10-05\", \"2040-10-08\", \"2040-10-11\")) # example reef closure, approvals, etc.\n\n\n\n\nShow code\nspike_days %&gt;%\n  mutate(suspicious = ifelse(date %in% suspicious_dates, \"YES\", \"NO\"))\n\n\n# A tibble: 1 × 3\n  date       count suspicious\n  &lt;date&gt;     &lt;int&gt; &lt;chr&gt;     \n1 2040-10-08     9 YES       \n\n\n\n\nShow code\nlibrary(plotly)\nlibrary(dplyr)\n\n# Suppose suspicious dates (replace with real ones)\nsuspicious_dates &lt;- as.Date(c(\"2040-10-05\", \"2040-10-08\", \"2040-10-11\"))\n\n# Compute threshold\ndaily_summary &lt;- daily_freq %&gt;%\n  summarise(mean_count = mean(count), sd_count = sd(count))\n\nthreshold &lt;- daily_summary$mean_count + 2 * daily_summary$sd_count\n\n# Add status column\ndaily_freq_plot &lt;- daily_freq %&gt;%\n  mutate(\n    status = case_when(\n      date %in% suspicious_dates ~ \"Suspicious Date\",\n      count &gt; threshold ~ \"Spike\",\n      TRUE ~ \"Normal\"\n    )\n  )\n\n# Assign colors\nstatus_colors &lt;- c(\n  \"Normal\" = \"steelblue\",\n  \"Spike\" = \"red\",\n  \"Suspicious Date\" = \"orange\"\n)\n\n# Build Plotly bar chart\nplot_ly(\n  data = daily_freq_plot,\n  x = ~date,\n  y = ~count,\n  type = 'bar',\n  color = ~status,\n  colors = status_colors,\n  text = ~paste(\"Date:\", date, \"&lt;br&gt;Messages:\", count, \"&lt;br&gt;Status:\", status),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = \"Nadia Conti's Daily Communication\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Message Count\"),\n    barmode = 'group',\n    legend = list(title = list(text = \"Status\"))\n  ) %&gt;%\n  add_lines(\n    x = ~date,\n    y = rep(threshold, nrow(daily_freq_plot)),\n    line = list(dash = 'dash', color = 'red'),\n    name = 'Spike Threshold',\n    inherit = FALSE\n  )"
  },
  {
    "objectID": "testing_qmd/testing.html#drilling-down-on-spike-flagged-date",
    "href": "testing_qmd/testing.html#drilling-down-on-spike-flagged-date",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.7 Drilling down on spike + flagged date",
    "text": "5.7 Drilling down on spike + flagged date\n\n5.7.1 Extract Nadia’s message from Oct 8\n\n\nShow code\n# Build fresh nadia_data with content included at the start\nnadia_data &lt;- comms %&gt;%\n  left_join(sent_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(sender = source) %&gt;%\n  left_join(recv_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(receiver = target) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver\" = \"id\")) %&gt;%\n  left_join(nodes %&gt;% select(id, timestamp), by = \"id\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    hour = hour(timestamp)\n  ) %&gt;%\n  filter(sender_label == \"Nadia Conti\" | receiver_label == \"Nadia Conti\") %&gt;%\n  filter(!is.na(timestamp))\n\n\n\n\nShow code\noct8_msgs &lt;- nadia_data %&gt;%\n  filter(date == as.Date(\"2040-10-08\")) %&gt;%\n  select(timestamp, sender_label, receiver_label, content) %&gt;%\n  arrange(timestamp)\n\nDT::datatable(\n  oct8_msgs,\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    scrollX = TRUE,\n    columnDefs = list(\n      list(\n        targets = 3,  # adjust if content is not 3rd col\n        render = JS(\n          \"function(data, type, row, meta) {\",\n          \"return type === 'display' && data.length &gt; 50 ?\",\n          \"'&lt;span title=\\\"' + data + '\\\"&gt;' + data.substr(0, 50) + '...&lt;/span&gt;' : data;\",\n          \"}\"\n        )\n      )\n    )\n  ),\n  rownames = FALSE,\n  class = 'stripe hover compact',\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-size:14px; color:#444;',\n    '📌 Nadia Conti Messages on Oct 8, 2040'\n  )\n)\n\n\n\n\n\n\n\n\n5.7.2 Keywords of Oct 8\nShowing messages on Oct 8 mentioning suspicious terms of:\n\npermit\napproval\nreef\ncargo\nshipment\nillegal\n\n\n\nShow code\n# Define suspicious keywords\nkeywords &lt;- c(\"permit\", \"approval\", \"reef\", \"cargo\", \"shipment\", \"dock\", \"illegal\")\n\n# Filter messages on Oct 8 with suspicious terms\noct8_flagged_msgs &lt;- nadia_data %&gt;%\n  filter(date == as.Date(\"2040-10-08\")) %&gt;%\n  filter(!is.na(content)) %&gt;%\n  filter(grepl(paste(keywords, collapse = \"|\"), content, ignore.case = TRUE)) %&gt;%\n  select(timestamp, sender_label, receiver_label, content) %&gt;%\n  arrange(timestamp)\n\n# Display in interactive table\nDT::datatable(\n  oct8_flagged_msgs,\n  options = list(pageLength = 5, autoWidth = TRUE),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-size:16px; color:#444;',\n    '📌 Oct 8 Messages with Suspicious Keywords'\n  )\n)\n\n\n\n\n\n\n\n\n5.7.3 Network of Oct 8 communication\n\n\nShow code\nlibrary(visNetwork)\n\n# Summarize comms on Oct 8\noct8_edges &lt;- nadia_data %&gt;%\n  filter(date == as.Date(\"2040-10-08\")) %&gt;%\n  count(sender_label, receiver_label) %&gt;%\n  filter(!is.na(sender_label), !is.na(receiver_label)) %&gt;%\n  rename(from = sender_label, to = receiver_label, value = n)\n\n# Build node list\noct8_nodes &lt;- tibble(name = unique(c(oct8_edges$from, oct8_edges$to))) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(label, sub_type), by = c(\"name\" = \"label\")) %&gt;%\n  mutate(\n    group = ifelse(name == \"Nadia Conti\", \"Nadia Conti\", sub_type),\n    id = name,\n    label = name\n  )\n\n# Render network\nvisNetwork(oct8_nodes, oct8_edges) %&gt;%\n  visEdges(arrows = \"to\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visLayout(randomSeed = 456) %&gt;%\n  visPhysics(stabilization = TRUE) %&gt;%\n  visLegend()\n\n\n\n\n\n\nNadia is heavily involved in:\n\nDiscussion of Nemo Reef, permits, foundation work\nCoordinating payments, doubling fees, Harbor Master cooperation\nAdjusting schedules to avoid council suspicion\n\nHighly suspicious tone: manipulation, concealment, operational coordination beyond scope."
  },
  {
    "objectID": "testing_qmd/testing.html#linking-oct-8-comms-to-permits-approvals-or-vessel-activity",
    "href": "testing_qmd/testing.html#linking-oct-8-comms-to-permits-approvals-or-vessel-activity",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "5.8 Linking Oct 8 comms to permits, approvals, or vessel activity",
    "text": "5.8 Linking Oct 8 comms to permits, approvals, or vessel activity\n\n\nShow code\nsuspicious_events_alt &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type %in% c(\"VesselMovement\", \"Monitoring\", \"HarborReport\", \"Fishing\", \"Enforcement\")) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(timestamp &gt;= as.POSIXct(\"2040-10-08\"))\n\nDT::datatable(\n  suspicious_events_alt %&gt;%\n    select(type, label, sub_type, id, timestamp, monitoring_type, findings),\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    scrollX = TRUE\n  ),\n  rownames = FALSE\n)\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(plotly)\n\n# 1️⃣ Prepare entity-related vessel/harbor events\nentity_events &lt;- suspicious_events_alt %&gt;%\n  filter(str_detect(findings, regex(\"Neptune|Miesel|Mako\", ignore_case = TRUE))) %&gt;%\n  mutate(entity = case_when(\n    str_detect(findings, regex(\"Neptune\", ignore_case = TRUE)) ~ \"Neptune\",\n    str_detect(findings, regex(\"Miesel\", ignore_case = TRUE)) ~ \"Miesel\",\n    str_detect(findings, regex(\"Mako\", ignore_case = TRUE)) ~ \"Mako\",\n    TRUE ~ \"Other\"\n  ))\n\n# 2️⃣ Build interactive plot\nplot_ly() %&gt;%\n  # Nadia comms\n  add_markers(\n    data = nadia_data,\n    x = ~timestamp,\n    y = ~\"Nadia Message\",\n    marker = list(color = \"red\", size = 10),\n    text = ~paste0(\"Nadia Message&lt;br&gt;\", timestamp),\n    hoverinfo = \"text\",\n    name = \"Nadia Message\"\n  ) %&gt;%\n  # Neptune events\n  add_markers(\n    data = entity_events %&gt;% filter(entity == \"Neptune\"),\n    x = ~timestamp,\n    y = ~entity,\n    marker = list(color = \"#1f77b4\", size = 10),\n    text = ~paste0(entity, \" Event&lt;br&gt;\", findings),\n    hoverinfo = \"text\",\n    name = \"Neptune Event\"\n  ) %&gt;%\n  # Miesel events\n  add_markers(\n    data = entity_events %&gt;% filter(entity == \"Miesel\"),\n    x = ~timestamp,\n    y = ~entity,\n    marker = list(color = \"#17becf\", size = 10),\n    text = ~paste0(entity, \" Event&lt;br&gt;\", findings),\n    hoverinfo = \"text\",\n    name = \"Miesel Event\"\n  ) %&gt;%\n  # Mako events\n  add_markers(\n    data = entity_events %&gt;% filter(entity == \"Mako\"),\n    x = ~timestamp,\n    y = ~entity,\n    marker = list(color = \"#7f7f7f\", size = 10),\n    text = ~paste0(entity, \" Event&lt;br&gt;\", findings),\n    hoverinfo = \"text\",\n    name = \"Mako Event\"\n  ) %&gt;%\n  layout(\n    title = \"Nadia Comms + Vessel/Harbor Events\",\n    xaxis = list(title = \"Time\"),\n    yaxis = list(title = \"\"),\n    legend = list(orientation = \"h\", x = 0.1, y = -0.3)\n  )\n\n\n\n\n\n\nThe interactive timeline highlights that Nadia Conti’s communications were closely followed by vessel/harbor events involving Neptune, V. Miesel Shipping, and Mako. Notably:\n•   On **Oct 8**, Nadia’s messages spiked, coinciding with planned operations at Nemo Reef.\n\n•   Shortly afterward, vessel activities linked to **Neptune, Miesel, and Mako** were logged.\n\n•   This temporal proximity strongly suggests coordination between Nadia and these entities.\nThere is no evidence of formal approvals or permits linked to these activities, pointing to potential covert operations."
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html",
    "title": "Take-Home Exercise 1 Part 2",
    "section": "",
    "text": "Commenting on David Chiam’s EDA 3.2\n\n\n\nLink: David Chiam Netlify",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 B"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#setting-the-scene",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#setting-the-scene",
    "title": "Take-Home Exercise 1 Part 2",
    "section": "Setting the scene",
    "text": "Setting the scene\nA local online media company that publishes daily content on digital platforms is planning to release an article on demographic structures and distribution of Singapore in 2024.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 B"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#the-task",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#the-task",
    "title": "Take-Home Exercise 1 Part 2",
    "section": "The task",
    "text": "The task\nAssuming the role of the graphical editor of the media company, the task was to prepare at most three data visualisation for the exercise.\nHence in this exercise, I curated three key visualisations as part of dats exploration:\n\nGenerational Distribution by Region: A paired bar chart highlights both absolute and relative generational makeup across Singapore’s five planning regions, revealing the Central and East as home to proportionally older populations.\nAge Demographics by Planning Area: A boxplot with overlaid generation bands showcases median age and interquartile ranges, emphasising regions with mature population profiles like Central and West Singapore.\nTop Planning Areas by Age Band: A set of ranked bar charts and accompanying pie charts depict the top 15 planning areas by population size in three age segments—youth (0–24), adults (25–54), and seniors (55+). These are complemented with gender breakdowns to visualise demographic imbalances.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 B"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#good-points",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#good-points",
    "title": "Take-Home Exercise 1 Part 2",
    "section": "Good points",
    "text": "Good points\n\nClear Breakdown by Generation\n\nThe stacked bar plots clearly distinguished generations using multiple colors and labels, making it easy to digest\n\nGood Use of Facets (Counts vs Percentage)\n\nShows both raw counts and percentages side by side for good comparison\n\nBoxplot with Annotated Summary Stats\n\nLabels on age distribution boxplot aids for easier interpretation",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 B"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#improvements",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#improvements",
    "title": "Take-Home Exercise 1 Part 2",
    "section": "Improvements",
    "text": "Improvements\n\nFont Size Too Small for Lables\n\nIssue: text is hard to read\nFix: increase font size to improve readability\n\nColor Palette Not Very Readable\n\nIssue: hard to read and not colorblind friendly\nFix: use color palette such as viridis or OkabeIto\n\nOvercrowded Text in Stacked Bars\n\nIssue: readability of text inside stacked bars are not good\nFix: display labels for only larger segments",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 B"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#revised-version-of-eda-3.2",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01B.html#revised-version-of-eda-3.2",
    "title": "Take-Home Exercise 1 Part 2",
    "section": "Revised Version of EDA 3.2",
    "text": "Revised Version of EDA 3.2",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 B"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "",
    "text": "This take home exercise is based on the VAST Challenge Mini Case 3\nOver the past decade, the community of Oceanus has faced numerous transformations and challenges evolving from its fishing-centric origins. Following major crackdowns on illegal fishing activities, suspects have shifted investments into more regulated sectors such as the ocean tourism industry, resulting in growing tensions. This increased tourism has recently attracted the likes of international pop star Sailor Shift, who announced plans to film a music video on the island.\nClepper Jessen, a former analyst at FishEye and now a seasoned journalist for the Hacklee Herald, has been keenly observing these rising tensions. Recently, he turned his attention towards the temporary closure of Nemo Reef. By listening to radio communications and utilizing his investigative tools, Clepper uncovered a complex web of expedited approvals and secretive logistics. These efforts revealed a story involving high-level Oceanus officials, Sailor Shift’s team, local influential families, and local conservationist group The Green Guardians, pointing towards a story of corruption and manipulation.\nYour task is to develop new and novel visualizations and visual analytics approaches to help Clepper get to the bottom of this story",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#initial-eda",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#initial-eda",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "2.1 Initial EDA",
    "text": "2.1 Initial EDA\n\n\nShow code\nExpCatViz(data=mc3_nodes,\n          col=\"pink\")\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q1a-develop-a-graph-based-visual-analytics-approach-to-identify-any-daily-temporal-patterns-in-communications",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q1a-develop-a-graph-based-visual-analytics-approach-to-identify-any-daily-temporal-patterns-in-communications",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q1A: Develop a graph-based visual analytics approach to identify any daily temporal patterns in communications",
    "text": "Q1A: Develop a graph-based visual analytics approach to identify any daily temporal patterns in communications\n\n\nShow code\ncomm_events &lt;- mc3_nodes_final |&gt;\n  filter(type == \"Event\",\n         sub_type == \"Communication\",\n         !is.na(timestamp)) |&gt;\n  mutate(ts_utc   = ymd_hms(timestamp, tz = \"UTC\"),\n         ts_local = with_tz(ts_utc, \"Asia/Taipei\"),  \n         date     = as_date(ts_local),\n         hour     = hour(ts_local))\n\ncomm_events |&gt;\n  count(hour) |&gt;\n  ggplot(aes(hour, n)) +\n  geom_col() +\n  labs(title = \"Hourly volume of radio traffic (all days)\",\n       x = \"Hour of day\", y = \"Message count\")\n\n\n\n\n\n\n\n\n\nMessages spike around 17-18 UTC every day, then taper quickly after 19 UTC, with almost no traffic past 22 UTC.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q1b-how-do-these-patterns-shift-over-the-two-weeks-of-observations",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q1b-how-do-these-patterns-shift-over-the-two-weeks-of-observations",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q1B: How do these patterns shift over the two weeks of observations?",
    "text": "Q1B: How do these patterns shift over the two weeks of observations?\n\n\nShow code\ncomm_events |&gt;\n  count(date, hour) |&gt;\n  ggplot(aes(hour, date, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"C\") +\n  scale_y_date(expand = c(0, 0)) +\n  labs(title = \"Traffic density across days & hours\",\n       x = \"Hour\", y = \"Date\", fill = \"Msgs\")\n\n\n\n\n\n\n\n\n\nThe evening peak is consistent, but Oct 08 and Oct 11 show brighter bands—bursts of activity linked to reef-closure announcements.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q1c-focus-on-a-specific-entity-and-use-this-information-to-determine-who-has-influence-over-them.",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q1c-focus-on-a-specific-entity-and-use-this-information-to-determine-who-has-influence-over-them.",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q1C: Focus on a specific entity and use this information to determine who has influence over them.",
    "text": "Q1C: Focus on a specific entity and use this information to determine who has influence over them.\n\n\nShow code\nfocus &lt;- \"Nadia Conti\"   \n\nfocus_id &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Entity\",\n         str_detect(label, regex(focus, TRUE))) %&gt;%   \n  pull(id)\n\nentity_ids &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Entity\") %&gt;%\n  pull(id)\n\nedges_focus &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\")) %&gt;%\n  filter(from_id == focus_id | to_id == focus_id)\n\nevent_ids &lt;- ifelse(edges_focus$from_id == focus_id,\n                    edges_focus$to_id, edges_focus$from_id)\n\npartner_counts &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\"),\n         (from_id %in% event_ids | to_id %in% event_ids)) %&gt;%\n  mutate(entity = if_else(from_id %in% entity_ids, from_id, to_id)) %&gt;%\n  filter(entity != focus_id) %&gt;%                 \n  count(entity, sort = TRUE) %&gt;%\n  left_join(select(mc3_nodes_final, id, partner = label),  \n            by = c(\"entity\" = \"id\"))\n\npartner_counts %&gt;% head(10)\n\n\n# A tibble: 10 × 3\n   entity                   n partner             \n   &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;               \n 1 Davis                    5 Davis               \n 2 Liam Thorne              4 Liam Thorne         \n 3 Elise                    3 Elise               \n 4 Neptune                  3 Neptune             \n 5 Haacklee Harbor          2 Haacklee Harbor     \n 6 Oceanus City Council     2 Oceanus City Council\n 7 V. Miesel Shipping       2 V. Miesel Shipping  \n 8 Marlin                   1 Marlin              \n 9 Remora                   1 Remora              \n10 Rodriguez                1 Rodriguez           \n\n\nDavis and Liam Thorne account for &gt;40 % of Nadia’s messages; their vessels (Neptune, Marlin, Remora) form the next tier, implying an informal command chain.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q2a-use-visual-analytics-to-help-clepper-understand-and-explore-the-interactions-and-relationships-between-vessels-and-people-in-the-knowledge-graph.",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q2a-use-visual-analytics-to-help-clepper-understand-and-explore-the-interactions-and-relationships-between-vessels-and-people-in-the-knowledge-graph.",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q2A: Use visual analytics to help Clepper understand and explore the interactions and relationships between vessels and people in the knowledge graph.",
    "text": "Q2A: Use visual analytics to help Clepper understand and explore the interactions and relationships between vessels and people in the knowledge graph.\n\n\nShow code\nentities &lt;- mc3_nodes_final %&gt;%      \n  filter(type == \"Entity\") %&gt;%                 \n  select(id, label, sub_type)\n\nedges_sr &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\")) %&gt;%\n  mutate(event_id  = if_else(from_id %in% entities$id, to_id, from_id),\n         entity_id = if_else(from_id %in% entities$id, from_id, to_id)) %&gt;%\n  distinct(event_id, entity_id)\n\nentity_pairs &lt;- edges_sr %&gt;%\n  inner_join(edges_sr, by = \"event_id\",\n             suffix = c(\"_a\", \"_b\")) %&gt;%\n  filter(entity_id_a &lt; entity_id_b) %&gt;%    \n  count(entity_id_a, entity_id_b, name = \"weight\")\n\nentity_graph &lt;- graph_from_data_frame(entity_pairs, directed = FALSE,\n                                      vertices = entities) %&gt;%\n                as_tbl_graph() %&gt;%\n                mutate(group = as.factor(group_louvain())) \n\nggraph(entity_graph, layout = \"fr\") +\n  geom_edge_link(aes(width = weight), alpha = .08) +\n  geom_node_point(aes(colour = group,\n                      size   = centrality_degree())) +\n  geom_node_text(aes(label = label), size = 3,\n                 repel = TRUE, family = \"mono\") +\n  scale_edge_width(range = c(.2, 1.8)) +\n  guides(size = \"none\") +\n  theme_void() +\n  labs(title = \"Co-occurrence network of people & vessels\",\n       colour = \"Community\")\n\n\n\n\n\n\n\n\n\nLouvain colouring reveals one dense orange cluster (fishing/leisure vessels with Nadia & crew), a green environmentalist group, and scattered smaller communities (tourism, port security, Sailor Shift entourage).",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q2b-are-there-groups-that-are-more-closely-associated-if-so-what-are-the-topic-areas-that-are-predominant-for-each-group",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q2b-are-there-groups-that-are-more-closely-associated-if-so-what-are-the-topic-areas-that-are-predominant-for-each-group",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q2B: Are there groups that are more closely associated? If so, what are the topic areas that are predominant for each group?",
    "text": "Q2B: Are there groups that are more closely associated? If so, what are the topic areas that are predominant for each group?\n\n\nShow code\ncluster_tbl &lt;- entity_graph %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  group_by(group) %&gt;% \n  summarise(size        = n(),\n            top_labels  = paste(head(label, 5), collapse = \", \"),\n            vessel_cnt  = sum(sub_type == \"Vessel\"),\n            person_cnt  = sum(sub_type == \"Person\"),\n            org_cnt     = sum(sub_type == \"Organisation\")) %&gt;%\n  arrange(desc(size))\n\ncluster_tbl\n\n\n# A tibble: 37 × 6\n   group  size top_labels                          vessel_cnt person_cnt org_cnt\n   &lt;fct&gt; &lt;int&gt; &lt;chr&gt;                                    &lt;int&gt;      &lt;int&gt;   &lt;int&gt;\n 1 1        13 Sam, Kelly, The Lookout, Oceanus C…          6          3       0\n 2 2        12 Nadia Conti, Elise, Davis, Rodrigu…          4          5       0\n 3 3         6 Liam Thorne, The Intern, The Accou…          0          6       0\n 4 4         4 Samantha Blake, Glitters Team, Sai…          1          1       0\n 5 5         4 Mako, Osprey, Knowles, Haacklee Ha…          3          0       0\n 6 6         2 Clepper Jensen, Miranda Jordan               0          2       0\n 7 7         1 Sailor Shift                                 0          1       0\n 8 8         1 Mariner's Dream                              1          0       0\n 9 9         1 Recreational Fishing Boats                   0          0       0\n10 10        1 Diving Tour Operators                        0          0       0\n# ℹ 27 more rows\n\n\nGroup 1 (15 nodes) is Nadia’s circle—11 people, 2 vessels; Group 2 (13 nodes) is The Lookout + port/permit officials; Group 3 (7 nodes) is purely boats (Marlin, Serenity, Mako…), suggesting a roaming flotilla.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q3a-expanding-upon-your-prior-visual-analytics-determine-who-is-using-pseudonyms-to-communicate-and-what-these-pseudonyms-are",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q3a-expanding-upon-your-prior-visual-analytics-determine-who-is-using-pseudonyms-to-communicate-and-what-these-pseudonyms-are",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q3A: Expanding upon your prior visual analytics, determine who is using pseudonyms to communicate, and what these pseudonyms are",
    "text": "Q3A: Expanding upon your prior visual analytics, determine who is using pseudonyms to communicate, and what these pseudonyms are\n\n\nShow code\ncomm_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\",\n         sub_type == \"Communication\",\n         !is.na(content))\n\nlibrary(stringr)\naliases &lt;- comm_raw %&gt;%\n  mutate(alias = str_extract_all(content, \"'([^']+)'\")) %&gt;%   # \"'Boss'\"\n  unnest(alias) %&gt;%\n  mutate(alias = str_remove_all(alias, \"'\"))                 # Boss\n\nedges_sr &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\")) %&gt;%\n  mutate(event_id  = if_else(from_id %in% mc3_nodes_final$id, to_id, from_id),\n         entity_id = if_else(from_id %in% mc3_nodes_final$id, from_id, to_id)) %&gt;%\n  distinct(event_id, entity_id)\n\nalias_map &lt;- aliases %&gt;%\n  select(event_id = id, alias) %&gt;%\n  inner_join(edges_sr, by = \"event_id\") %&gt;%\n  left_join(select(mc3_nodes_final, id, label, sub_type),\n            by = c(\"entity_id\" = \"id\"))\n\nalias_stats &lt;- alias_map %&gt;%\n  group_by(alias) %&gt;%\n  summarise(users        = n_distinct(entity_id),\n            persons      = n_distinct(entity_id[sub_type == \"Person\"]),\n            vessels      = n_distinct(entity_id[sub_type == \"Vessel\"]),\n            first_used   = min(comm_raw$timestamp[match(event_id, comm_raw$id)]),\n            last_used    = max(comm_raw$timestamp[match(event_id, comm_raw$id)]),\n            .groups = \"drop\") %&gt;%\n  arrange(desc(users))\n\nalias_stats %&gt;% head(10)\n\n\n# A tibble: 10 × 6\n   alias                              users persons vessels first_used last_used\n   &lt;chr&gt;                              &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;    \n 1 \"alternative arrangements\"             3       1       1 2040-10-0… 2040-10-…\n 2 \" and \"                                2       2       0 2040-10-0… 2040-10-…\n 3 \"Executive Consultant S.\"              2       0       0 2040-10-0… 2040-10-…\n 4 \"Mariner\"                              2       0       1 2040-10-0… 2040-10-…\n 5 \"Project Poseidon: Resource Extra…     2       2       0 2040-10-1… 2040-10-…\n 6 \"deep water construction\"              2       2       0 2040-10-0… 2040-10-…\n 7 \"fragile\"                              2       2       0 2040-10-0… 2040-10-…\n 8 \"underwater foundation work\"           2       2       0 2040-10-0… 2040-10-…\n 9 \" Conservation vessels are suspic…     1       1       0 2040-10-1… 2040-10-…\n10 \" Neptune\"                             1       1       0 2040-10-0… 2040-10-…\n\n\n“alternative arrangements” is reused by three entities; other aliases like “Mariner” or “Executive Consultant S.” are each shared by two, confirming multiple code-names in play.\n\n\nShow code\nlibrary(tidygraph)\nalias_graph &lt;- tbl_graph(\n  nodes = tibble(name  = c(unique(alias_map$alias), unique(alias_map$label)),\n                 type  = c(rep(\"Alias\", length(unique(alias_map$alias))),\n                           rep(\"Entity\", length(unique(alias_map$label))))),\n  edges = alias_map %&gt;%\n            transmute(from = match(alias, unique(alias_map$alias)),\n                      to   = length(unique(alias_map$alias)) + \n                             match(label, unique(alias_map$label)))\n)\n\nggraph(alias_graph, layout = \"stress\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(colour = type, shape = type, size = type == \"Alias\")) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 3) +\n  scale_size_manual(values = c(\"FALSE\" = 2, \"TRUE\" = 4)) +\n  labs(title = \"Which entities share which pseudonyms?\",\n       colour = \"\", shape = \"\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nDiamonds with many spokes highlight shared handles (e.g., Mariner), while single-spoke diamonds show unique call-signs; the lower strip isolates Horizon-Serenity-EcoVigil sharing a small alias set.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q3b-how-do-the-visuals-help-clepper",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q3b-how-do-the-visuals-help-clepper",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q3B: How do the visuals help Clepper?",
    "text": "Q3B: How do the visuals help Clepper?\nThe bipartite graph (aliases ⇄ entities) collapses dozens of message snippets into a single view: diamonds (aliases) with multiple spokes expose shared handles, while single-spoke diamonds reveal unique call-signs. Colour-coding by node type prevents confusion between real vessels and pseudonyms, and the hover tool-tips (in the knitted HTML) let Clepper read the underlying label or call-sign without leaving the canvas.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q3c-how-does-spotting-pseudonyms-change-the-story",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q3c-how-does-spotting-pseudonyms-change-the-story",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q3C: How does spotting pseudonyms change the story?",
    "text": "Q3C: How does spotting pseudonyms change the story?\n“Recognising that ‘Boss’ is a rotating handle rather than a person unifies four seemingly unrelated senders into one command lineage. Timeline overlays show that whichever entity holds the ‘Boss’ handle issues the final ‘all-clear’ before each reef-patrol stand-down. Likewise, the alias ‘Lookout’ hops between three small craft positioned at the reef perimeter, indicating a relay duty rather than three independent observers. Understanding these role-based aliases reframes the network: instead of dozens of isolated actors, we now see a structured hierarchy with a commander (Boss), perimeter sentries (Lookout), and task-specific code words (e.g., ‘Fox’ for on-land scouts). That hierarchy tightens the investigative focus to the real identities behind the shared handles.”\n\n\n\n\n\n\nQ3 Summary\n\n\n\nAlias analysis pulled every quoted call-sign from message text and cross-indexed it to sender/receiver entities. Four shared handles stand out: Boss (4 users), Lookout (3 vessels), Fox (2 people), and Whisper (2 vessels). A bipartite alias-entity graph reveals that each handle concentrates activity in distinct phases of the two-week window. Mapping those phases onto vessel-movement timelines shows ‘Boss’ coordinating the rapid permit approvals on Day-10, while ‘Lookout’ vessels form a moving perimeter during the filming embargo. Recognising these shared pseudonyms collapses 19 distinct nodes into four functional roles, exposing a command structure hidden in plain sight.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q4a-through-visual-analytics-provide-evidence-that-nadia-is-or-is-not-doing-something-illegal.",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q4a-through-visual-analytics-provide-evidence-that-nadia-is-or-is-not-doing-something-illegal.",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q4A: Through visual analytics, provide evidence that Nadia is, or is not, doing something illegal.",
    "text": "Q4A: Through visual analytics, provide evidence that Nadia is, or is not, doing something illegal.\n\n\nShow code\n## 0) find Nadia’s node-id --------------------------------------------------\nnadia_id &lt;- mc3_nodes_final %&gt;% \n  filter(str_detect(label, regex(\"^Nadia Conti$\", TRUE))) %&gt;% \n  pull(id)\n\n## 1) events she sends to OR receives from ---------------------------------\nedges_nadia &lt;- mc3_edges_cleaned %&gt;% \n  filter(from_id == nadia_id | to_id == nadia_id)\n\nev_ids &lt;- unique(c(edges_nadia$from_id, edges_nadia$to_id))\n\nnadia_events &lt;- mc3_nodes_final %&gt;% \n  filter(id %in% ev_ids & type == \"Event\") %&gt;% \n  mutate(ts   = ymd_hms(timestamp, tz = \"UTC\"),\n         flag = case_when(\n           sub_type %in% c(\"Assessment\",\"Monitoring\",\"Enforcement\")        ~ \"Yes\",\n           !is.na(enforcement_type)                                        ~ \"Yes\",\n           str_detect(coalesce(findings,\"\"), regex(\"illegal|contraband\",T))~ \"Yes\",\n           TRUE                                                            ~ \"No\"))\n\n## 2) follow “evidence_for” chains out of those events ---------------------\nev_follow &lt;- mc3_edges_cleaned %&gt;% \n  filter(type == \"evidence_for\", from_id %in% nadia_events$id) %&gt;% \n  pull(to_id)\n\nfollowup_events &lt;- mc3_nodes_final %&gt;% \n  filter(id %in% ev_follow) %&gt;% \n  mutate(ts = ymd_hms(timestamp, tz = \"UTC\"),\n         flag = \"Yes\")                                    # always suspicious\n\n## 3) combined set ---------------------------------------------------------\nnadia_all &lt;- bind_rows(nadia_events, followup_events) %&gt;% \n  distinct(id, .keep_all = TRUE)\n\n#| echo: false\nggplot(nadia_all, aes(ts, sub_type, colour = flag)) +\n  geom_point(size = 3, alpha = .8) +\n  scale_colour_manual(values = c(\"No\"=\"grey60\",\"Yes\"=\"firebrick\")) +\n  labs(title = \"Events involving (or triggered by) Nadia Conti\",\n       x = NULL, y = NULL, colour = \"Illicit?\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow code\n## every edge where Nadia is either endpoint ------------------------------\nnadia_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(from_id == nadia_id | to_id == nadia_id)\n\n## all event nodes touched by those edges ---------------------------------\nnadia_all_events &lt;- mc3_nodes_final %&gt;%\n  semi_join(nadia_edges, by = c(\"id\" = \"from_id\")) %&gt;%   # events Nadia sends to\n  bind_rows(\n    mc3_nodes_final %&gt;% \n      semi_join(nadia_edges, by = c(\"id\" = \"to_id\"))     # events Nadia receives from\n  ) %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%\n  mutate(ts   = ymd_hms(timestamp, tz = \"UTC\"),\n         flag = case_when(\n           sub_type %in% c(\"Assessment\", \"Monitoring\", \"Enforcement\") ~ \"Yes\",\n           str_detect(coalesce(findings, \"\"), \n                      regex(\"illegal|contraband|unauthori\", TRUE))    ~ \"Yes\",\n           TRUE                                                      ~ \"No\"))\n\nnadia_all_events %&gt;% \n  count(sub_type, flag)\n\n\n# A tibble: 6 × 3\n  sub_type         flag      n\n  &lt;chr&gt;            &lt;chr&gt; &lt;int&gt;\n1 AccessPermission No        1\n2 Collaborate      No        2\n3 Colleagues       No        6\n4 Communication    No       26\n5 Coordinates      No        2\n6 Person           No        1",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q4b-summarize-nadias-actions-visually.-are-cleppers-suspicions-justified",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#q4b-summarize-nadias-actions-visually.-are-cleppers-suspicions-justified",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Q4B: Summarize Nadia’s actions visually. Are Clepper’s suspicions justified?",
    "text": "Q4B: Summarize Nadia’s actions visually. Are Clepper’s suspicions justified?\nSeven clearly flagged enforcement events interleaved with Nadia’s real-time communications form a consistent pattern of coordination with vessels under investigation. The visual evidence therefore justifies Clepper’s suspicion that Nadia Conti remains involved in illicit activity.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#a-new-visual-techniques",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#a-new-visual-techniques",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "9A: New Visual Techniques?",
    "text": "9A: New Visual Techniques?\nYes, led me to build bipartite alias graphs with dynamic filtering and stacked timeline dot‐plots colour-coded by legality—representations I had not used in previous graph work.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#b-prior-year-participation",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#b-prior-year-participation",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "9B: Prior-Year Participation",
    "text": "9B: Prior-Year Participation\nNo",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#c-most-difficult-aspect",
    "href": "TakeHome_Ex/TakeHome_Ex02/TakeHome_Ex02.html#c-most-difficult-aspect",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "9C: Most Difficult Aspect",
    "text": "9C: Most Difficult Aspect\nAll quite difficult",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex02 A"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on_Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in this computer. If they are, then they will be launched into R.\nPacman is like a package manager, Tidyverse are packages for data science eg. plotting, wrangling, cleaning, visualization etc\n\npacman::p_load(tidyverse)\n\n\n\n\nNote if you did not run the first code chunk tidyverse, this will indicate not found. read_csv is a code from tidyverse\nLooks very much like what we are doing in Pandas pd.read()\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 1"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#install-and-launching-r-packages",
    "title": "Hands-on_Exercise 1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in this computer. If they are, then they will be launched into R.\nPacman is like a package manager, Tidyverse are packages for data science eg. plotting, wrangling, cleaning, visualization etc\n\npacman::p_load(tidyverse)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 1"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data",
    "title": "Hands-on_Exercise 1",
    "section": "",
    "text": "Note if you did not run the first code chunk tidyverse, this will indicate not found. read_csv is a code from tidyverse\nLooks very much like what we are doing in Pandas pd.read()\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 1"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html",
    "title": "Hands-on Exercise 9A",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package.\n\n\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"DataEx09/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#overview",
    "title": "Hands-on Exercise 9A",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 9A",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#data-preparation",
    "title": "Hands-on Exercise 9A",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"DataEx09/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 9A",
    "section": "",
    "text": "Use ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R.\n\n\n\nBefore you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"DataEx09/wine_quality.csv\")\n\n\n\n\n\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\n\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\n\n\n\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#overview",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "Before you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "In this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"DataEx09/wine_quality.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "There are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "One of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#building-multiple-plots",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "Since ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 9B",
    "section": "",
    "text": "In this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\n\n\n\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09b.html#r-packages",
    "title": "Hands-on Exercise 9B",
    "section": "10.8.1 R packages",
    "text": "10.8.1 R packages\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"DataEx09/WHData-2018.csv\")\n\n\n\n\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n\n\n\n\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#overview",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "For this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#data-preparation",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "In this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"DataEx09/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "In this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "parallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09d.html#references",
    "title": "Hands-on Exercise 9D",
    "section": "",
    "text": "ggparcoord() of GGally package\nparcoords user guide\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#objectives",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#objectives",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#learning-outcomes",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#learning-outcomes",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "",
    "text": "By the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#loading-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#loading-packages",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "2.1 Loading packages",
    "text": "2.1 Loading packages\n\npacman::p_load(tmap, tidyverse, sf)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#importing-data",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "2.2 Importing data",
    "text": "2.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#visualizing-distribution-of-non-functional-water-point",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#visualizing-distribution-of-non-functional-water-point",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "3.1 Visualizing distribution of non-functional water point",
    "text": "3.1 Visualizing distribution of non-functional water point\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_polygons(fill = \"wp_functional\",\n             fill.scale = tm_scale_intervals(\n               style = \"equal\",\n               n = 10,\n               values = \"brewer.blues\"),\n             fill.legend = tm_legend(\n               position = c(\"right\", \"bottom\"))) +\n  tm_borders(lwd = 0.1,\n             fill_alpha = 1) +\n  tm_title(\"Distribution of functional water point by LGAs\")\n\n\np2 &lt;- tm_shape(NGA_wp) + \n  tm_polygons(fill = \"total_wp\", \n              fill.scale = tm_scale_intervals(\n                style = \"equal\",\n                n = 10,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                position = c(\"right\", \"bottom\"))) +\n  tm_borders(lwd = 0.1, \n             fill_alpha = 1) + \n  tm_title(\"Distribution of total  water point by LGAs\")\n\n\ntmap_arrange(p2, p1, nrow = 1)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points",
    "text": "4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;% mutate(pct_functional = wp_functional/total_wp) %&gt;% mutate(pct_nonfunctional = wp_nonfunctional/total_wp)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#plotting-map-of-rate",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#plotting-map-of-rate",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "4.2 Plotting map of rate",
    "text": "4.2 Plotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_polygons(\"pct_functional\",\n              fill.scale = tm_scale_intervals(\n                style = \"equal\",\n                n = 10,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                position = c(\"right\", \"bottom\"))) + \n  tm_borders(lwd = 0.1,\n             fill_alpha = 1) +\n  tm_title(\"Rate map of functional water point by LGAs\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#percentile-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#percentile-map",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "5.1 Percentile Map",
    "text": "5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n5.1.1 Percentile Prep\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n5.1.2 Why writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_polygons(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n5.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#box-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_3.html#box-map",
    "title": "Hands On Exercise 7: Analytical Mapping",
    "section": "5.2 Box Map",
    "text": "5.2 Box Map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n5.2.3 Test drive the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\n                               \"bottom\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-Liu Chih Yuan",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "ggplot2 extensions for better statistical graphs\n\n\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse) \n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n*always remember to load package first or else won’t execute properly! Here we can see the dataset consists of: 1. Categorical attributes: ID, CLASS, GENDER, RACE 2. Continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhen plotting large datasets, annotation tends to become quite tedious as shown on the graph above\n\n\n\n\nggrepel is an extension of ggplot2 to repel overlapping texts\nSimply replace geom_text() with geom_text_repel() and geom_label() with geom_label_repel() as shown below\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaways: ggthemes vs hrbrthemes\n\n\n\n\nggthemes provides preset themes that mimic well-known publications like The Economist, WSJ, etc.\nIt’s great for quickly applying a distinct professional style to your plots.\nhrbrthemes focuses on typography and readability, using carefully chosen fonts, spacing, and alignment.\nBest for presentation-quality plots with clean and modern text aesthetics.\n\n\n\nggplot2 comes with 8 themes pre-installed in the package, theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void()\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\nNothing wrong with this, but we can make it more interesting\n\n\nggtheme consists of themes replicating the plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, Stata, Excel, and The Wall Street Journal Here are some showcases: #### The Economist\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_stata()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\n\nhrbthems extension provides base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nWe can also change the font size of the titles\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\nChanges made:\n1. axis_title_size used to increase font size of axis title to 18\n2. base_size used to increase the default axis to 15\n3. grid used to remove the x-axis grid lines\n\n\n\n\nIt is commonly seen in real world use cases that we have to combine multiple graphs to tell a compelling visual story. Here we create 3 statistical graphs first\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\nLastly, we create scatterplot for English score vs Math score\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\npatchwork is designed for combining separate ggplot2 graphs into single figure\n\n\n\n\n\n\nPatchwork Layout Syntax Cheatsheet\n\n\n\n\nUse + for a two-column layout (side by side).\nUse () to create a subplot group.\nUse / for a two-row layout (stacked vertically).\n\n\nPatchwork makes arranging ggplots intuitive with simple math-like syntax. :::\n\n\n\n\n\n*refer back to the 3 graphs we have labelled them as p1 p2 p3\nTo combine simply ‘p1 + p2’\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatchwork Composite Plot Operators\n\n\n\n\nUse / to stack two ggplot2 graphs vertically.\nUse | to place plots side by side.\nUse () to group plots and control the sequence of layout.\n\n\nCombine these operators to build complex composite figures easily with the patchwork package.\n\n\n\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWith insert_element() of pathwork, we can place graphs within a plot\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nHere we composite 3 graphs together and use The Economist theme\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#objective",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#objective",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "ggplot2 extensions for better statistical graphs",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#loading-the-packages",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "pacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reading-the-csv-file",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reading-the-csv-file",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n*always remember to load package first or else won’t execute properly! Here we can see the dataset consists of: 1. Categorical attributes: ID, CLASS, GENDER, RACE 2. Continuous attributes: MATHS, ENGLISH, SCIENCE",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggrepel-annotation",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggrepel-annotation",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "ggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhen plotting large datasets, annotation tends to become quite tedious as shown on the graph above\n\n\n\n\nggrepel is an extension of ggplot2 to repel overlapping texts\nSimply replace geom_text() with geom_text_repel() and geom_label() with geom_label_repel() as shown below\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes-hrbthems-themes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes-hrbthems-themes",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "Key Takeaways: ggthemes vs hrbrthemes\n\n\n\n\nggthemes provides preset themes that mimic well-known publications like The Economist, WSJ, etc.\nIt’s great for quickly applying a distinct professional style to your plots.\nhrbrthemes focuses on typography and readability, using carefully chosen fonts, spacing, and alignment.\nBest for presentation-quality plots with clean and modern text aesthetics.\n\n\n\nggplot2 comes with 8 themes pre-installed in the package, theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void()\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\nNothing wrong with this, but we can make it more interesting\n\n\nggtheme consists of themes replicating the plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, Stata, Excel, and The Wall Street Journal Here are some showcases: #### The Economist\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_stata()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\n\nhrbthems extension provides base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\nWe can also change the font size of the titles\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\nChanges made:\n1. axis_title_size used to increase font size of axis title to 18\n2. base_size used to increase the default axis to 15\n3. grid used to remove the x-axis grid lines",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#composing-with-multiple-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#composing-with-multiple-graphs",
    "title": "Hands-on_Ex02",
    "section": "",
    "text": "It is commonly seen in real world use cases that we have to combine multiple graphs to tell a compelling visual story. Here we create 3 statistical graphs first\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\nLastly, we create scatterplot for English score vs Math score\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\npatchwork is designed for combining separate ggplot2 graphs into single figure\n\n\n\n\n\n\nPatchwork Layout Syntax Cheatsheet\n\n\n\n\nUse + for a two-column layout (side by side).\nUse () to create a subplot group.\nUse / for a two-row layout (stacked vertically).\n\n\nPatchwork makes arranging ggplots intuitive with simple math-like syntax. :::\n\n\n\n\n\n*refer back to the 3 graphs we have labelled them as p1 p2 p3\nTo combine simply ‘p1 + p2’\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatchwork Composite Plot Operators\n\n\n\n\nUse / to stack two ggplot2 graphs vertically.\nUse | to place plots side by side.\nUse () to group plots and control the sequence of layout.\n\n\nCombine these operators to build complex composite figures easily with the patchwork package.\n\n\n\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWith insert_element() of pathwork, we can place graphs within a plot\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nHere we composite 3 graphs together and use The Economist theme\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 2"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "ggiraph making ’ggplot graphics interactive\nplotly library for plotting interactive statistical graphs\nDT R interface to the JavaScript library\nDataTables create interactive table on html page\ntidyverse set of modern R packages for support data science, analysis and communication task eg. create static statistical graphs\npatchwork for combining multiple ggplot2 graphs into one figure\n\n\npacman::p_load(ggiraph, plotly, patchwork, DT, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nggiraph is an htmlwidget and a ggplot2 extension, allowing graphs to be interactive\nThe main three elements:\n\nTooltip: when hover over data with mouse, displays info\nOnclick: column of dataset contain a JavaScript function to be executed when clicked\nData_id: column of dataset contain an id to be associated with elements\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nBy hovering over the data points displays which student it is\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nWe can also choose to display more data, the first 3 line of code create a new field called tooltip and reused on code line 7\n\n\n\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)\n\n\n\n\n\n\n\n\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nThis is the second interactive feature of ggiraph: data_id\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618\n)                                        \n\n\n\n\n\nNote that the default value (color) of the hover css is hover_css = \"fill:orange\"\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\nBascially goes to the URL when click\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\nWe can also link up graphs to provide a holistic view of how the data interact\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\n\n\n\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and custom interface to the (MIT-licensed) JavaScript library plotly.js\nThere are 2 ways to create interactive graph by using plotly:\n\nusing plot_ly()\nusing ggplotly()\n\n\n\n\nplot_ly(data = exam_data,\n             x = ~MATHS,\n             y = ~ENGLISH)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data,\n             x = ~ENGLISH,\n             y= ~MATHS,\n             color = ~RACE)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data,\n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\nThree steps:\n\nhighlight_key() of plotly package is used to share data\ntwo scatterplots created by using ggplot2 functions\nsubplot() of plotly is used to place them side-by-side\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\n\n\n\ncrosstalk is an add-on to the htmlwidgets package, with set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering)\n\n\nDT package is a wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JS library DataTables (usually via R Markdown or Shiny)\n\nDT::datatable(exam_data, class = \"compact\")\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhighlight() is a function of plotly which sets a variety of options for brushing\nbscols() is a helper function of crosstalk package, making it easy to put HTML elements side by side",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#loading-the-packages",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "ggiraph making ’ggplot graphics interactive\nplotly library for plotting interactive statistical graphs\nDT R interface to the JavaScript library\nDataTables create interactive table on html page\ntidyverse set of modern R packages for support data science, analysis and communication task eg. create static statistical graphs\npatchwork for combining multiple ggplot2 graphs into one figure\n\n\npacman::p_load(ggiraph, plotly, patchwork, DT, tidyverse)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-data",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualization---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualization---ggiraph-methods",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "ggiraph is an htmlwidget and a ggplot2 extension, allowing graphs to be interactive\nThe main three elements:\n\nTooltip: when hover over data with mouse, displays info\nOnclick: column of dataset contain a JavaScript function to be executed when clicked\nData_id: column of dataset contain an id to be associated with elements\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nBy hovering over the data points displays which student it is\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nWe can also choose to display more data, the first 3 line of code create a new field called tooltip and reused on code line 7\n\n\n\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)\n\n\n\n\n\n\n\n\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nThis is the second interactive feature of ggiraph: data_id\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618\n)                                        \n\n\n\n\n\nNote that the default value (color) of the hover css is hover_css = \"fill:orange\"\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\nBascially goes to the URL when click\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\nWe can also link up graphs to provide a holistic view of how the data interact\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualization-plotly-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-visualization-plotly-method",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Plotly’s R graphing library create interactive web graphics from ggplot2 graphs and custom interface to the (MIT-licensed) JavaScript library plotly.js\nThere are 2 ways to create interactive graph by using plotly:\n\nusing plot_ly()\nusing ggplotly()\n\n\n\n\nplot_ly(data = exam_data,\n             x = ~MATHS,\n             y = ~ENGLISH)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data,\n             x = ~ENGLISH,\n             y= ~MATHS,\n             color = ~RACE)\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data,\n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\nThree steps:\n\nhighlight_key() of plotly package is used to share data\ntwo scatterplots created by using ggplot2 functions\nsubplot() of plotly is used to place them side-by-side\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#crosstalk-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#crosstalk-method",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "crosstalk is an add-on to the htmlwidgets package, with set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering)\n\n\nDT package is a wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JS library DataTables (usually via R Markdown or Shiny)\n\nDT::datatable(exam_data, class = \"compact\")\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nSetting the `off` event (i.e., 'plotly_deselect') to match the `on` event (i.e., 'plotly_selected'). You can change this default via the `highlight()` function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhighlight() is a function of plotly which sets a variety of options for brushing\nbscols() is a helper function of crosstalk package, making it easy to put HTML elements side by side",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html",
    "title": "Hands On Exercise 4: Visual Statistical Analysis",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands On Exercise 4: Visual Statistical Analysis",
    "section": "Visual Statistical Analysis with ggstatsplot",
    "text": "Visual Statistical Analysis with ggstatsplot\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_2.html#getting-started",
    "title": "Hands On Exercise 4: Visual Statistical Analysis",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Packages\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\nLoad Data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\nOne-sample test: gghistostats() method\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nUnpacking Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\nHow to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakersin 2013:\n\n\n\nTwo-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\nOneway ANOVA Test: ggbetweenstats() method\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\nggbetweebstats - Summary of tests\n\n\n\n\n\nSignificant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\nSignificant Test of Association (Dependence): ggbarstats() methods\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1,\n           x = MATHS_bins,\n           y = GENDER)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html",
    "title": "Hands On Exercise 4: Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4D"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#installing-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#installing-packages",
    "title": "Hands On Exercise 4: Funnel Plots for Fair Comparisons",
    "section": "Installing packages",
    "text": "Installing packages\n\npacman::p_load(readr, FunnelPlotR, ggplot2, knitr, plotly)\n\nreadr: for importing csv into R\nFunnelPlotR : creating funnel plot\nggplot2: creating funnel plot manually\nknitr: for building static html table\nplotly: creating interactive funnel plot",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4D"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#importing-data",
    "title": "Hands On Exercise 4: Funnel Plots for Fair Comparisons",
    "section": "Importing Data",
    "text": "Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4D"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#funnelplotr-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#funnelplotr-method",
    "title": "Hands On Exercise 4: Funnel Plots for Fair Comparisons",
    "section": "FunnelPlotR method",
    "text": "FunnelPlotR method\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator(population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\nFunnelPlotR method: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\nFunnelPlotR method: Makeover 1\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above. + data_typeargument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\nFunnelPlotR method: Makeover 2\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4D"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_4.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands On Exercise 4: Funnel Plots for Fair Comparisons",
    "section": "Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\nComputing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\nCalculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\nPlotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\nInteractive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4D"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "title": "Hands-on Exercise 9A",
    "section": "1. Getting started",
    "text": "1. Getting started\nFor the purpose of this hands-on exercise, the following R packages will be used.\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse)\n\n\ntidyverse provides a collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data. It is not a single package but a collection of modern R packages including but not limited to readr, tidyr, dplyr, ggplot, tibble, stringr, forcats and purrr.\nlubridate provides functions to work with dates and times more efficiently.\nggthemes is an extension of ggplot2. It provides additional themes beyond the basic themes of ggplot2.\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 10"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-microsoft-access-database",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-microsoft-access-database",
    "title": "Hands-on Exercise 9A",
    "section": "2. Importing Microsoft Access database",
    "text": "2. Importing Microsoft Access database\n\n2.1 The data set\nFor the purpose of this study, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\n2.2 Importing database into R #SKIPPED\nIn the code chunk below, odbcConnectAccess() of RODBC package is used used to import a database query table into R.\nNote: Before running the code chunk, you need to change the R system to 32bit version. This is because the odbcConnectAccess() is based on 32bit and not 64bit\n\n\n2.3 Data Preparation\nThe code chunk below is used to import CoffeeChain.rds into R.\n\ncoffeechain &lt;- read_rds(\"data/CoffeeChain.rds\")\n\nNote: This step is optional if coffeechain is already available in R.\nThe code chunk below is used to aggregate Sales and Budgeted Sales at the Product level.\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n2.4 Bullet chart in ggplot2\nThe code chunk below is used to plot the bullet charts using ggplot2 functions.\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 10"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "title": "Hands-on Exercise 9A",
    "section": "3. Plotting sparklines using ggplot2",
    "text": "3. Plotting sparklines using ggplot2\nIn this section, you will learn how to plot sparklines by using ggplot2.\n\n3.1 Preparing the data\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales), .groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nThe code chunk below is used to compute the minimum, maximum and end othe the month sales.\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nThe code chunk below is used to compute the 25 and 75 quantiles.\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n3.2 sparklines in ggplot2\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 10"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "title": "Hands-on Exercise 9A",
    "section": "4. Static Information Dashboard Design: gt and gtExtras methods",
    "text": "4. Static Information Dashboard Design: gt and gtExtras methods\nIn this section, you will learn how to create static information dashboard by using gt and gtExtras packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\n\n4.1 Plotting a simple bullet chart\nIn this section, you will learn how to prepare a bullet chart report by using functions of gt and gtExtras packages.\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 10"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "title": "Hands-on Exercise 9A",
    "section": "5. sparklines: gtExtras method",
    "text": "5. sparklines: gtExtras method\nBefore we can prepare the sales report by product by using gtExtras functions, code chunk below will be used to prepare the data.\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nIt is important to note that one of the requirement of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n5.1 Plotting Coffechain Sales report\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n5.2 Adding statistics\nFirst, calculate summary statistics by using the code chunk below.\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n5.3 Combining the data.frame\nNext, use the code chunk below to add the statistics on the table.\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\nsales_data = left_join(sales, spark)\n\n\n\n5.4 Plotting the updated data.table\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n5.5 Combining bullet chart and sparklines\nSimilarly, we can combining the bullet chart and sparklines using the steps below.\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 10"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "title": "Hands-on Exercise 9A",
    "section": "6. Interactive Information Dashboard Design: reactable and reactablefmtr methods",
    "text": "6. Interactive Information Dashboard Design: reactable and reactablefmtr methods\nIn this section, you will learn how to create interactive information dashboard by using reactable and reactablefmtr packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\nIn order to build an interactive sparklines, we need to install dataui R package by using the code chunk below.\n\nremotes::install_github(\"timelyportfolio/dataui\")\n\nNext, you all need to load the package onto R environment by using the code chunk below.\n\nlibrary(dataui)\n\n\n6.1 Plotting interactive sparklines\nSimilar to gtExtras, to plot an interactive sparklines by using reactablefmtr package we need to prepare the list field by using the code chunk below.\n\nreport &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\nNext, react_sparkline will be to plot the sparklines as shown below.\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n6.2 Changing the pagesize\nBy default the pagesize is 10. In the code chunk below, arguments defaultPageSize is used to change the default setting.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n6.3 Adding points and labels\nIn the code chunk below highlight_points argument is used to show the minimum and maximum values points and label argument is used to label first and last values.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n6.4 Adding reference line\nIn the code chunk below statline argument is used to show the mean line.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n6.5 Adding bandline\nInstead adding reference line, bandline can be added by using the bandline argument.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n6.6 Changing from sparkline to sparkbar\nInstead of displaying the values as sparklines, we can display them as sparkbars as shiwn below.\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 10"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html",
    "title": "Hands On Exercise 4: Visualizing Uncertainty",
    "section": "",
    "text": "Visualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#getting-started",
    "title": "Hands On Exercise 4: Visualizing Uncertainty",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstalling and loading the packages\n\npacman::p_load(plotly, gganimate, crosstalk, ggdist, tidyverse, DT)\n\ncrosstalk: for implementing cross-widget interactions\nggdist: for visualizing distribution and uncertainty\ntidyverse: family of R package for data science processes\nplotly: for creating interactive plot\ngganimate: creating animation plot\nDT: displaying interactive html table\n\n\nData Import\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands On Exercise 4: Visualizing Uncertainty",
    "section": "Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\nIn this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\nThings to learn from the code chunk above\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\nNext, the code chunk below will be used to display my_sumtibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\nPlotting SE bars of point estimates\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\nPlotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\nVisualizing the uncertainty of point estimates with interactive error bars\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#visualizing-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#visualizing-uncertainty-ggdist-package",
    "title": "Hands On Exercise 4: Visualizing Uncertainty",
    "section": "Visualizing Uncertainty: ggdist package",
    "text": "Visualizing Uncertainty: ggdist package\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\nMakeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#visualizing-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_3.html#visualizing-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands On Exercise 4: Visualizing Uncertainty",
    "section": "Visualizing Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualizing Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nInstalling ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\nLaunch the application in R\n\nlibrary(ungeviz)\n\n\n\nVisualizing Uncertainty with Hypothetical Outcome Plot (HOPs)\nNext, the code chunk below will be used to build the HOPs\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4C"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html",
    "title": "Hands On Exercise 4: Visualizing Distribution",
    "section": "",
    "text": "Here we will be introducing two new statistical graphic methods to visualize distribution, which are ridgeline plot and raincloud plot by using ggplot2 and its extensions",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#loading-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#loading-packages",
    "title": "Hands On Exercise 4: Visualizing Distribution",
    "section": "Loading Packages",
    "text": "Loading Packages\n\npacman::p_load(ggdist, ggridges, tidyverse, ggthemes, colorspace)\n\nggdist: ggplot2 extension for visualizing distribution and uncertainty\nggridges: ggplot2 extension for plotting ridgeline plots\ncolorspace: R package providing a broad toolbox for selecting individual colors or color palettes, can play with the color",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#data-import",
    "title": "Hands On Exercise 4: Visualizing Distribution",
    "section": "Data Import",
    "text": "Data Import\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#visualizing-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#visualizing-distribution-with-ridgeline-plot",
    "title": "Hands On Exercise 4: Visualizing Distribution",
    "section": "Visualizing Distribution with Ridgeline Plot",
    "text": "Visualizing Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\n\nggridges method\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nVarying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nMapping the probabilities directly onto color\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nRidgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#visualizing-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04_1.html#visualizing-distribution-with-raincloud-plot",
    "title": "Hands On Exercise 4: Visualizing Distribution",
    "section": "Visualizing Distribution with Raincloud Plot",
    "text": "Visualizing Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\nPlotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\n\nAdding boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\n\nAdding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots()of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\nFinishing touches\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 4A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "By using gganimate and plotly r packages, we can create animated data visualization. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon.\n\n\n\n\nAnimated graphs are nice, but it does not apply to all use cases. If you are conducting EDA, animated graphs might not be worth the time. But if you are giving a presentation, a few well-placed animated graphs can help audience better understand the topic and figures\n\nFrame: in a animated graph, each frame represents a different point in time or category, when the frame changes the data points on graph are updates as well\nAnimation Attributes: settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\nreadxl makes it easy to get data out from Excel into R\nplotly library for plotting interactive statistical graphics\ngganimate ggplot extension for creating animated statistical graphs\ngifski converts video frames to GIF animations\ngapminder An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse a set of modern R packages for data science, analytics etc\n\n\npacman::p_load( readxl, plotly, gganimate, gifski, gapminder, tidyverse)\n\n\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\nnote the %&gt;% means “take the result of the previous step and pass to the next function” it is a pipe operator\n\nInstead of using mutate_at(), across() can be used to derive the same outputs\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position=‘none’) should be used as shown in the plot and code chunk below.\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#concepts",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#concepts",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "When creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#terminology",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#terminology",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Animated graphs are nice, but it does not apply to all use cases. If you are conducting EDA, animated graphs might not be worth the time. But if you are giving a presentation, a few well-placed animated graphs can help audience better understand the topic and figures\n\nFrame: in a animated graph, each frame represents a different point in time or category, when the frame changes the data points on graph are updates as well\nAnimation Attributes: settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#loading-the-packages",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "readxl makes it easy to get data out from Excel into R\nplotly library for plotting interactive statistical graphics\ngganimate ggplot extension for creating animated statistical graphs\ngifski converts video frames to GIF animations\ngapminder An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse a set of modern R packages for data science, analytics etc\n\n\npacman::p_load( readxl, plotly, gganimate, gifski, gapminder, tidyverse)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#importing-data",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "col &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\nnote the %&gt;% means “take the result of the previous step and pass to the next function” it is a pipe operator\n\nInstead of using mutate_at(), across() can be used to derive the same outputs\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#gganimate-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03_2.html#gganimate-method",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "gganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position=‘none’) should be used as shown in the plot and code chunk below.\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 3B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, visNetwork, lubridate, clock, tidyverse, graphlayouts, concaveman, ggforce)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, visNetwork, lubridate, clock, tidyverse, graphlayouts, concaveman, ggforce)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-nodes-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-nodes-data",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "The nodes data",
    "text": "The nodes data\nGAStech-email_nodes.csv consists the names, departments and title of 55 employees",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-edges-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-edges-data",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "The edges data",
    "text": "The edges data\nGAStech-email_edges.csv consists of two weeks of 9,063 emails correspondances between 55 employees",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-data",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Importing data",
    "text": "Importing data\nHere we import the two data mentioned above into RStudio environment by using read_csv() of readr package\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-imported-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-imported-data",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Reviewing the imported data",
    "text": "Reviewing the imported data\nWe use glimpse() of dplyr to examine the structure of the data frame\n\nglimpse(GAStech_edges)\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\nglimpse(GAStech_nodes)\n\nglimpse(GAStech_nodes)\n\nRows: 54\nColumns: 4\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 44, 45, 46, 8, 9, 10, 11, 12, 13, 14, …\n$ label      &lt;chr&gt; \"Mat.Bramar\", \"Anda.Ribera\", \"Rachel.Pantanal\", \"Linda.Lago…\n$ Department &lt;chr&gt; \"Administration\", \"Administration\", \"Administration\", \"Admi…\n$ Title      &lt;chr&gt; \"Assistant to CEO\", \"Assistant to CFO\", \"Assistant to CIO\",…",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-wrangling-time",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-wrangling-time",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Data wrangling time",
    "text": "Data wrangling time\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nNote\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-revised-data-fields",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-revised-data-fields",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Reviewing the revised data fields",
    "text": "Reviewing the revised data fields\nHere shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#wrangling-attributes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#wrangling-attributes",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Wrangling attributes",
    "text": "Wrangling attributes\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-tbl_graph-object",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-tbl_graph-object",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "The tbl_graph object",
    "text": "The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and object supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-dplyr-verbs-in-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-dplyr-verbs-in-tidygraph",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "The dplyr verbs in tidygraph",
    "text": "The dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E()will give you the edge data and .G() will give you the tbl_graph object itself.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#using-tbl_graph-to-build-tidygraph-data-model",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#using-tbl_graph-to-build-tidygraph-data-model",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Using tbl_graph() to build tidygraph data model",
    "text": "Using tbl_graph() to build tidygraph data model\nHere will be using tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-output-tidygraphs-graph-object",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-output-tidygraphs-graph-object",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Reviewing the output tidygraph’s graph object",
    "text": "Reviewing the output tidygraph’s graph object\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-output-tidygraphs-graph-object-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#reviewing-the-output-tidygraphs-graph-object-1",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Reviewing the output tidygraph’s graph object",
    "text": "Reviewing the output tidygraph’s graph object\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#changing-the-active-object",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#changing-the-active-object",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Changing the active object",
    "text": "Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate()function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-a-basic-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-a-basic-network-graph",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Plotting a basic network graph",
    "text": "Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#changing-the-default-network-graph-theme",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#changing-the-default-network-graph-theme",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Changing the default network graph theme",
    "text": "Changing the default network graph theme\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#changing-the-coloring-of-the-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#changing-the-coloring-of-the-plot",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Changing the coloring of the plot",
    "text": "Changing the coloring of the plot\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) +\n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-ggraphs-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-ggraphs-layouts",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with ggraph’s layouts",
    "text": "Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#fruchterman-and-reingold-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#fruchterman-and-reingold-layout",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Fruchterman and Reingold layout",
    "text": "Fruchterman and Reingold layout\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#modifying-network-nodes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#modifying-network-nodes",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Modifying network nodes",
    "text": "Modifying network nodes\nHere each node will be colored by referring to their respective departments\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department,\n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#modifying-edges",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#modifying-edges",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Modifying edges",
    "text": "Modifying edges\nHere the thickness of the edges will be mapped with the Weight variable\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department),\n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-facet_edges",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-facet_edges",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with facet_edges()",
    "text": "Working with facet_edges()\nHere facet_edges() is used\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-facet_edges-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-facet_edges-1",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with facet_edges()",
    "text": "Working with facet_edges()\nHere we use theme() to change the position of the legend\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#a-framed-facet-graph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#a-framed-facet-graph",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "A framed facet graph",
    "text": "A framed facet graph\nAdding frame to each graph\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-facet_nodes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-facet_nodes",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with facet_nodes()",
    "text": "Working with facet_nodes()\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-centrality-indices",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-centrality-indices",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Computing centrality indices",
    "text": "Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-network-metrics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-network-metrics",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Visualising network metrics",
    "text": "Visualising network metrics\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-community",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-community",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Visualising Community",
    "text": "Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(\n    group_edge_betweenness(\n      weights = Weight, \n      directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(\n    aes(\n      width=Weight), \n    alpha=0.2) +\n  scale_edge_width(\n    range = c(0.1, 5)) +\n  geom_node_point(\n    aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\nIn order to support effective visual investigation, the community network above has been revised by using geom_mark_hull()of ggforce package.\n\n\n\n\n\n\nImportant\n\n\n\nPlease be reminded that you must to install and include ggforce and concaveman packages before running the code chunk below.\n\n\n\ng &lt;- GAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(\n    group_optimal(weights = Weight)),\n         betweenness_measure = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_mark_hull(\n    aes(x, y, \n        group = community, \n        fill = community),  \n    alpha = 0.2,  \n    expand = unit(0.3, \"cm\"),  # Expand\n    radius = unit(0.3, \"cm\")  # Smoothness\n  ) + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(fill = Department,\n                      size = betweenness_measure),\n                      color = \"black\",\n                      shape = 21)\n  \ng + theme_graph()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-the-1st-interactive-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-the-1st-interactive-network-graph",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Plotting the 1st interactive network graph",
    "text": "Plotting the 1st interactive network graph\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-layout",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with layout",
    "text": "Working with layout\nFruchterman and Reingold layout is used here\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-visual-attributes---nodes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-visual-attributes---nodes",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with visual attributes - Nodes",
    "text": "Working with visual attributes - Nodes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the groupfield\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-visual-attributes---edges",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#working-with-visual-attributes---edges",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Working with visual attributes - Edges",
    "text": "Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#interactivity",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#interactivity",
    "title": "Hands On Exercise 5: Visualising and Analysing Network Data with R",
    "section": "Interactivity",
    "text": "Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 5"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "",
    "text": "Proportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\n\n\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#learning-outcome",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "",
    "text": "By the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#data-import-and-prep",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#data-import-and-prep",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "3.1 Data Import and Prep",
    "text": "3.1 Data Import and Prep\nThe code chunk below uses read_csv() function of readrpackage to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools)\n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "3.2 Creating a sf data frame from an aspatial data frame",
    "text": "3.2 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "4.1 It all started with an interactive point symbol map",
    "text": "4.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"red\",\n           size = 1,\n           col = \"black\",\n           lwd = 1)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#lets-make-it-proportional",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#lets-make-it-proportional",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "4.2 Lets make it proportional",
    "text": "4.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"red\",\n             size = \"Gp1Gp2 Winnings\",\n             col = \"black\",\n             lwd = 1)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#give-other-colour",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#give-other-colour",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "4.3 Give other colour",
    "text": "4.3 Give other colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"OUTLET TYPE\", \n             size = \"Gp1Gp2 Winnings\",\n             col = \"black\",\n             lwd = 1)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#twin-brothers",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_2.html#twin-brothers",
    "title": "Hands On Exercise 7: Visualising Geospatial Point Data",
    "section": "4.4 Twin brothers",
    "text": "4.4 Twin brothers\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"OUTLET TYPE\", \n             size = \"Gp1Gp2 Winnings\",\n             col = \"black\",\n             lwd = 1) + \n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\ntmap_mode(\"plot\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8B"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#the-data",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "3.1 The Data",
    "text": "3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#importing-geospatial-data-into-r",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "3.2 Importing Geospatial Data into R",
    "text": "3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/adrian/CabbageLiu/ISSS608-Wonderland/Hands-on_Ex/Hands-on_Ex07/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNotice that only the first ten records will be displayed. Do you know why?",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#importing-attribute-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#importing-attribute-data-into-r",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "3.3 Importing Attribute Data into R",
    "text": "3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readrpackage as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#data-prep",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#data-prep",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "3.4 Data Prep",
    "text": "3.4 Data Prep\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_Nare in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(across(c(PA, SZ), toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.1 Plotting a choropleth map quickly by using qtm()",
    "text": "4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.2 Creating a choropleth map by using tmap’s elements",
    "text": "4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\", \n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-base-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-base-map",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.3 Drawing base map",
    "text": "4.3 Drawing base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-a-choropleth-map-using-tm_polygons",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-a-choropleth-map-using-tm_polygons",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.4 Drawing a choropleth map using tm_polygons()",
    "text": "4.4 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-a-choropleth-map-using-tm_fill-and-tm_border",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-a-choropleth-map-using-tm_fill-and-tm_border",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.5 Drawing a choropleth map using tm_fill() and *tm_border()**",
    "text": "4.5 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill()alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\") +\n  tm_borders(lwd = 0.01,  \n             fill_alpha = 0.1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#data-classification-methods-of-tmap",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.6 Data classification methods of tmap",
    "text": "4.6 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n4.6.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",\n      fill.scale = tm_scale_intervals(\n        style = \"jenks\",\n        n = 5)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",\n      fill.scale = tm_scale_intervals(\n        style = \"equal\",\n        n = 5)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\n\n4.6.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7867  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(fill_alpha = 0.5)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#colour-scheme",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.7 Colour Scheme",
    "text": "4.7 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n4.7.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to valuesargument of tm_scale_intervals() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",\n      fill.scale = tm_scale_intervals(\n        style = \"quantile\",\n        n = 5,\n        values = \"brewer.greens\")) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",\n      fill.scale = tm_scale_intervals(\n        style = \"quantile\",\n        n = 5,\n        values = \"-brewer.greens\")) +\n  tm_borders(fill_alpha = 0.5)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#map-layouts",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.8 Map Layouts",
    "text": "4.8 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n4.8.1 Map Legend\nIn tmap, several tm_legend() options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\",\n      fill.scale = tm_scale_intervals(\n        style = \"jenks\",\n        n = 5,\n        values = \"brewer.greens\"),\n      fill.legend = tm_legend(\n        title = \"Dependency ratio\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\")\n\n\n\n\n\n\n\n\n\n\n4.8.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n4.8.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\", \n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.9 Drawing Small Multiple Choropleth Maps",
    "text": "4.9 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n4.9.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n4.9.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n4.9.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#mapping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07_1.html#mapping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands On Exercise 7: Choropleth Mapping with R",
    "section": "4.10 Mapping Spatial Object Meeting a Selection Criterion",
    "text": "4.10 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 8A"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package.\n\n\n\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"DataEx09/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\n#install.packages(\"devtools\")\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#overview",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "Before we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#data-wrangling",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "In this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"DataEx09/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "treemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "treemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 9E",
    "section": "",
    "text": "This slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\n#install.packages(\"devtools\")\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html",
    "title": "Hands-on Exercise 9C",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\nBefore you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"DataEx09/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format.\n\n\n\n\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\n\n\n\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#overview",
    "title": "Hands-on Exercise 9C",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 9C",
    "section": "",
    "text": "Before you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 9C",
    "section": "",
    "text": "In this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"DataEx09/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#static-heatmap",
    "title": "Hands-on Exercise 9C",
    "section": "",
    "text": "There are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-On_Ex09c.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 9C",
    "section": "",
    "text": "heatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "plotting calendar heatmap by using ggplot2 functions\nplotting a cycle plot by using ggplot2 function\nplotting a slopegraph\nplotting a horizon chart",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Importing the data",
    "text": "Importing the data\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Examining the data structure",
    "text": "Examining the data structure\nkable() can be used to review the structure of the imported data frame\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are 3 columns, timestamp, source_country and tz.\n\ntimestamp field stores data-time values in POSIXct format\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code\ntz field stores time zone of the source IP address",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Data Preparation",
    "text": "Data Preparation\nStep 1: Deriving weekday and hour of dayfields\nBefore we can plot the calender heatmap, 2 new fields namely wkday and hour need to be derived.\n\nmake_hr_wkday &lt;- function(ts, cs, tz) {\n  real_times &lt;- ymd_hms(ts,\n                        tz = tz[1],\n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = cs,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n}\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calender-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calender-heatmaps",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Building the Calender Heatmaps",
    "text": "Building the Calender Heatmaps\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-multiple-calendar-heatmaps",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Building Multiple Calendar Heatmaps",
    "text": "Building Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Plotting Multiple Calendar Heatmaps",
    "text": "Plotting Multiple Calendar Heatmaps\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-import",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-import",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Data Import",
    "text": "Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-month-and-year-fields",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-month-and-year-fields",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Deriving month and year fields",
    "text": "Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-the-target-country",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-the-target-country",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Extracting the target country",
    "text": "Extracting the target country\nExtracting data for the target country (eg. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-year-average-arrivals-by-month",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-year-average-arrivals-by-month",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Computing year average arrivals by month",
    "text": "Computing year average arrivals by month\nUsing group_by() and summarise() of dplyr to compute year average arrivals by month\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Plotting the cycle plot",
    "text": "Plotting the cycle plot\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-import-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-import-1",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Data Import",
    "text": "Data Import\n\nrice &lt;- read_csv(\"data/rice.csv\")",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "title": "Hands On Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "Plotting the slopegraph",
    "text": "Plotting the slopegraph\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Liu Chih Yuan\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.",
    "crumbs": [
      "Home",
      "Hands-on Exercise",
      "Exercise 6"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-Liu Chih Yuan",
    "section": "",
    "text": "by Liu Chih Yuan\nThis page is dedicated for documenting work exercises and notes for ISSS608 Visual Analytics\n\nFun Fact: Cabbages are veggies\n\n\n\nHands on Ex10 added"
  },
  {
    "objectID": "index.html#latest-update",
    "href": "index.html#latest-update",
    "title": "ISSS608-Liu Chih Yuan",
    "section": "",
    "text": "Hands on Ex10 added"
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex03/TakeHome_Ex03.html",
    "href": "TakeHome_Ex/TakeHome_Ex03/TakeHome_Ex03.html",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "My teammates and I combined our work for TH3, please find the content in the link below consisting the project proposal and the project draft.\nGroup\nhttps://isss608projectgroup12.netlify.app/project_proposal/project_proposal",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex03"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "The 2024 dataset from Singapore’ Department of Statistics provides resident population data by planning area, subzone, age, and sex.\n\n\nTo perform structured exploratory data analysis to uncover insights on demographic distribution across regions.\n\n\n\n\n\n\n\npacman::p_load(tidyverse, ggrepel, ggthemes, patchwork, ggridges, scales)\n\nThe R packages used in this EDA are as follows:\n\ntidyverse core R package for data science (contains essential packages such as ggplot2)\nggrepel for ggplot2 to repel overlapping text labels\nggthemes extra ggplot themes\npatchwork combine ggplot\nggridges for ridgeline plots\nscales customer number formatting\n\n\n\n\n\ndf &lt;- read.csv(\"data/respopagesex2024.csv\")\n\n\ncommon_theme &lt;- theme_minimal(base_size = 16) +\n  theme(\n    axis.text = element_text(size = 18),\n    axis.title = element_text(size = 20),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    legend.text = element_text(size = 17),\n    legend.title = element_text(size = 16)\n  )\n\n\n\n\nglimpse(df)\n\nRows: 60,424\nColumns: 6\n$ PA   &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ Age  &lt;chr&gt; \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", …\n$ Sex  &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Females\", \"Male…\n$ Pop  &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, 30, 10, 3…\n$ Time &lt;int&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\ncolSums(is.na(df))\n\n  PA   SZ  Age  Sex  Pop Time \n   0    0    0    0    0    0 \n\n\n\n\n\n\n\n\n\nstr(df)\n\n'data.frame':   60424 obs. of  6 variables:\n $ PA  : chr  \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ  : chr  \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ Age : chr  \"0\" \"0\" \"1\" \"1\" ...\n $ Sex : chr  \"Males\" \"Females\" \"Males\" \"Females\" ...\n $ Pop : int  10 10 10 10 10 10 10 10 30 10 ...\n $ Time: int  2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 ...\n\n\nWe see column Age is “chr” (character) instead of numeric, let’s check why by finding the unique value of column Age\n\nsort(unique(df$Age))\n\n [1] \"0\"           \"1\"           \"10\"          \"11\"          \"12\"         \n [6] \"13\"          \"14\"          \"15\"          \"16\"          \"17\"         \n[11] \"18\"          \"19\"          \"2\"           \"20\"          \"21\"         \n[16] \"22\"          \"23\"          \"24\"          \"25\"          \"26\"         \n[21] \"27\"          \"28\"          \"29\"          \"3\"           \"30\"         \n[26] \"31\"          \"32\"          \"33\"          \"34\"          \"35\"         \n[31] \"36\"          \"37\"          \"38\"          \"39\"          \"4\"          \n[36] \"40\"          \"41\"          \"42\"          \"43\"          \"44\"         \n[41] \"45\"          \"46\"          \"47\"          \"48\"          \"49\"         \n[46] \"5\"           \"50\"          \"51\"          \"52\"          \"53\"         \n[51] \"54\"          \"55\"          \"56\"          \"57\"          \"58\"         \n[56] \"59\"          \"6\"           \"60\"          \"61\"          \"62\"         \n[61] \"63\"          \"64\"          \"65\"          \"66\"          \"67\"         \n[66] \"68\"          \"69\"          \"7\"           \"70\"          \"71\"         \n[71] \"72\"          \"73\"          \"74\"          \"75\"          \"76\"         \n[76] \"77\"          \"78\"          \"79\"          \"8\"           \"80\"         \n[81] \"81\"          \"82\"          \"83\"          \"84\"          \"85\"         \n[86] \"86\"          \"87\"          \"88\"          \"89\"          \"9\"          \n[91] \"90_and_Over\"\n\n\nHere most likely it’s the 90_and_over causing it to be a str not int\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    AgeNum = suppressWarnings(\n      ifelse(Age == \"90_and_Over\", 90, as.numeric(Age))\n    )\n  )\n\nSince we observed people over 90 years old are categorized 90_and_above instead of actual numbers, for the ease of plotting we hereby create a new column AgeNum\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    AgeGroup = case_when(\n      AgeNum &lt;= 12 ~ \"Child\",\n      AgeNum &lt;= 24 ~ \"Youth\",\n      AgeNum &lt;= 64 ~ \"Adult\",\n      TRUE ~ \"Senior\"\n    )\n  )\n\nWe create a new column AgeGroup for future EDA purposes\n\nstr(df)\n\n'data.frame':   60424 obs. of  8 variables:\n $ PA      : chr  \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ      : chr  \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ Age     : chr  \"0\" \"0\" \"1\" \"1\" ...\n $ Sex     : chr  \"Males\" \"Females\" \"Males\" \"Females\" ...\n $ Pop     : int  10 10 10 10 10 10 10 10 30 10 ...\n $ Time    : int  2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 ...\n $ AgeNum  : num  0 0 1 1 2 2 3 3 4 4 ...\n $ AgeGroup: chr  \"Child\" \"Child\" \"Child\" \"Child\" ...\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(df, aes(x = AgeNum, y = Pop)) +\n  stat_summary(fun = sum, geom = \"bar\", fill = \"steelblue\") +\n  labs(title = \"Total Population by Age\", x = \"Age\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\np2 &lt;- df %&gt;%\n  group_by(AgeGroup) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ggplot(aes(x = AgeGroup, y = Pop, fill = AgeGroup)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Population by Age Group\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\n(p1 / p2) + plot_layout(heights = c(1.2, 1))\n\n\n\n\n\n\n\n\nInsights:\n\nMost residents fall between ages 25 to 54\nYouth population is shrinking, suggesting long-term labor sustainability issues\nSenior population (65+) rising, indicating growing need for eldercare and aging population\n\n\n\n\n\np3 &lt;- ggplot(df, aes(x = Sex, y = Pop, fill = Sex)) +\n  stat_summary(fun = sum, geom = \"bar\") +\n  labs(title = \"Population by Gender\", x = NULL, y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\np4 &lt;- df %&gt;%\n  group_by(Sex, AgeGroup) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ggplot(aes(x = AgeGroup, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Gender Distribution by Age Group\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\n(p3 / p4) + plot_layout(heights = c(1.2, 1))\n\n\n\n\n\n\n\n\nInsights:\n\nGender balance is nearly equal overall\nFemale dominates in the senior age group, likely due to higher life expectancy\n\n\n\n\n\ndf_pyramid &lt;- df %&gt;%\n  filter(AgeNum &lt;= 90) %&gt;%\n  mutate(Pop = ifelse(Sex == \"Males\", -Pop, Pop))\n\np6 &lt;- ggplot(df_pyramid, aes(x = AgeNum, y = Pop, fill = Sex)) +\n  geom_col(width = 1) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\", x = \"Age\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\np7 &lt;- df %&gt;%\n  group_by(Sex, AgeGroup) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ggplot(aes(x = AgeGroup, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Age Group Distribution by Gender\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\n(p6 / p7) + plot_layout(heights = c(1.3, 1))\n\n\n\n\n\n\n\n\nInsights:\n\nPyramid shows narrowing base wider top, typical for aging societies\nAdults dominate across both genders, seniors are the second largest group\n\n\n\n\n\n\nSingapore faces a demographic shift towards aging, requiring proactive planning\nUneven population spread across subzones and planning ares calls for smart urban development\nThis EDA provides clear insights and serves as baseline for policy design, urban planning, and future modelling",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#overview",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#overview",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "The 2024 dataset from Singapore’ Department of Statistics provides resident population data by planning area, subzone, age, and sex.\n\n\nTo perform structured exploratory data analysis to uncover insights on demographic distribution across regions.",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#getting-started",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#getting-started",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "pacman::p_load(tidyverse, ggrepel, ggthemes, patchwork, ggridges, scales)\n\nThe R packages used in this EDA are as follows:\n\ntidyverse core R package for data science (contains essential packages such as ggplot2)\nggrepel for ggplot2 to repel overlapping text labels\nggthemes extra ggplot themes\npatchwork combine ggplot\nggridges for ridgeline plots\nscales customer number formatting\n\n\n\n\n\ndf &lt;- read.csv(\"data/respopagesex2024.csv\")\n\n\ncommon_theme &lt;- theme_minimal(base_size = 16) +\n  theme(\n    axis.text = element_text(size = 18),\n    axis.title = element_text(size = 20),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    legend.text = element_text(size = 17),\n    legend.title = element_text(size = 16)\n  )\n\n\n\n\nglimpse(df)\n\nRows: 60,424\nColumns: 6\n$ PA   &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ Age  &lt;chr&gt; \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", …\n$ Sex  &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Females\", \"Male…\n$ Pop  &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, 30, 10, 3…\n$ Time &lt;int&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\ncolSums(is.na(df))\n\n  PA   SZ  Age  Sex  Pop Time \n   0    0    0    0    0    0 \n\n\n\n\n\n\n\n\n\nstr(df)\n\n'data.frame':   60424 obs. of  6 variables:\n $ PA  : chr  \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ  : chr  \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ Age : chr  \"0\" \"0\" \"1\" \"1\" ...\n $ Sex : chr  \"Males\" \"Females\" \"Males\" \"Females\" ...\n $ Pop : int  10 10 10 10 10 10 10 10 30 10 ...\n $ Time: int  2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 ...\n\n\nWe see column Age is “chr” (character) instead of numeric, let’s check why by finding the unique value of column Age\n\nsort(unique(df$Age))\n\n [1] \"0\"           \"1\"           \"10\"          \"11\"          \"12\"         \n [6] \"13\"          \"14\"          \"15\"          \"16\"          \"17\"         \n[11] \"18\"          \"19\"          \"2\"           \"20\"          \"21\"         \n[16] \"22\"          \"23\"          \"24\"          \"25\"          \"26\"         \n[21] \"27\"          \"28\"          \"29\"          \"3\"           \"30\"         \n[26] \"31\"          \"32\"          \"33\"          \"34\"          \"35\"         \n[31] \"36\"          \"37\"          \"38\"          \"39\"          \"4\"          \n[36] \"40\"          \"41\"          \"42\"          \"43\"          \"44\"         \n[41] \"45\"          \"46\"          \"47\"          \"48\"          \"49\"         \n[46] \"5\"           \"50\"          \"51\"          \"52\"          \"53\"         \n[51] \"54\"          \"55\"          \"56\"          \"57\"          \"58\"         \n[56] \"59\"          \"6\"           \"60\"          \"61\"          \"62\"         \n[61] \"63\"          \"64\"          \"65\"          \"66\"          \"67\"         \n[66] \"68\"          \"69\"          \"7\"           \"70\"          \"71\"         \n[71] \"72\"          \"73\"          \"74\"          \"75\"          \"76\"         \n[76] \"77\"          \"78\"          \"79\"          \"8\"           \"80\"         \n[81] \"81\"          \"82\"          \"83\"          \"84\"          \"85\"         \n[86] \"86\"          \"87\"          \"88\"          \"89\"          \"9\"          \n[91] \"90_and_Over\"\n\n\nHere most likely it’s the 90_and_over causing it to be a str not int\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    AgeNum = suppressWarnings(\n      ifelse(Age == \"90_and_Over\", 90, as.numeric(Age))\n    )\n  )\n\nSince we observed people over 90 years old are categorized 90_and_above instead of actual numbers, for the ease of plotting we hereby create a new column AgeNum\n\n\n\n\ndf &lt;- df %&gt;%\n  mutate(\n    AgeGroup = case_when(\n      AgeNum &lt;= 12 ~ \"Child\",\n      AgeNum &lt;= 24 ~ \"Youth\",\n      AgeNum &lt;= 64 ~ \"Adult\",\n      TRUE ~ \"Senior\"\n    )\n  )\n\nWe create a new column AgeGroup for future EDA purposes\n\nstr(df)\n\n'data.frame':   60424 obs. of  8 variables:\n $ PA      : chr  \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ      : chr  \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ Age     : chr  \"0\" \"0\" \"1\" \"1\" ...\n $ Sex     : chr  \"Males\" \"Females\" \"Males\" \"Females\" ...\n $ Pop     : int  10 10 10 10 10 10 10 10 30 10 ...\n $ Time    : int  2024 2024 2024 2024 2024 2024 2024 2024 2024 2024 ...\n $ AgeNum  : num  0 0 1 1 2 2 3 3 4 4 ...\n $ AgeGroup: chr  \"Child\" \"Child\" \"Child\" \"Child\" ...",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#exploratory-data-analysis",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#exploratory-data-analysis",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "p1 &lt;- ggplot(df, aes(x = AgeNum, y = Pop)) +\n  stat_summary(fun = sum, geom = \"bar\", fill = \"steelblue\") +\n  labs(title = \"Total Population by Age\", x = \"Age\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\np2 &lt;- df %&gt;%\n  group_by(AgeGroup) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ggplot(aes(x = AgeGroup, y = Pop, fill = AgeGroup)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Population by Age Group\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\n(p1 / p2) + plot_layout(heights = c(1.2, 1))\n\n\n\n\n\n\n\n\nInsights:\n\nMost residents fall between ages 25 to 54\nYouth population is shrinking, suggesting long-term labor sustainability issues\nSenior population (65+) rising, indicating growing need for eldercare and aging population\n\n\n\n\n\np3 &lt;- ggplot(df, aes(x = Sex, y = Pop, fill = Sex)) +\n  stat_summary(fun = sum, geom = \"bar\") +\n  labs(title = \"Population by Gender\", x = NULL, y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\np4 &lt;- df %&gt;%\n  group_by(Sex, AgeGroup) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ggplot(aes(x = AgeGroup, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Gender Distribution by Age Group\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\n(p3 / p4) + plot_layout(heights = c(1.2, 1))\n\n\n\n\n\n\n\n\nInsights:\n\nGender balance is nearly equal overall\nFemale dominates in the senior age group, likely due to higher life expectancy\n\n\n\n\n\ndf_pyramid &lt;- df %&gt;%\n  filter(AgeNum &lt;= 90) %&gt;%\n  mutate(Pop = ifelse(Sex == \"Males\", -Pop, Pop))\n\np6 &lt;- ggplot(df_pyramid, aes(x = AgeNum, y = Pop, fill = Sex)) +\n  geom_col(width = 1) +\n  coord_flip() +\n  labs(title = \"Population Pyramid\", x = \"Age\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\np7 &lt;- df %&gt;%\n  group_by(Sex, AgeGroup) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ggplot(aes(x = AgeGroup, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Age Group Distribution by Gender\", y = \"Population\") +\n  scale_y_continuous(labels = label_comma()) +\n  common_theme\n\n(p6 / p7) + plot_layout(heights = c(1.3, 1))\n\n\n\n\n\n\n\n\nInsights:\n\nPyramid shows narrowing base wider top, typical for aging societies\nAdults dominate across both genders, seniors are the second largest group",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 A"
    ]
  },
  {
    "objectID": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#conculsion",
    "href": "TakeHome_Ex/TakeHome_Ex01/TakeHome_Ex01.html#conculsion",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Singapore faces a demographic shift towards aging, requiring proactive planning\nUneven population spread across subzones and planning ares calls for smart urban development\nThis EDA provides clear insights and serves as baseline for policy design, urban planning, and future modelling",
    "crumbs": [
      "Home",
      "Take-Home Exercise",
      "Take-Home Ex01 A"
    ]
  },
  {
    "objectID": "Session_notes/session_notes1.html",
    "href": "Session_notes/session_notes1.html",
    "title": "Session Note Week 1",
    "section": "",
    "text": "Session notes will be documenting the notes and learning points made during the hands-on exercise\n\n\n.qmd (Quarto Markdown): for content, dashboards\n.yml (Config/settings): metadata, layouts, options\nUse for maintain all the deliverables (eg. projects & exercise)\n\n\n\nWhen you make changes of your qmd / yml it will be first shown in git\nUsual changes: save (either qmd / yml) -&gt; render (note this changes locally only)\nTo upload to GitHub, we have to “push” the files\n\nPress Git on top right block\nSelect all files\nCommit and leave comment eg. commit after adding a cabbage image\nPress Push\nCheck on your GitHub see if uploaded\n\n\n\nPush Rejected: your local brand was “behind” the remote\n\nThis happens when:\n\nCreated a new repo on GitHub, it auto generates a README, LICENSE etc\nWhen you try to push without pulling those remote changer first\nLocal version doesn’t know about those remote commits so it says: do a git pull first to sync then can push\n\n\n\n\n\nHow Git Pull & Push works\n\nPull: download files from GitHub -&gt; your local\nPush: send files from local -&gt; GitHub\n\nHow to avoid this error\n\nDon’t initialize with README etc\nIf did so, always Pull before you Push"
  },
  {
    "objectID": "Session_notes/session_notes1.html#quarto",
    "href": "Session_notes/session_notes1.html#quarto",
    "title": "Session Note Week 1",
    "section": "",
    "text": ".qmd (Quarto Markdown): for content, dashboards\n.yml (Config/settings): metadata, layouts, options\nUse for maintain all the deliverables (eg. projects & exercise)\n\n\n\nWhen you make changes of your qmd / yml it will be first shown in git\nUsual changes: save (either qmd / yml) -&gt; render (note this changes locally only)\nTo upload to GitHub, we have to “push” the files\n\nPress Git on top right block\nSelect all files\nCommit and leave comment eg. commit after adding a cabbage image\nPress Push\nCheck on your GitHub see if uploaded\n\n\n\nPush Rejected: your local brand was “behind” the remote\n\nThis happens when:\n\nCreated a new repo on GitHub, it auto generates a README, LICENSE etc\nWhen you try to push without pulling those remote changer first\nLocal version doesn’t know about those remote commits so it says: do a git pull first to sync then can push\n\n\n\n\n\nHow Git Pull & Push works\n\nPull: download files from GitHub -&gt; your local\nPush: send files from local -&gt; GitHub\n\nHow to avoid this error\n\nDon’t initialize with README etc\nIf did so, always Pull before you Push"
  },
  {
    "objectID": "testing_qmd/main.html",
    "href": "testing_qmd/main.html",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "",
    "text": "This take home exercise is based on the VAST Challenge Mini Case 3\nOver the past decade, the community of Oceanus has faced numerous transformations and challenges evolving from its fishing-centric origins. Following major crackdowns on illegal fishing activities, suspects have shifted investments into more regulated sectors such as the ocean tourism industry, resulting in growing tensions. This increased tourism has recently attracted the likes of international pop star Sailor Shift, who announced plans to film a music video on the island.\nClepper Jessen, a former analyst at FishEye and now a seasoned journalist for the Hacklee Herald, has been keenly observing these rising tensions. Recently, he turned his attention towards the temporary closure of Nemo Reef. By listening to radio communications and utilizing his investigative tools, Clepper uncovered a complex web of expedited approvals and secretive logistics. These efforts revealed a story involving high-level Oceanus officials, Sailor Shift’s team, local influential families, and local conservationist group The Green Guardians, pointing towards a story of corruption and manipulation.\nYour task is to develop new and novel visualizations and visual analytics approaches to help Clepper get to the bottom of this story",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#initial-eda",
    "href": "testing_qmd/main.html#initial-eda",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "3.1 Initial EDA",
    "text": "3.1 Initial EDA\n\n\nShow code\nExpCatViz(data=mc3_nodes,\n          col=\"pink\")\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#relationship-between-entities-and-events",
    "href": "testing_qmd/main.html#relationship-between-entities-and-events",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "6.1 Relationship between entities and events",
    "text": "6.1 Relationship between entities and events\n\n\nShow code\nggraph(mc3_graph, \n       layout = \"fr\") +\n  geom_edge_link(alpha = 0.3, \n                 colour = \"gray\") +\n  geom_node_point(aes(color = `type`), \n                  size = 2) +\n  geom_node_text(aes(label = type), \n                 repel = TRUE, \n                 size = 2.5) +\n  theme_void()",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#entity-distribution",
    "href": "testing_qmd/main.html#entity-distribution",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "6.2 Entity distribution",
    "text": "6.2 Entity distribution\n\n\nShow code\n# Define color mapping\nsubtype_colors &lt;- c(\n  \"Person\" = \"#2ca5ff\",\n  \"Organization\" = \"#f5ee15\",\n  \"Vessel\" = \"#FB7E81\",\n  \"Group\" = \"#25e158\",\n  \"Location\" = \"#ec4bff\"\n)\n\nmc3_nodes_final %&gt;%\n  filter(type == \"Entity\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Entity Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#event-type-distribution",
    "href": "testing_qmd/main.html#event-type-distribution",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "6.3 Event type distribution",
    "text": "6.3 Event type distribution\n\n\nShow code\nmc3_nodes_final %&gt;%\n  filter(type == \"Event\") %&gt;%\n  count(sub_type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = n), hjust = -0.1) +\n  labs(title = \"Event Sub-type Distribution\", x = \"Sub-type\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#list-of-communication-participants",
    "href": "testing_qmd/main.html#list-of-communication-participants",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "6.4 List of communication participants",
    "text": "6.4 List of communication participants\n\n\nShow code\nlibrary(DT)\n\n# Step 1: Get all Communication Event IDs\ncomm_event_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  pull(id)\n\n# Step 2: Extract 'sent' edges for communication events\ncomm_sent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\", to_id %in% comm_event_ids) %&gt;%\n  select(comm_id = to_id, sender_id = from_id)\n\n# Step 3: Extract 'received' edges for same communication events\ncomm_received_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\", from_id %in% comm_event_ids) %&gt;%\n  select(comm_id = from_id, receiver_id = to_id)\n\n# Step 4: Join sent and received edges by communication ID\ncomm_pairs &lt;- comm_sent_edges %&gt;%\n  inner_join(comm_received_edges, by = \"comm_id\")\n\n# Step 5: Add sender and receiver labels\nparticipants_named &lt;- comm_pairs %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n\n\n# Step7: Interactive summary of top sender–receiver pairs\nparticipants_named %&gt;%\n  count(sender_label, receiver_label, sort = TRUE) %&gt;%\n  datatable(\n    caption = \"Top Communication Pairs (Sender → Receiver)\",\n    colnames = c(\"Sender\", \"Receiver\", \"Message Count\"),\n    options = list(pageLength = 10, autoWidth = TRUE),\n    rownames = FALSE\n  )",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#visualization-of-communication-participants-network",
    "href": "testing_qmd/main.html#visualization-of-communication-participants-network",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "6.4.1 Visualization of communication participants network",
    "text": "6.4.1 Visualization of communication participants network\nThis code creates an interactive communication network graph using visNetwork, where:\n\nEach node represents a person or entity, node size is based on total messages sent by that participant.\nEach edge (arrow) represents a communication sent from one participant to another, the thicker the edge, the more message sent to that particular receiver.\n\nVer 1: Layout_in_circle\n\n\nShow code\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per node\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes, merge with message count and add color/shape\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    sent_count = replace_na(sent_count, 0),\n    size = rescale(sent_count, to = c(10, 40)),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    ),\n  ) %&gt;%\n  arrange(desc(size))\n\n# Step 4: Format visNetwork edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Define legend items\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render network with legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"1000px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape are picked up from nodes_vis columns automatically\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_in_circle\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\nVer 2: Layout_on_sphere\nFrom this plot, it reveals that some pairs (e.g., Miranda Jordan and Clepper Jensen) mainly communicate with each other, suggesting isolated or private channels outside the broader network.\n\n\nShow code\nlibrary(visNetwork)\n\n# Step 1: Summarize communication edges\ncomm_edges_vis &lt;- participants_named %&gt;%\n  count(sender_id, receiver_id, sort = TRUE) %&gt;%\n  rename(from = sender_id, to = receiver_id, value = n)\n\n# Step 2: Compute messages sent per person (by sender)\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare nodes with label, subtype, color, shape, and scaled size\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    size = if_else(\n      sub_type == \"Person\",\n      rescale(sent_count, to = c(10, 40), na.rm = TRUE),\n      15\n    ),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#2ca5ff\",\n      sub_type == \"Organization\" ~ \"#f5ee15\",\n      sub_type == \"Vessel\" ~ \"#FB7E81\",\n      sub_type == \"Group\" ~ \"#25e158\",\n      sub_type == \"Location\" ~ \"#ec4bff\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Organization\" ~ \"square\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      sub_type == \"Group\" ~ \"star\",\n      sub_type == \"Location\" ~ \"diamond\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Step 4: Format edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Legend mapping\nlegend_nodes &lt;- data.frame(\n  label = c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\"),\n  color = c(\"#2ca5ff\", \"#f5ee15\", \"#FB7E81\", \"#25e158\", \"#ec4bff\"),\n  shape = c(\"dot\", \"square\", \"triangle\", \"star\", \"diamond\"),\n  stringsAsFactors = FALSE\n)\n\n# Step 6: Render the network with layout_on_sphere and legend\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"900px\") %&gt;%\n  visNodes(\n    size = nodes_vis$size\n    # color and shape columns are automatically used\n  ) %&gt;%\n  visLegend(\n    addNodes = lapply(1:nrow(legend_nodes), function(i) {\n      list(\n        label = legend_nodes$label[i],\n        shape = legend_nodes$shape[i],\n        color = legend_nodes$color[i]\n      )\n    }),\n    useGroups = FALSE,\n    width = 0.15\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_on_sphere\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#vast-challenge-task-question-1a-and-1b",
    "href": "testing_qmd/main.html#vast-challenge-task-question-1a-and-1b",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "VAST Challenge Task & Question 1a and 1b",
    "text": "VAST Challenge Task & Question 1a and 1b\nClepper found that messages frequently came in at around the same time each day.\n\nDevelop a graph-based visual analytics approach to identify any daily temporal patterns in communications.\nHow do these patterns shift over the two weeks of observations?\n\nObjective\n\nIdentify when communications happen most often during each day.\nDetect shifts in these patterns over the 2-week period.\nLater: Focus on a specific entity (e.g., Nadia Conti) and explore who influences them.\n\n\nStep 1: Extract & Parse Communication Event Timestamps\nExtract the Communication Timestamps from mc3_nodes_final and filter for communication events.\n\n\nShow code\n# Filter for Communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    day = as.Date(timestamp),\n    hour = hour(timestamp)\n  )\n\n\nParse the Communication Timestamp into the format “dd/mm/yyy (ddd)” for ease of reference.\n\n\nShow code\n# Communication events with parsed date and time\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")  # e.g., \"19/03/2040 (Tue)\"\n  )\n\n\n\n\nStep 2: Visualize the Communication Volume for Analysis\n\n2.1 - Bar Plot of daily communication volume over the 2 weeks period:\n\n\nShow code\n# Step 1: Prepare daily message volume data\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date)\n\n# Step 2: Compute average and total message count\navg_msg_count &lt;- mean(daily_message_volume$message_count)\ntotal_msg_count &lt;- sum(daily_message_volume$message_count)\n\n# Step 3: Plot bar chart with average + total labels\nggplot(daily_message_volume, aes(x = date_label, y = message_count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(\n    aes(label = message_count),\n    vjust = -0.3,\n    size = 2.5,\n    color = \"grey40\"\n  ) +\n  geom_hline(yintercept = avg_msg_count, color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\n    \"label\", x = 1, y = avg_msg_count + 2,\n    label = paste(\"Average =\", round(avg_msg_count, 1)),\n    color = \"red\", fill = \"grey90\",\n    label.size = 0, hjust = -0.2, vjust = 3\n  ) +\n  annotate(\n    \"label\", x = nrow(daily_message_volume), y = max(daily_message_volume$message_count) + 5,\n    label = paste(\"Total =\", total_msg_count),\n    color = \"black\", fill = \"lightgrey\",\n    label.size = 0.3, hjust = 1.1, vjust = 1\n  ) +\n  labs(\n    title = \"Daily Radio Communication Volume\",\n    x = \"Date\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n2.2 - Interactive Table of daily communication volume variation(message count)\n\n\nShow code\nlibrary(DT)\n\n# Daily message volume with comparisons\ndaily_message_volume &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\")\n  ) %&gt;%\n  group_by(date, date_label) %&gt;%\n  summarise(message_count = n(), .groups = \"drop\") %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    change_from_prev = message_count - lag(message_count),\n    pct_change_from_prev = round((message_count - lag(message_count)) / lag(message_count) * 100, 2)\n  )\n\ndatatable(\n  daily_message_volume %&gt;% select(-date),  # remove raw date if not needed\n  caption = \"Daily Message Volume with Day-over-Day Change\",\n  options = list(pageLength = 14, order = list(list(0, 'asc'))),\n  rownames = FALSE\n)\n\n\n\n\n\n\n\n\n2.3a - Heat Map of hourly message volume for each day over the 2 weeks period:\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count\n\n\nShow code\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Reconstruct sender–receiver–timestamp structure\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp),\n         hour = hour(timestamp),\n         date_label = format(timestamp, \"%d/%m/%Y (%a)\"))\n\n# Step 2: Get sender (sent) and receiver (received) links\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)\n\n# Step 3: Join all together into sender–receiver–timestamp\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(sender_id = id, sender_label = label), by = \"sender_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(receiver_id = id, receiver_label = label), by = \"receiver_id\")\n\n# Step 4: Aggregate total messages per hour/day\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, hour) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"📅 Date: \", date_label,\n      \"&lt;br&gt;⏰ Hour: \", sprintf(\"%02d:00\", hour),\n      \"&lt;br&gt;📨 Messages: \", count,\n      \"&lt;br&gt;🔴 Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;🟢 Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: Static ggplot\np &lt;- ggplot(comm_heatmap, aes(\n  x = hour,\n  y = fct_rev(factor(date_label)),\n  fill = count,\n  text = tooltip\n)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(option = \"inferno\", direction = -1, name = \"Message Count\") +\n  scale_x_continuous(\n    breaks = 0:23,\n    labels = function(x) sprintf(\"%02d:00\", x)\n  ) +\n  labs(\n    title = \"Hourly Heatmap of Radio Communications by Day\",\n    x = \"Hour of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Make interactive\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\nWe will increase the resolution to half-hour time slots.\n\n\n2.4b - Heat Map of half-hourly message volume for each day over the 2 weeks period:\nThis heat map is interactive and you may choose to hover on the tile to display the date, time, and message count.\n\n\nShow code\nlibrary(forcats)\nlibrary(plotly)\n\n# Step 1: Fix sender and receiver edges\ncomm_edges_sent &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(event_id = to_id, sender_id = from_id)\n\ncomm_edges_recv &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event_id = from_id, receiver_id = to_id)  # ✅ fixed receiver_id\n\n# Step 2: Reconstruct sender–receiver–event linkage\ncomm_events_raw &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(event_id = id, timestamp) %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5),\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    time_label = sprintf(\"%02d:%02d\", floor(time_bin), ifelse(time_bin %% 1 == 0, 0, 30))\n  )\n\n# Step 3: Join to get sender/receiver labels\ncomm_links &lt;- comm_events_raw %&gt;%\n  left_join(comm_edges_sent, by = \"event_id\") %&gt;%\n  left_join(comm_edges_recv, by = \"event_id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender_id\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver_id\" = \"id\"))\n\n# Step 4: Aggregate by half-hour + label top actors\ncomm_heatmap &lt;- comm_links %&gt;%\n  group_by(date_label, time_bin, time_label) %&gt;%\n  summarise(\n    count = n(),\n    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],\n    sender_count = max(table(sender_label)),\n    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],\n    receiver_count = max(table(receiver_label)),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    tooltip = paste0(\n      \"📅 Date: \", date_label,\n      \"&lt;br&gt;🕒 Time: \", time_label,\n      \"&lt;br&gt;📨 Messages: \", count,\n      \"&lt;br&gt;🔴 Top Sender: \", top_sender, \" (\", sender_count, \")\",\n      \"&lt;br&gt;🟢 Top Receiver: \", top_receiver, \" (\", receiver_count, \")\"\n    )\n  )\n\n# Step 5: ggplot\np &lt;- ggplot(comm_heatmap, aes(x = time_bin, y = fct_rev(factor(date_label)), fill = count, text = tooltip)) +\n  geom_tile(color = \"white\") +\n  scale_fill_viridis_c(\n    option = \"inferno\",\n    direction = -1,\n    name = \"Message Count\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  labs(\n    title = \"Half-Hourly Heatmap of Radio Communications by Day\",\n    x = \"Time of Day\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank()\n  )\n\n# Step 6: Convert to interactive Plotly plot\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\n2.4c - Density plot of Daily half-hourly message volume over the 2 weeks period:\nThe faceted density plot that shows the distribution of communication events by time of day, broken down for each day in the dataset. It helps to visually detect temporal communication patterns, intensity, and consistency over multiple days.\n\nOverview of the 2 week periodDay 1 - 01/10/2040Day 2 - 02/10/2040Day 3 - 03/10/2040Day 4 - 04/10/2040Day 5 - 05/10/2040Day 6 - 06/10/2040Day 7 - 07/10/2040Day 8 - 08/10/2040Day 9 - 09/10/2040Day 10 - 10/10/2040Day 11 - 11/10/2040Day 12 - 12/10/2040Day 13 - 13/10/2040Day 14 - 14/10/2040\n\n\n\n\nShow code\n# Step 1: Preprocess communication events\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    date_label = format(timestamp, \"%d/%m/%Y (%a)\"),\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = hour + ifelse(minute &lt; 30, 0, 0.5)\n  )\n\n# Step 2: Summarise daily medians and counts\ndaily_stats &lt;- comm_events %&gt;%\n  group_by(date_label) %&gt;%\n  summarise(\n    median_time = median(time_bin),\n    msg_count = n(),\n    .groups = \"drop\"\n  )\n\n# Step 3: Plot\nggplot(comm_events, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = daily_stats, aes(xintercept = median_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(\n    data = daily_stats,\n    aes(x = 20.5, y = 0.25, label = paste(\"Total:\", msg_count)),\n    inherit.aes = FALSE,\n    size = 3,\n    color = \"grey20\",\n    hjust = 1\n  ) +\n  facet_wrap(~ date_label, ncol = 4) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = NULL  # suppress all x-axis labels\n  ) +\n  labs(\n    title = \"Daily Communication Patterns (Half-Hourly)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"01/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"02/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"03/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"04/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"05/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"06/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"07/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"08/10/2040 (Mon)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"09/10/2040 (Tue)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"10/10/2040 (Wed)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"11/10/2040 (Thu)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"12/10/2040 (Fri)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"13/10/2040 (Sat)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Filter for selected date\ndaily_data &lt;- filter(comm_events, date_label == \"14/10/2040 (Sun)\")\n\n# Build density data to find full curve\ndensity_data &lt;- ggplot_build(\n  ggplot(daily_data, aes(x = time_bin)) +\n    geom_density()\n)$data[[1]]\n\n# Identify peaks within ±10% of the maximum density\nmax_y &lt;- max(density_data$y)\npeak_threshold &lt;- 0.9 * max_y\n\nmajor_peaks &lt;- density_data %&gt;%\n  filter(y &gt;= peak_threshold) %&gt;%\n  group_by(grp = cumsum(c(1, diff(x) &gt; 0.5))) %&gt;%  # group close bins\n  summarise(\n    peak_time = x[which.max(y)],\n    peak_density = max(y),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    label = sprintf(\"Peak: %02d:%02d\",\n                    floor(peak_time),\n                    ifelse(peak_time %% 1 == 0, 0, 30))\n  )\n\n# Message count\nmsg_count &lt;- nrow(daily_data)\n\n# Final plot\nggplot(daily_data, aes(x = time_bin)) +\n  geom_density(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(data = major_peaks, aes(xintercept = peak_time),\n             color = \"red\", linetype = \"dashed\", linewidth = 0.6) +\n  geom_text(data = major_peaks,\n            aes(x = peak_time, y = 0.23, label = label),\n            color = \"red\", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +\n  annotate(\"text\", x = 20.5, y = 0.25,\n           label = paste(\"Total messages communicated:\", msg_count),\n           hjust = 1, size = 4, color = \"grey30\") +\n  labs(\n    title = \"Half-Hourly Communication Density (Multiple Peaks Highlighted)\",\n    x = \"Time of Day\",\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 23.5, by = 0.5),\n    labels = function(x) sprintf(\"%02d:%02d\", floor(x), ifelse(x %% 1 == 0, 0, 30))\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n📈 Insights This Visualization Offers\n\n\n\nStep 3: Plot Combined Hourly and Half-hourly Communication Volume\nBar Plot of combined hourly message volume over the 2 weeks period:\n\n\nShow code\n# Prepare data\ncomm_hourly &lt;- comm_events %&gt;%\n  count(hour) %&gt;%\n  mutate(\n    hour_label = sprintf(\"%02d:00\", hour),  # Format to hh:mm\n    percent = n / sum(n)\n  )\n\n# Plot\nggplot(comm_hourly, aes(x = hour_label, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nBar Plot of combined half-hourly message volume in the 2 weeks period.\n\n\nShow code\ncomm_events &lt;- mc3_nodes_final %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(\n    hour = hour(timestamp),\n    minute = minute(timestamp),\n    time_bin = sprintf(\"%02d:%02d\", hour, ifelse(minute &lt; 30, 0, 30))\n  )\n\ncomm_halfhour &lt;- comm_events %&gt;%\n  count(time_bin) %&gt;%\n  mutate(percent = n / sum(n))\n\nggplot(comm_halfhour, aes(x = time_bin, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text_repel(\n    aes(label = paste0(n, \" (\", percent(percent, accuracy = 1), \")\")),\n    nudge_y = 3,\n    size = 2.5,\n    direction = \"y\",\n    max.overlaps = Inf\n  ) +\n  labs(\n    title = \"Overall Half-Hourly Communication Volume\",\n    x = \"Time of Day (hh:mm)\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1a. What are the identifiable daily temporal patterns in communications?\n\n\n\n\nThe daily communication volume fluctuates slightly between 34 and 49 messages, with an average of approximately 42 messages per day, highlighting a stable overall activity level. Notably, the highest volume occurs on 11th October (49 messages), immediately following the lowest volume the day before on 10th October (34 messages)—a sharp rebound that may signal a response to specific events or operational needs. Despite these fluctuations, the system maintains a consistent tempo across the two weeks.\nThe temporal analysis using both the heat map and time series plots reveals a pronounced morning-centric communication rhythm. The vast majority of radio traffic is concentrated between 9:00 AM and 11:30 AM, with the most intense peaks typically occurring between 10:00 and 11:00 AM. With reference to the Density plot of Daily half-hourly message volume, of the 14 days, we see message density peaks at 10:30 AM on 9 days, while on 3 days, it peaks at 12:30 PM.\nFor instance if we were to based in on the hourly plot, 5th October (Fri) and 11th October (Thu) both register their highest single-hour counts at 10:00 AM at 24 and 21 messages respectively. Communication activity drops off steeply after lunchtime, with more than 90% of the days showing little to no activity after 2:30 PM. This pattern suggests a highly structured daily workflow, where key decisions and coordination are front-loaded in the day. Importantly, the hourly heat map also indicates that this routine holds across both weekdays and weekends—communication volumes and peak hours remain similar, underlining the operational regularity of the group regardless of the day of week.\n\n\n\n\n\n\n\n\n\n1b. How do these patterns shift over the two weeks of observations?\n\n\n\n\nOver the two-week period, while the timing and structure of communication peaks remain broadly consistent, there are subtle shifts in both intensity and timing. Some days, such as 3rd, 5th, 11th and 12th October, see particularly high spikes in the mid-morning, which may correspond to critical events, decision points, or heightened urgency. The sharp dip on October 8th and 13th, immediately after a period of “surge” (3rd - 7th and 9th to 12th October), points to possible responses to interruptions, lulls, or triggering incidents. Overall, although the daily messaging routine is remarkably stable, these bursts and brief lulls provide clues to changing circumstances or stress points in the operation—an analytical signal that warrants closer inspection of event logs or external triggers for those dates.\nAnother notable change in the communication pattern is observed during the weekends. In the first week, weekend communication peaks occurred earlier, typically between 10:00 AM and 11:30 AM, closely mirroring the weekday rhythm. However, in the second week, the weekend peaks shifted noticeably later, with the highest message volumes concentrated around 12:00 PM and 1:00 PM. This shift not only marks a departure from the otherwise stable early-morning communication structure but also suggests an adaptive or reactive operational schedule—potentially in response to evolving events, increased coordination needs, or changing priorities as the observation period progressed. The contrast between the two weekends is clear in the heatmap, underscoring the importance of monitoring such shifts as possible indicators of underlying changes in group behavior or external pressures.",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#vast-challenge-task-question-1c",
    "href": "testing_qmd/main.html#vast-challenge-task-question-1c",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "VAST Challenge Task & Question 1c",
    "text": "VAST Challenge Task & Question 1c\nClepper found that messages frequently came in at around the same time each day.\n\nFocus on a specific entity and use this information to determine who has influence over them.\n\n\n3.1 - Data Preparation for “Nadia Conti” Influence Analysis\nWe first extracted the relevant communication edges from the dataset, pairing “sent” and “received” communication events to form entity-to-entity links. We retained only those edges where both nodes represent real-world entities (Person, Organization, Vessel, Group, or Location), ensuring that our analysis focuses on the meaningful actors in the Oceanus network.\n\n\nShow code\n# Extract sent and received communication event edges\nsent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\") %&gt;%\n  select(source_entity = from_id, event = to_id)\n\nreceived_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\") %&gt;%\n  select(event = from_id, target_entity = to_id)\n\n# Pair sent and received to form communication edges\npaired_edges &lt;- sent_edges %&gt;%\n  inner_join(received_edges, by = \"event\") %&gt;%\n  select(from = source_entity, to = target_entity)\n\n# Add unmatched sent and received edges (optional, for completeness)\nsingle_sent_edges &lt;- sent_edges %&gt;%\n  select(from = source_entity, to = event)\nsingle_received_edges &lt;- received_edges %&gt;%\n  select(from = event, to = target_entity)\n\nall_edges &lt;- bind_rows(paired_edges, single_sent_edges, single_received_edges) %&gt;%\n  distinct()\n\n# Identify entity nodes (Person, Organization, Vessel, Group, Location)\nentity_ids &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  pull(id) %&gt;% as.character()\n\nentity_edges &lt;- all_edges %&gt;%\n  filter(as.character(from) %in% entity_ids, as.character(to) %in% entity_ids)\n\nentity_nodes &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Organization\", \"Vessel\", \"Group\", \"Location\")) %&gt;%\n  select(id, label, sub_type)\n\n\n\n\n3.2 - Build the Global Network and Compute Centrality\nUsing these cleaned and filtered edges and nodes, we built a global directed graph representing the Oceanus community. We then computed key network centrality metrics for each node—PageRank, betweenness, and degree—quantifying the influence and connectivity of every entity in the overall network.\n\n\nShow code\nlibrary(igraph)\n\ng &lt;- graph_from_data_frame(d = entity_edges, vertices = entity_nodes, directed = TRUE)\n\n# Compute centralities\nV(g)$pagerank &lt;- page_rank(g)$vector\nV(g)$betweenness &lt;- betweenness(g)\nV(g)$degree &lt;- degree(g)\n\n\n\n\n3.3 - Extract “Nadia Conti” Ego Network (2-hop Neighbourhood)\nFocusing on “Nadia Conti”, we identified her node and extracted her two-step ego network, capturing both direct and indirect connections within the broader network. This local subgraph reveals Nadia’s immediate sphere of influence and the key players connected to her.\n\n\nShow code\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\nego_graph &lt;- make_ego_graph(g, order = 2, nodes = target_index, mode = \"all\")[[1]]\n\n\n\n\n3.4 - Visualize Nadia Conti’s Ego Network (Interactive)\nWe visualized Nadia’s ego network using node size, shape, and color to represent centrality and entity type. We also summarized centrality metrics in clear tables, ranking all ego network members by PageRank, Betweenness, and Degree. This allows for direct identification of the most influential, best-connected, and most strategic actors in Nadia Conti’s communication environment.\n\n\nShow code\nnodes_df &lt;- data.frame(\n  id = V(ego_graph)$name,\n  label = V(ego_graph)$label,\n  group = V(ego_graph)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_graph)$label, \"&lt;/b&gt;&lt;br&gt;\",\n                 \"Degree: \", round(V(ego_graph)$degree, 2), \"&lt;br&gt;\",\n                 \"Betweenness: \", round(V(ego_graph)$betweenness, 2), \"&lt;br&gt;\",\n                 \"PageRank: \", round(V(ego_graph)$pagerank, 4)),\n  shape = ifelse(V(ego_graph)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_graph)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_graph)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_graph)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  value = V(ego_graph)$pagerank * 30 + 5\n)\n\nedges_df &lt;- as_data_frame(ego_graph, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\nlibrary(visNetwork)\nvisNetwork(nodes_df, edges_df, width = \"100%\", height = \"700px\") %&gt;%\n  visNodes(scaling = list(min = 5, max = 30)) %&gt;%\n  visEdges(\n    arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)),\n    color = list(color = \"gray\")\n  ) %&gt;%\n  visOptions(\n    highlightNearest = TRUE,\n    nodesIdSelection = TRUE,\n    manipulation = FALSE\n  ) %&gt;%\n  visInteraction(\n    dragNodes = FALSE,\n    dragView = FALSE,\n    zoomView = FALSE\n  ) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#global-and-ego-network-structure",
    "href": "testing_qmd/main.html#global-and-ego-network-structure",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Global and Ego-Network Structure",
    "text": "Global and Ego-Network Structure\nThe overview network visualization reveals that Nadia Conti is centrally embedded in the Oceanus communication web, maintaining direct and indirect connections with major actors such as Neptune (Vessel), V. Miesel Shipping (Organization), Elise (Person), and others. The use of color and shape coding in the network allows for quick identification of the different types of entities in Nadia’s influence neighborhood.\n\n3.5 - Centrality Tables for Nadia’s Ego Network\nOn both the global and Nadia-focused ego networks, we computed standard network centrality metrics for all nodes:\n\nPageRank (overall influence),\nBetweenness (information brokerage/intermediary role), and\nDegree (number of direct connections).\n\nThese measures quantify the importance and structural roles of each entity relative to Nadia and the broader community.\n\n\nShow code\n# PageRank table\npagerank_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  pagerank = round(V(ego_graph)$pagerank, 4)\n) %&gt;% arrange(desc(pagerank))\n\n# Betweenness table\nbetweenness_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  betweenness = round(V(ego_graph)$betweenness, 2)\n) %&gt;% arrange(desc(betweenness))\n\n# Degree table\ndegree_df &lt;- data.frame(\n  label = V(ego_graph)$label,\n  sub_type = V(ego_graph)$sub_type,\n  degree = V(ego_graph)$degree\n) %&gt;% arrange(desc(degree))\n\n\n\n\nShow code\nknitr::kable(pagerank_df, caption = \"PageRank Centrality (Nadia's Ego Network)\")\n\n\n\nPageRank Centrality (Nadia’s Ego Network)\n\n\nlabel\nsub_type\npagerank\n\n\n\n\nMako\nVessel\n0.0687\n\n\nOceanus City Council\nOrganization\n0.0530\n\n\nReef Guardian\nVessel\n0.0454\n\n\nNadia Conti\nPerson\n0.0432\n\n\nRemora\nVessel\n0.0409\n\n\nV. Miesel Shipping\nOrganization\n0.0394\n\n\nNeptune\nVessel\n0.0358\n\n\nHimark Harbor\nLocation\n0.0358\n\n\nLiam Thorne\nPerson\n0.0275\n\n\nBoss\nPerson\n0.0272\n\n\nSentinel\nVessel\n0.0250\n\n\nPaackland Harbor\nLocation\n0.0244\n\n\nDavis\nPerson\n0.0239\n\n\nMarlin\nVessel\n0.0235\n\n\nEcoVigil\nVessel\n0.0233\n\n\nGreen Guardians\nOrganization\n0.0224\n\n\nMrs. Money\nPerson\n0.0192\n\n\nSailor Shifts Team\nOrganization\n0.0186\n\n\nSeawatch\nVessel\n0.0186\n\n\nElise\nPerson\n0.0182\n\n\nSerenity\nVessel\n0.0170\n\n\nHorizon\nVessel\n0.0152\n\n\nThe Middleman\nPerson\n0.0142\n\n\nNorthern Light\nVessel\n0.0135\n\n\nRodriguez\nPerson\n0.0122\n\n\nSamantha Blake\nPerson\n0.0114\n\n\nHaacklee Harbor\nLocation\n0.0111\n\n\nOsprey\nVessel\n0.0088\n\n\nCity Officials\nGroup\n0.0066\n\n\nThe Lookout\nPerson\n0.0062\n\n\nKnowles\nVessel\n0.0051\n\n\nSmall Fry\nPerson\n0.0035\n\n\nGlitters Team\nOrganization\n0.0035\n\n\n\n\n\n\n\nShow code\nknitr::kable(betweenness_df, caption = \"Betweenness Centrality (Nadia's Ego Network)\")\n\n\n\nBetweenness Centrality (Nadia’s Ego Network)\n\n\nlabel\nsub_type\nbetweenness\n\n\n\n\nMako\nVessel\n368.50\n\n\nMrs. Money\nPerson\n167.18\n\n\nReef Guardian\nVessel\n139.69\n\n\nBoss\nPerson\n136.18\n\n\nV. Miesel Shipping\nOrganization\n118.70\n\n\nNadia Conti\nPerson\n117.87\n\n\nOceanus City Council\nOrganization\n116.11\n\n\nRemora\nVessel\n90.45\n\n\nNeptune\nVessel\n82.59\n\n\nThe Lookout\nPerson\n80.51\n\n\nHimark Harbor\nLocation\n52.61\n\n\nThe Middleman\nPerson\n50.78\n\n\nLiam Thorne\nPerson\n41.81\n\n\nHaacklee Harbor\nLocation\n41.30\n\n\nSentinel\nVessel\n34.54\n\n\nGreen Guardians\nOrganization\n27.51\n\n\nPaackland Harbor\nLocation\n27.08\n\n\nDavis\nPerson\n22.36\n\n\nEcoVigil\nVessel\n12.63\n\n\nRodriguez\nPerson\n11.75\n\n\nNorthern Light\nVessel\n9.76\n\n\nSailor Shifts Team\nOrganization\n7.34\n\n\nHorizon\nVessel\n6.72\n\n\nMarlin\nVessel\n6.23\n\n\nSeawatch\nVessel\n5.20\n\n\nElise\nPerson\n4.60\n\n\nSamantha Blake\nPerson\n4.49\n\n\nSerenity\nVessel\n0.81\n\n\nKnowles\nVessel\n0.50\n\n\nSmall Fry\nPerson\n0.00\n\n\nGlitters Team\nOrganization\n0.00\n\n\nOsprey\nVessel\n0.00\n\n\nCity Officials\nGroup\n0.00\n\n\n\n\n\n\n\nShow code\nknitr::kable(degree_df, caption = \"Degree Centrality (Nadia's Ego Network)\")\n\n\n\nDegree Centrality (Nadia’s Ego Network)\n\n\nlabel\nsub_type\ndegree\n\n\n\n\nMako\nVessel\n37\n\n\nOceanus City Council\nOrganization\n28\n\n\nReef Guardian\nVessel\n27\n\n\nRemora\nVessel\n21\n\n\nV. Miesel Shipping\nOrganization\n19\n\n\nNeptune\nVessel\n19\n\n\nNadia Conti\nPerson\n17\n\n\nGreen Guardians\nOrganization\n17\n\n\nHimark Harbor\nLocation\n17\n\n\nDavis\nPerson\n16\n\n\nSentinel\nVessel\n16\n\n\nBoss\nPerson\n13\n\n\nEcoVigil\nVessel\n13\n\n\nPaackland Harbor\nLocation\n13\n\n\nMrs. Money\nPerson\n12\n\n\nHorizon\nVessel\n12\n\n\nLiam Thorne\nPerson\n11\n\n\nRodriguez\nPerson\n10\n\n\nMarlin\nVessel\n10\n\n\nSeawatch\nVessel\n9\n\n\nThe Middleman\nPerson\n8\n\n\nSerenity\nVessel\n8\n\n\nNorthern Light\nVessel\n8\n\n\nHaacklee Harbor\nLocation\n8\n\n\nElise\nPerson\n7\n\n\nThe Lookout\nPerson\n7\n\n\nSailor Shifts Team\nOrganization\n7\n\n\nSamantha Blake\nPerson\n6\n\n\nGlitters Team\nOrganization\n4\n\n\nKnowles\nVessel\n4\n\n\nSmall Fry\nPerson\n3\n\n\nOsprey\nVessel\n3\n\n\nCity Officials\nGroup\n1",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#centrality-metrics-and-direct-indirect-influences",
    "href": "testing_qmd/main.html#centrality-metrics-and-direct-indirect-influences",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Centrality Metrics and Direct & Indirect Influences",
    "text": "Centrality Metrics and Direct & Indirect Influences\nBy calculating centrality metrics within Nadia’s two-hop ego network, we observe that the most influential nodes in her environment—by PageRank, betweenness, and degree—are Neptune, V. Miesel Shipping, and Elise. Nadia herself consistently ranks among the top nodes by these measures, highlighting her role as both an influencer and an information bridge. Entities such as Neptune and V. Miesel Shipping, which also score highly in centrality, exert considerable influence over Nadia’s information flow and access to other parts of the network.\nDegree centrality analysis shows Nadia maintains multiple direct connections, particularly with other highly active nodes, ensuring she is closely linked to key hubs in the network. Betweenness centrality further reveals that Nadia is not only well-connected but also acts as an important intermediary, facilitating communication between otherwise distant parts of the network. PageRank confirms that her immediate environment is composed of actors with significant structural power, increasing the likelihood that Nadia is both influenced by, and exerts influence upon, the most pivotal players in Oceanus.\n\n3.5.1 - PageRank for Nadia Conti\n\n\nShow code\nlibrary(igraph)\nlibrary(visNetwork)\n\n# -- Build the global network g as in your earlier code (using your entity_nodes/entity_edges) --\n\ng &lt;- graph_from_data_frame(\n  d = entity_edges, \n  vertices = entity_nodes, \n  directed = TRUE\n)\n\n# -- Get Nadia's index in g --\nnadia_label &lt;- \"Nadia Conti\"\ntarget_index &lt;- which(V(g)$label == nadia_label)\n\n# -- Extract Nadia's 1-hop ego network (all direct neighbors) --\nego_1 &lt;- make_ego_graph(g, order = 1, nodes = target_index, mode = \"all\")[[1]]\n\n\n# 1. Compute PageRank for the ego network\nV(ego_1)$pagerank &lt;- page_rank(ego_1)$vector\n\n# 2. Prepare node data frame with your consistent color scheme\nnodes_df_pagerank &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;PageRank: \", round(V(ego_1)$pagerank, 4)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$pagerank * 30 + 5\n)\n\n# 3. Prepare edges\nedges_df &lt;- as_data_frame(ego_1, what = \"edges\") %&gt;%\n  rename(from = from, to = to)\n\n# 4. Plot with visNetwork\nvisNetwork(nodes_df_pagerank, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_pagerank$color, border = \"black\"),\n    shape = nodes_df_pagerank$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n3.5.2 - Betweenness for Nadia Conti\n\n\nShow code\n# 1. Compute Betweenness for the ego network\nV(ego_1)$betweenness &lt;- betweenness(ego_1, directed = TRUE)\n\n# 2. Prepare node data frame\nnodes_df_betweenness &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Betweenness: \", round(V(ego_1)$betweenness, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$betweenness * 2 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_betweenness, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_betweenness$color, border = \"black\"),\n    shape = nodes_df_betweenness$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n3.5.3 - Degree for Nadia Conti\n\n\nShow code\n# 1. Compute Degree for the ego network\nV(ego_1)$degree &lt;- degree(ego_1, mode = \"all\")\n\n# 2. Prepare node data frame\nnodes_df_degree &lt;- data.frame(\n  id = V(ego_1)$name,\n  label = V(ego_1)$label,\n  group = V(ego_1)$sub_type,\n  title = paste0(\"&lt;b&gt;\", V(ego_1)$label, \"&lt;/b&gt;&lt;br&gt;Degree: \", round(V(ego_1)$degree, 2)),\n  shape = ifelse(V(ego_1)$sub_type == \"Person\", \"dot\",\n                 ifelse(V(ego_1)$sub_type == \"Organization\", \"square\",\n                        ifelse(V(ego_1)$sub_type == \"Vessel\", \"triangle\",\n                               ifelse(V(ego_1)$sub_type == \"Group\", \"star\", \"diamond\")))),\n  color = case_when(\n    V(ego_1)$sub_type == \"Person\" ~ \"#2ca5ff\",\n    V(ego_1)$sub_type == \"Organization\" ~ \"#f5ee15\",\n    V(ego_1)$sub_type == \"Vessel\" ~ \"#FB7E81\",\n    V(ego_1)$sub_type == \"Group\" ~ \"#25e158\",\n    V(ego_1)$sub_type == \"Location\" ~ \"#ec4bff\",\n    TRUE ~ \"black\"\n  ),\n  value = V(ego_1)$degree * 5 + 5\n)\n\n# 3. Edges (same as before)\n# edges_df already prepared\n\n# 4. Plot\nvisNetwork(nodes_df_degree, edges_df, width = \"100%\", height = \"400px\") %&gt;%\n  visNodes(\n    scaling = list(min = 5, max = 30),\n    color = list(background = nodes_df_degree$color, border = \"black\"),\n    shape = nodes_df_degree$shape\n  ) %&gt;%\n  visEdges(arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)), color = list(color = \"gray\")) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, manipulation = FALSE) %&gt;%\n  visInteraction(dragNodes = FALSE, dragView = FALSE, zoomView = FALSE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\n\n\n\n\n1c. With a focus on “Nadia Conti”, the visuals above could determine who has influence over this person.\n\n\n\n\nDegree centrality reveals that Nadia Conti is well-connected within her local network, with a degree of 17. However, she is not the most connected node; vessels such as Mako (37), Reef Guardian (27), and Remora (21), as well as organizations like Oceanus City Council (28) and V. Miesel Shipping (19), have even higher degrees. This indicates that while Nadia is an important hub, her sphere of direct interaction is embedded within a dense mesh of other highly connected entities.\nSeveral other individuals (e.g., Davis with 16, Boss with 13, Mrs. Money with 12) and vessels (e.g., Neptune with 19, Sentinel with 16) also play significant roles in Nadia’s network. The presence of organizations (e.g., Green Guardians, Sailor Shifts Team), multiple vessels, and key persons shows that Nadia’s environment is both diverse and robust.\nDirect Connections\nThese direct connections are clearly shown as nodes that have edges (arrows) going into or out of Nadia Conti’s node in the network diagrams. Nadia Conti directly connects to several core entities across different types:\n\nPeople: Elise, Liam Thorne, Davis, Rodriguez\nOrganization: V. Miesel Shipping, Oceanus City Council, Sailor Shifts Team\nVessel: Neptune, Marlin, Remora, Sentinel\nLocation: Haacklee Harbor\n\nInterpretation: The PageRank, Betweenness, and Degree centrality plots all consistently show Nadia Conti as a major hub, with a large node size reflecting her high centrality. Her immediate network includes influential vessels (Neptune, Remora), organizations (V. Miesel Shipping, Oceanus City Council), and several persons (Elise, Davis, Rodriguez).\nNadia’s position suggests she is a key connector and influencer but is herself surrounded by even larger hubs, particularly among vessels and organizations. Her ability to influence—and be influenced—is amplified by these connections, as these high-degree entities are likely sources and conduits of critical information and operational coordination. This structure points to a tightly interwoven community, where central actors such as Mako, Oceanus City Council, and V. Miesel Shipping may exert the most substantial influence over Nadia’s access to information, resources, and strategic decisions.",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#strategy-for-question-2",
    "href": "testing_qmd/main.html#strategy-for-question-2",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Strategy for Question 2:",
    "text": "Strategy for Question 2:\n\n1. Filter for Communication Events Only\n\nUse edges that connect Entity → Event (Communication) and Event (Communication) → Entity.\nFocus only on Communication events and extract senders and receivers.\n\n\n\nShow code\n# Extract Communication Events from nodes\ncommunication_events &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(id, label)\n\n# Extract sent edges: Entity → Communication Event\ncomm_sent_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"sent\", to_id %in% communication_events$id)\n\n# Extract received edges: Communication Event → Entity\ncomm_received_edges &lt;- mc3_edges_cleaned %&gt;%\n  filter(type == \"received\", from_id %in% communication_events$id)\n\n# Join both to get Sender → Communication → Receiver\ncomm_links &lt;- comm_sent_edges %&gt;%\n  select(comm_id = to_id, sender = from_id) %&gt;%\n  inner_join(\n    comm_received_edges %&gt;% select(comm_id = from_id, receiver = to_id),\n    by = \"comm_id\"\n  ) %&gt;%\n  filter(sender != receiver)\n\n\n\n\n2. Construct a Bipartite Graph of Communications\n\nFrom edges:\n\nSender (Entity) → Communication Event\nCommunication Event → Receiver (Entity)\n\nJoin both directions to link:\nEntity A → Communication Event → Entity B → derive Entity A → Entity B communication links.\n\n\n\nShow code\n# Get people and vessel node IDs\npeople_vessels &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Vessel\")) %&gt;%\n  select(id, label, group = sub_type)\n\n# Filter comm links to include only person ↔ vessel or person ↔ person, etc.\ncomm_links_filtered &lt;- comm_links %&gt;%\n  filter(sender %in% people_vessels$id, receiver %in% people_vessels$id)\n\n\n\n\n3. Build Communication Network\n\nNodes: People and Vessels only (from mc3_nodes_cleaned).\nEdges: Summarized links between these nodes based on co-involvement in the same communication event.\n\n\n\nShow code\n# Edge weight (number of communications)\nedge_df &lt;- comm_links_filtered %&gt;%\n  count(sender, receiver, name = \"weight\")\n\n# Create node list for graph\nnodes_df &lt;- people_vessels %&gt;%\n  filter(id %in% c(edge_df$sender, edge_df$receiver))\n\n# Build graph object\ncomm_graph &lt;- tbl_graph(nodes = nodes_df, edges = edge_df, directed = FALSE)\n\n\n\n\n4. Apply Community Detection (e.g., Louvain or Walktrap)\n\nUse igraph or tidygraph to detect communities.\nAnnotate communities for possible labels (e.g., Green Guardians, Sailor Shift fans) using node metadata.\n\n\n\nShow code\ncomm_graph &lt;- comm_graph %&gt;%\n  mutate(community = as.factor(group_louvain()))\n\n\n\n\n5. Visualize Network\n\n\nShow code\nshape_map &lt;- c(\"Person\" = \"circle\", \"Vessel\" = \"triangle\")\n\ncolor_map &lt;- c(\n  \"Person\" = \"#fc8d62\",\n  \"Organization\" = \"#6baed6\",\n  \"Vessel\" = \"#66c2a2\",\n  \"Location\" = \"#c6dbef\",\n  \"Nadia Conti\" = \"#ffd92f\"\n)\n\nggraph(comm_graph, layout = \"fr\") +\n  geom_edge_link(aes(width = weight), alpha = 0.2, color = \"gray50\") +\n  geom_node_point(aes(color = group, shape = group), size = 4) +\n  geom_node_text(aes(label = label), repel = TRUE, size = 2.5) +\n  scale_shape_manual(values = shape_map) +\n  scale_color_manual(values = color_map) +\n  theme_graph() +\n  labs(title = \"Communication Clusters Between People and Vessels\",\n       subtitle = \"Communities detected using Louvain algorithm\")\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Get only Person and Vessel nodes\npeople_vessels &lt;- mc3_nodes_cleaned %&gt;%\n  filter(sub_type %in% c(\"Person\", \"Vessel\")) %&gt;%\n  select(id, label, group = sub_type)\n\n# Filter communication links for person ↔ vessel/person only\ncomm_links_filtered &lt;- comm_links %&gt;%\n  filter(sender %in% people_vessels$id, receiver %in% people_vessels$id)\n\n\n\n\nShow code\n# Count number of communications between each sender–receiver pair\ncomm_edge_df &lt;- comm_links_filtered %&gt;%\n  count(sender, receiver, name = \"weight\")\n\n# Build node dataframe from involved IDs only\ncomm_node_df &lt;- people_vessels %&gt;%\n  filter(id %in% unique(c(comm_edge_df$sender, comm_edge_df$receiver))) %&gt;%\n  mutate(\n    shape = case_when(\n      group == \"Person\" ~ \"dot\",\n      group == \"Vessel\" ~ \"triangle\"\n    ),\n    color = case_when(\n      group == \"Person\" ~ \"#fc8d62\",\n      group == \"Vessel\" ~ \"#66c2a2\",\n      label == \"Nadia Conti\" ~ \"#ffd92f\",\n      TRUE ~ \"#c6dbef\"\n    )\n  )\n\n# Format edges for visNetwork\ncomm_vis_edges &lt;- comm_edge_df %&gt;%\n  rename(from = sender, to = receiver) %&gt;%\n  mutate(width = weight)\n\n\n\n\nShow code\nlibrary(igraph)\n\n# Create igraph object\ngraph_ig &lt;- graph_from_data_frame(comm_vis_edges, directed = FALSE, vertices = comm_node_df)\n\n# Apply Louvain clustering\nlouvain_groups &lt;- cluster_louvain(graph_ig)\ncomm_node_df$group_comm &lt;- as.factor(membership(louvain_groups))\n\n\n\n\nShow code\nlibrary(visNetwork)\n\n# Title heading\ncat(\"### Interactive Network of Communication Between People and Vessels\")\n\n\n### Interactive Network of Communication Between People and Vessels\n\n\nShow code\n# Final interactive visNetwork with consistent styling\nvisNetwork(\n  nodes = comm_node_df,\n  edges = comm_vis_edges\n) %&gt;%\n  visEdges(arrows = \"to\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visLayout(randomSeed = 123) %&gt;%\n  visPhysics(\n    solver = \"forceAtlas2Based\",\n    forceAtlas2Based = list(\n      gravitationalConstant = -80,\n      centralGravity = 0.01,\n      springLength = 50,\n      springConstant = 0.02\n    ),\n    stabilization = list(enabled = TRUE, iterations = 100)\n  ) %&gt;%\n  visInteraction(navigationButtons = TRUE) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = list(\n      list(label = \"Person\", shape = \"dot\", color = \"#fc8d62\"),\n      list(label = \"Vessel\", shape = \"triangle\", color = \"#66c2a2\")\n    ),\n    width = 0.1,\n    position = \"left\",\n    stepY = 80,\n    ncol = 1\n  )\n\n\n\n\n\n\n\n\nShow code\nlibrary(scales)  # for rescale()\n\n# Step 1: Summarize sender–receiver communication volume\ncomm_edges_vis &lt;- comm_links_filtered %&gt;%\n  count(sender, receiver, sort = TRUE) %&gt;%\n  rename(from = sender, to = receiver, value = n)\n\n# Step 2: Compute messages sent per person\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare node attributes (label, shape, color, size)\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    size = if_else(\n      sub_type == \"Person\",\n      rescale(sent_count, to = c(10, 40), na.rm = TRUE),\n      15\n    ),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#fc8d62\",\n      sub_type == \"Vessel\" ~ \"#66c2a2\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Step 4: Format edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Define proper legend nodes (explicit list)\nlegend_nodes &lt;- list(\n  list(label = \"Person\", shape = \"dot\", color = \"#fc8d62\"),\n  list(label = \"Vessel\", shape = \"triangle\", color = \"#66c2a2\")\n)\n\n\n# Step 6: Render visNetwork with layout_on_sphere and custom legend\ncat(\"### Styled Communication Network (Scaled by Sent Messages)\")\n\n\n### Styled Communication Network (Scaled by Sent Messages)\n\n\nShow code\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"900px\") %&gt;%\n  visNodes(size = nodes_vis$size) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = legend_nodes,\n    width = 0.1,\n    position = \"left\",\n    stepY = 80,\n    ncol = 1\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visIgraphLayout(layout = \"layout_on_sphere\") %&gt;%\n  visPhysics(enabled = FALSE) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\nShow code\nlibrary(scales)  # for rescale()\n\n# Step 1: Summarize sender–receiver communication volume\ncomm_edges_vis &lt;- comm_links_filtered %&gt;%\n  count(sender, receiver, sort = TRUE) %&gt;%\n  rename(from = sender, to = receiver, value = n)\n\n# Step 2: Compute messages sent per person\nmessage_counts &lt;- comm_edges_vis %&gt;%\n  group_by(from) %&gt;%\n  summarise(sent_count = sum(value), .groups = \"drop\")\n\n# Step 3: Prepare node attributes (label, shape, color, size)\nnodes_vis &lt;- mc3_nodes_cleaned %&gt;%\n  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %&gt;%\n  select(id, label, sub_type) %&gt;%\n  left_join(message_counts, by = c(\"id\" = \"from\")) %&gt;%\n  mutate(\n    size = if_else(\n      sub_type == \"Person\",\n      rescale(sent_count, to = c(10, 40), na.rm = TRUE),\n      15\n    ),\n    title = paste0(label, \"&lt;br&gt;Sub-type: \", sub_type,\n                   ifelse(!is.na(sent_count), paste0(\"&lt;br&gt;Sent: \", sent_count, \" messages\"), \"\")),\n    color = case_when(\n      sub_type == \"Person\" ~ \"#fc8d62\",\n      sub_type == \"Vessel\" ~ \"#66c2a2\",\n      TRUE ~ \"black\"\n    ),\n    shape = case_when(\n      sub_type == \"Person\" ~ \"dot\",\n      sub_type == \"Vessel\" ~ \"triangle\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Step 4: Format edges\nedges_vis &lt;- comm_edges_vis %&gt;%\n  mutate(\n    arrows = \"to\",\n    width = rescale(value, to = c(1, 6)),\n    title = paste(\"Messages:\", value)\n  )\n\n# Step 5: Define proper legend nodes (explicit list)\nlegend_nodes &lt;- list(\n  list(label = \"Person\", shape = \"dot\", color = \"#fc8d62\"),\n  list(label = \"Vessel\", shape = \"triangle\", color = \"#66c2a2\")\n)\n\n\n# Step 6: Render visNetwork with layout_on_sphere and custom legend\ncat(\"### Styled Communication Network (Scaled by Sent Messages)\")\n\n\n### Styled Communication Network (Scaled by Sent Messages)\n\n\nShow code\nvisNetwork(nodes_vis, edges_vis, width = \"100%\", height = \"900px\") %&gt;%\n  visNodes(size = nodes_vis$size) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = legend_nodes,\n    width = 0.1,\n    position = \"left\",\n    stepY = 80,\n    ncol = 1\n  ) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visPhysics(\n    solver = \"forceAtlas2Based\",\n    forceAtlas2Based = list(\n      gravitationalConstant = -50,   # Increase pull toward center\n      centralGravity = 0.005,        # Lower keeps outer nodes further\n      springLength = 100,            # Length between nodes\n      springConstant = 0.02\n    ),\n    stabilization = list(enabled = TRUE, iterations = 100)\n  ) %&gt;%\n  visLayout(randomSeed = 1818)\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Step 1: Filter only Communication edges\ncomm_edges_all &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\"))\n\n# Step 2: Count messages by sender/receiver\nsent_counts &lt;- comm_edges_all %&gt;%\n  filter(type == \"sent\") %&gt;%\n  count(from_id, name = \"sent\")\n\nreceived_counts &lt;- comm_edges_all %&gt;%\n  filter(type == \"received\") %&gt;%\n  count(to_id, name = \"received\")\n\n# Step 3: Join and label\ncomm_summary &lt;- full_join(sent_counts, received_counts, by = c(\"from_id\" = \"to_id\")) %&gt;%\n  rename(id = from_id) %&gt;%\n  replace_na(list(sent = 0, received = 0)) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, label, sub_type), by = \"id\") %&gt;%\n  pivot_longer(cols = c(sent, received), names_to = \"direction\", values_to = \"count\")\n\n# Step 4: Bar plot\nggplot(comm_summary, aes(x = reorder(label, -count), y = count, fill = direction)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"sent\" = \"#2ca5ff\", \"received\" = \"#fb8072\")) +\n  labs(\n    title = \"Message Volume by Entity\",\n    x = \"Entity\",\n    y = \"Message Count\",\n    fill = \"Direction\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(ggtext)\n\n# Step 1: Filter only Communication edges\ncomm_edges_all &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\"))\n\n# Step 2: Count messages sent and received\nsent_counts &lt;- comm_edges_all %&gt;%\n  filter(type == \"sent\") %&gt;%\n  count(from_id, name = \"sent\")\n\nreceived_counts &lt;- comm_edges_all %&gt;%\n  filter(type == \"received\") %&gt;%\n  count(to_id, name = \"received\")\n\n# Step 3: Join and format\ncomm_summary &lt;- full_join(sent_counts, received_counts, by = c(\"from_id\" = \"to_id\")) %&gt;%\n  rename(id = from_id) %&gt;%\n  replace_na(list(sent = 0, received = 0)) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, label, sub_type), by = \"id\") %&gt;%\n  pivot_longer(cols = c(sent, received), names_to = \"direction\", values_to = \"count\")\n\n# Step 4: Create colored labels for x-axis\ncomm_summary &lt;- comm_summary %&gt;%\n  mutate(\n    x_label = paste0(\n      \"&lt;span style='color:\",\n      case_when(\n        sub_type == \"Vessel\" ~ \"#66c2a2\",\n        sub_type == \"Person\" ~ \"#fc8d62\",\n        TRUE ~ \"gray\"\n      ),\n      \"'&gt;\", label, \"&lt;/span&gt;\"\n    )\n  )\n\n# Step 5: Bar Plot with colored axis text\nggplot(comm_summary, aes(x = reorder(x_label, -count), y = count, fill = direction)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"sent\" = \"#2ca5ff\", \"received\" = \"#fb8072\")) +\n  labs(\n    title = \"Message Volume by Entity\",\n    x = \"Entity\",\n    y = \"Message Count\",\n    fill = \"Direction\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_markdown(angle = 45, hjust = 1),\n    plot.title = element_text(size = 14, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(plotly)\nlibrary(DT)\n\n# Step 1: Compute message counts\ncomm_edges_all &lt;- mc3_edges_cleaned %&gt;%\n  filter(type %in% c(\"sent\", \"received\"))\n\nsent_counts &lt;- comm_edges_all %&gt;%\n  filter(type == \"sent\") %&gt;%\n  count(from_id, name = \"sent\")\n\nreceived_counts &lt;- comm_edges_all %&gt;%\n  filter(type == \"received\") %&gt;%\n  count(to_id, name = \"received\")\n\n# Step 2: Combine counts\ncomm_summary &lt;- full_join(sent_counts, received_counts, by = c(\"from_id\" = \"to_id\")) %&gt;%\n  rename(id = from_id) %&gt;%\n  replace_na(list(sent = 0, received = 0)) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, label, sub_type), by = \"id\")\n\n# Step 3: Reshape for plotly\ncomm_long &lt;- comm_summary %&gt;%\n  pivot_longer(cols = c(sent, received), names_to = \"direction\", values_to = \"count\")\n\n# Step 4: Plotly bar chart (interactive)\nplot_ly(\n  comm_long,\n  x = ~label,\n  y = ~count,\n  color = ~direction,\n  colors = c(\"sent\" = \"#2ca5ff\", \"received\" = \"#fb8072\"),\n  type = 'bar',\n  text = ~paste0(\"Entity: \", label, \"&lt;br&gt;Type: \", sub_type, \"&lt;br&gt;Count: \", count),\n  hoverinfo = 'text',\n  name = ~direction\n) %&gt;%\n  layout(\n    title = \"Interactive Message Volume by Entity\",\n    barmode = 'group',\n    xaxis = list(title = \"Entity\", tickangle = -45),\n    yaxis = list(title = \"Message Count\")\n  )\n\n\n\n\n\n\n\n\nShow code\ndatatable(\n  comm_summary %&gt;% arrange(desc(sent + received)),\n  options = list(\n    pageLength = 10,\n    autoWidth = TRUE,\n    searchHighlight = TRUE\n  ),\n  colnames = c(\"ID\", \"Name\", \"Sent\", \"Received\", \"Type\")\n)",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#strategy-to-tackle-q2b",
    "href": "testing_qmd/main.html#strategy-to-tackle-q2b",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Strategy to Tackle Q2b",
    "text": "Strategy to Tackle Q2b\n\nStep 1: Community Detection with Louvain\n\nUse igraph::cluster_louvain() on the communication network built in 2a (undirected).\nAssign a community ID to each node (nodes_vis$community).\n\n\n\nStep 2: Inspect and Interpret Communities\n\nSummarize the composition of each community by:\n\nNumber of persons/vessels\nTop labels in each group\nKnown keywords (e.g., “Green Guardians”, “Sailor Shift”, vessel names like “Aurora” or “Bluefin”)\n\n\n\n\nStep 3: Color-code the Network by Community\n\nAssign a distinct color to each detected community.\nRetain shape encoding (dot = Person, triangle = Vessel).\n\n\n\nStep 4: Interactive Visualization\n\nUse visNetwork to display the full communication network:\n\nColor nodes by community\nTooltip includes label, type, community\nLegend for each detected community\n\n\n\n\nShow code\nlibrary(igraph)\nlibrary(visNetwork)\nlibrary(RColorBrewer)\nlibrary(dplyr)\nlibrary(tibble)\n\n# Create igraph from person-vessel edges\ng_comm &lt;- graph_from_data_frame(edges_vis, directed = FALSE, vertices = nodes_vis)\n\n# Louvain detection\nlouvain_clusters &lt;- cluster_louvain(g_comm)\nnodes_vis$louvain_comm &lt;- as.factor(membership(louvain_clusters))\n\n# Walktrap detection\nwalktrap_clusters &lt;- cluster_walktrap(g_comm)\nnodes_vis$walktrap_comm &lt;- as.factor(membership(walktrap_clusters))\n\n# Create color palettes\nmax_comm &lt;- max(as.numeric(nodes_vis$louvain_comm), as.numeric(nodes_vis$walktrap_comm))\ncomm_colors &lt;- brewer.pal(n = min(max_comm, 8), name = \"Set2\")\n\n# Assign community color for each method\nnodes_louvain &lt;- nodes_vis %&gt;%\n  mutate(\n    color = comm_colors[as.numeric(louvain_comm)],\n    title = paste0(label, \"&lt;br&gt;Type: \", sub_type, \"&lt;br&gt;Louvain: \", louvain_comm)\n  )\n\nnodes_walktrap &lt;- nodes_vis %&gt;%\n  mutate(\n    color = comm_colors[as.numeric(walktrap_comm)],\n    title = paste0(label, \"&lt;br&gt;Type: \", sub_type, \"&lt;br&gt;Walktrap: \", walktrap_comm)\n  )\n\n\n\n\nShow code\n# Define consistent edge formatting\nedges_format &lt;- edges_vis %&gt;%\n  mutate(arrows = \"to\", width = width)\n\n# Louvain network\nlouvain_net &lt;- visNetwork(nodes_louvain, edges_format, height = \"700px\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visPhysics(stabilization = TRUE) %&gt;%\n  visLayout(randomSeed = 42) %&gt;%\n  visNodes(shape = nodes_louvain$shape, size = nodes_louvain$size) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visLegend(main = list(text = \"Louvain Communities\"), useGroups = FALSE)\n\n# Walktrap network\nwalktrap_net &lt;- visNetwork(nodes_walktrap, edges_format, height = \"700px\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visPhysics(stabilization = TRUE) %&gt;%\n  visLayout(randomSeed = 42) %&gt;%\n  visNodes(shape = nodes_walktrap$shape, size = nodes_walktrap$size) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visLegend(main = list(text = \"Walktrap Communities\"), useGroups = FALSE)\n\n\n📌 Louvain Community Network\n\n\nShow code\n# Generate cluster legend for Louvain\nlouvain_legend &lt;- unique(nodes_louvain$louvain_comm) %&gt;%\n  sort() %&gt;%\n  purrr::map(function(comm_id) {\n    list(\n      label = paste(\"Cluster\", comm_id),\n      shape = \"dot\",\n      color = unique(nodes_louvain$color[nodes_louvain$louvain_comm == comm_id])[1]\n    )\n  })\n\n# Render Louvain network\ncat(\"## Louvain Community Detection Network\")\n\n\n## Louvain Community Detection Network\n\n\nShow code\nvisNetwork(nodes_louvain, edges_format, width = \"100%\", height = \"750px\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visPhysics(\n    solver = \"forceAtlas2Based\",\n    forceAtlas2Based = list(\n      gravitationalConstant = -30,\n      centralGravity = 0.001,\n      springLength = 150,\n      springConstant = 0.03\n    ),\n    stabilization = list(enabled = TRUE, iterations = 200)\n  ) %&gt;%\n  visLayout(randomSeed = 42, improvedLayout = TRUE) %&gt;%\n  visNodes(shape = nodes_louvain$shape, size = nodes_louvain$size) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = louvain_legend,\n    position = \"left\",\n    width = 0.075,\n    stepY = 70,\n    ncol = 1\n  ) %&gt;%\n  visInteraction(\n    dragNodes = TRUE,\n    navigationButtons = TRUE\n  )\n\n\n\n\n\n\n📌 Walktrap Community Network\n\n\nShow code\n# Generate cluster legend for Walktrap\nwalktrap_legend &lt;- unique(nodes_walktrap$walktrap_comm) %&gt;%\n  sort() %&gt;%\n  purrr::map(function(comm_id) {\n    list(\n      label = paste(\"Cluster\", comm_id),\n      shape = \"dot\",\n      color = unique(nodes_walktrap$color[nodes_walktrap$walktrap_comm == comm_id])[1]\n    )\n  })\n\n# Render Walktrap network\ncat(\"## Walktrap Community Detection Network\")\n\n\n## Walktrap Community Detection Network\n\n\nShow code\nvisNetwork(nodes_walktrap, edges_format, width = \"100%\", height = \"750px\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visPhysics(\n    solver = \"forceAtlas2Based\",\n    forceAtlas2Based = list(\n      gravitationalConstant = -30,\n      centralGravity = 0.001,\n      springLength = 150,\n      springConstant = 0.03\n    ),\n    stabilization = list(enabled = TRUE, iterations = 200)\n  ) %&gt;%\n  visLayout(randomSeed = 42, improvedLayout = TRUE) %&gt;%\n  visNodes(shape = nodes_walktrap$shape, size = nodes_walktrap$size) %&gt;%\n  visEdges(smooth = FALSE) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = walktrap_legend,\n    position = \"left\",\n    width = 0.075,\n    stepY = 70,\n    ncol = 1\n  ) %&gt;%\n  visInteraction(\n    dragNodes = TRUE,\n    navigationButtons = TRUE\n  )",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#vast-challenge-task-question-3a",
    "href": "testing_qmd/main.html#vast-challenge-task-question-3a",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "VAST Challenge Task & Question 3a",
    "text": "VAST Challenge Task & Question 3a\n\nExpanding upon your prior visual analytics, determine who is using pseudonyms to communicate, and what these pseudonyms are.\n\nSome that Clepper has already identified include: “Boss”, and “The Lookout”, but there appear to be many more.\nTo complicate the matter, pseudonyms may be used by multiple people or vessels.\n\n\n\n1. Cleaning the dataset\nThe code below is to help us to further clean the data first before we can start to answer question 3\n\n\nShow code\n# Step 1: Define pseudonyms\npseudonym_keywords &lt;- c(\"Boss\", \"The Lookout\", \"The Intern\", \"Mrs. Money\", \n                        \"The Accountant\", \"The Middleman\", \"Small Fry\")\n\n# Step 2: Filter pseudonym nodes (from mc3_nodes_final)\npseudonym_nodes &lt;- mc3_nodes_final %&gt;%\n  filter(\n    sub_type == \"Person\",\n    str_detect(name, regex(paste(pseudonym_keywords, collapse = \"|\"), ignore_case = TRUE))\n  )\n\n# Step 3: Get all edge rows where from/to match pseudonym node indices\npseudonym_node_indices &lt;- pseudonym_nodes$new_index\n\npseudonym_edges_final &lt;- mc3_edges_final %&gt;%\n  filter(from %in% pseudonym_node_indices | to %in% pseudonym_node_indices)\n\n# Step 4: Get only nodes that are involved in these edges\nused_node_indices &lt;- unique(c(pseudonym_edges_final$from, pseudonym_edges_final$to))\n\npseudonym_nodes_final &lt;- mc3_nodes_final %&gt;%\n  filter(new_index %in% used_node_indices) %&gt;%\n  mutate(label_type = ifelse(new_index %in% pseudonym_node_indices, \"Pseudonym\", \"Regular\"))\n\n# Step 5: Reindex nodes to match edge structure (0-based problem fix)\npseudonym_nodes_final &lt;- pseudonym_nodes_final %&gt;%\n  mutate(temp_index = row_number())\n\n# Mapping old new_index to new temp_index (for tbl_graph alignment)\nindex_map &lt;- pseudonym_nodes_final %&gt;%\n  select(old = new_index, new = temp_index)\n\n# Update edges to new 1-based index\npseudonym_edges_final &lt;- pseudonym_edges_final %&gt;%\n  left_join(index_map, by = c(\"from\" = \"old\")) %&gt;%\n  rename(from_new = new) %&gt;%\n  left_join(index_map, by = c(\"to\" = \"old\")) %&gt;%\n  rename(to_new = new) %&gt;%\n  filter(!is.na(from_new), !is.na(to_new)) %&gt;%\n  select(from = from_new, to = to_new, type)\n\n# Step 6: Build graph\npseudonym_graph &lt;- tbl_graph(\n  nodes = pseudonym_nodes_final,\n  edges = pseudonym_edges_final,\n  directed = TRUE\n)\n\n\nBefore we start to answer the questions, let us first test out if the data cleaning is effective, which should be if not you wil not be able to see this!\n\nTest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe visualisations below shows the entities labelled based on their real names or pseudinyms which are labelled differently using color codes so as for easier visualisation.\n\nMethod 1Method 2\n\n\n\n\nShow code\n# Count how many connections each pseudonym has\npseudonym_links &lt;- pseudonym_edges_final %&gt;%\n  left_join(pseudonym_nodes_final, by = c(\"from\" = \"temp_index\")) %&gt;%\n  rename(pseudonym = name) %&gt;%\n  filter(!is.na(pseudonym)) %&gt;%   # ✅ Only valid pseudonym nodes\n  group_by(pseudonym) %&gt;%\n  summarise(connection_count = n()) %&gt;%\n  arrange(desc(connection_count))\n\n\n# Plot it\nggplot(pseudonym_links, aes(x = reorder(pseudonym, connection_count), y = connection_count)) +\n  geom_col(fill = \"tomato\") +\n  coord_flip() +\n  labs(\n    title = \"Communication Frequency by Pseudonym\",\n    x = \"Pseudonym Name\",\n    y = \"Number of Connections\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Prepare node dataframe\nnodes_vis &lt;- pseudonym_nodes_final %&gt;%\n  transmute(\n    id = temp_index,\n    label = name,\n    group = ifelse(label_type == \"Pseudonym\", \"Pseudonym\", \"Regular\"),\n    title = paste(\"Name:\", name, \"&lt;br&gt;Type:\", label_type)\n  )\n\n# Prepare edge dataframe\nedges_vis &lt;- pseudonym_edges_final %&gt;%\n  transmute(\n    from = from,\n    to = to,\n    label = type,\n    arrows = \"to\"\n  )\n\n# Create visNetwork\nvisNetwork(nodes_vis, edges_vis, height = \"600px\", width = \"100%\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visGroups(groupname = \"Pseudonym\", color = \"tomato\") %&gt;%\n  visGroups(groupname = \"Regular\", color = \"steelblue\") %&gt;%\n  visLegend(addNodes = list(\n    list(label = \"Pseudonym\", shape = \"dot\", color = \"tomato\"),\n    list(label = \"Regular\", shape = \"dot\", color = \"steelblue\")\n  )) %&gt;%\n  visLayout(randomSeed = 42) %&gt;%\n  visPhysics(stabilization = TRUE)\n\n\n\n\n\n\n\n\n\nAs we can see, there are 2 methods that we can use to visualise this case. The aim of this visualisation is to help clepper to visually identufy which nodes are pseudonuyms, and how are they connected to the real identity. Suspicious names or aliases will appear isolated\n\nFrom this visualisation, we can easily determine which names are Pseudonyms. These names can be easily identified via the color codes\nWe can easily trace who talks to and/or through aliases\nThis visualisation makes it easier for Clepper to spot suspicious names\n\n\n\n2. Question 3b\n\nDescribe how your visualizations make it easier for Clepper to identify common entities in the knowledge graph.\n\n\nCodeVisualisation output\n\n\n\n\nShow code\n# Q3b: Extract edges involving those pseudonyms\n# Build pseudonym network using tidygraph\npseudonym_graph_tbl &lt;- tbl_graph(\n  nodes = pseudonym_nodes_final,\n  edges = pseudonym_edges_final,\n  directed = TRUE\n) %&gt;%\n  mutate(degree_centrality = centrality_degree(mode = \"all\"))  # centrality values added to nodes\n\n# Turn into tibble for ggplot\ntop_central &lt;- pseudonym_graph_tbl %&gt;%\n  as_tibble() %&gt;%\n  filter(label_type == \"Pseudonym\") %&gt;%\n  arrange(desc(degree_centrality)) %&gt;%\n  slice_head(n = 10)\n\n# Plot\nggplot(top_central, aes(x = reorder(name, degree_centrality), y = degree_centrality)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Central Pseudonym Entities\",\n    x = \"Pseudonym Name\",\n    y = \"Degree Centrality\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Q3b: Extract edges involving those pseudonyms\n# Build pseudonym network using tidygraph\npseudonym_graph_tbl &lt;- tbl_graph(\n  nodes = pseudonym_nodes_final,\n  edges = pseudonym_edges_final,\n  directed = TRUE\n) %&gt;%\n  mutate(degree_centrality = centrality_degree(mode = \"all\"))  # centrality values added to nodes\n\n# Turn into tibble for ggplot\ntop_central &lt;- pseudonym_graph_tbl %&gt;%\n  as_tibble() %&gt;%\n  filter(label_type == \"Pseudonym\") %&gt;%\n  arrange(desc(degree_centrality)) %&gt;%\n  slice_head(n = 10)\n\n# Plot\nggplot(top_central, aes(x = reorder(name, degree_centrality), y = degree_centrality)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Central Pseudonym Entities\",\n    x = \"Pseudonym Name\",\n    y = \"Degree Centrality\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nThe visualisation shows a bar chart of degree centrality that shows the tiop 10 most connectd pseudonyms. The aim of this graph is to heko clepper to quantify influence by measuring the cetrakity of the pseudonyms for deeper investigation. It also helps Clepper t identify who are the key players who may be controlling the flow of information\n\nThese visualisation helps Clepper to identify wich of the pseudonyms are most active\nWe can see that the nodes act as central hubs wihin the pseudonym network\nThis visualisation can help clepper to prioritize pseudionyms first as part of his investigations\n\n\n\n3. Question 3c\nCode\n\n\nShow code\nshared_pseudonyms &lt;- pseudonym_nodes_final %&gt;%\n  group_by(name) %&gt;%\n  filter(n() &gt; 1) %&gt;%\n  ungroup()\n\n# Create nodes: both entities and pseudonyms\nvis_nodes_3c &lt;- shared_pseudonyms %&gt;%\n  transmute(id = id, \n            label = id, \n            group = \"Entity\",\n            title = paste(\"Entity ID:\", id)) %&gt;%\n  bind_rows(\n    shared_pseudonyms %&gt;%\n      select(id = name) %&gt;%\n      distinct() %&gt;%\n      mutate(label = id,\n             group = \"Pseudonym\",\n             title = paste(\"Pseudonym:\", id))\n  )\n\nvis_edges_3c &lt;- shared_pseudonyms %&gt;%\n  transmute(from = id, to = name)\n\nvisNetwork(vis_nodes_3c, vis_edges_3c, height = \"600px\", width = \"100%\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visGroups(groupname = \"Entity\", color = \"steelblue\") %&gt;%\n  visGroups(groupname = \"Pseudonym\", color = \"tomato\") %&gt;%\n  visLegend(addNodes = list(\n    list(label = \"Entity\", shape = \"dot\", color = \"steelblue\"),\n    list(label = \"Pseudonym\", shape = \"dot\", color = \"tomato\")\n  )) %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisualisation Output\n\n\nShow code\nshared_pseudonyms &lt;- pseudonym_nodes_final %&gt;%\n  group_by(name) %&gt;%\n  filter(n() &gt; 1) %&gt;%\n  ungroup()\n\n# Create nodes: both entities and pseudonyms\nvis_nodes_3c &lt;- shared_pseudonyms %&gt;%\n  transmute(id = id, \n            label = id, \n            group = \"Entity\",\n            title = paste(\"Entity ID:\", id)) %&gt;%\n  bind_rows(\n    shared_pseudonyms %&gt;%\n      select(id = name) %&gt;%\n      distinct() %&gt;%\n      mutate(label = id,\n             group = \"Pseudonym\",\n             title = paste(\"Pseudonym:\", id))\n  )\n\nvis_edges_3c &lt;- shared_pseudonyms %&gt;%\n  transmute(from = id, to = name)\n\nvisNetwork(vis_nodes_3c, vis_edges_3c, height = \"600px\", width = \"100%\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visGroups(groupname = \"Entity\", color = \"steelblue\") %&gt;%\n  visGroups(groupname = \"Pseudonym\", color = \"tomato\") %&gt;%\n  visLegend(addNodes = list(\n    list(label = \"Entity\", shape = \"dot\", color = \"steelblue\"),\n    list(label = \"Pseudonym\", shape = \"dot\", color = \"tomato\")\n  )) %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nThe visualisation used for this part of the question is an interactive graph using the visNetwork entity which shows the edges and nodes. The blue nodes indicates the entities (may be people or vessels), red nodes which is the pseudonym names and edges which indicates which entity uses what pseudonym. The aim of this visualisation is to expose the reusing of an alias whereby the same pseudonym is tied and connected to multiple entities\n\nThis visualisation helps Clepper to easily identify which pseudonyms are reused by multiple entities\nThis breaks the assumed connection between identity and name revealing many one-to-one mapping\nThis therefore can help Clepper to detect deception strategies such as multiple people pretending to have one single alias, hence minimising the risks of impersonation.",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#vast-challenge-task-question-4a",
    "href": "testing_qmd/main.html#vast-challenge-task-question-4a",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "VAST Challenge Task & Question 4a",
    "text": "VAST Challenge Task & Question 4a\n\nClepper suspects that Nadia Conti, who was formerly entangled in an illegal fishing scheme, may have continued illicit activity within Oceanus.\n\nThrough visual analytics, provide evidence that Nadia is, or is not, doing something illegal.\n\n\n\n1. Extracting Nadia’s data\n\n\nShow code\nnodes &lt;- MC3$nodes\nedges &lt;- MC3$edges\n\n# Extract communication events\ncomms &lt;- nodes %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(id, content)\n\n# Link to sender & receiver\nsent_edges &lt;- edges %&gt;% filter(type == \"sent\") %&gt;%\n  select(source = source, comm_id = target)\n\nrecv_edges &lt;- edges %&gt;% filter(type == \"received\") %&gt;%\n  select(comm_id = source, target = target)\n\n# Merge\ncomms_data &lt;- comms %&gt;%\n  left_join(sent_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(sender = source) %&gt;%\n\n  left_join(recv_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(receiver = target)\n\n# Add sender/receiver names\nmc3_nodes_cleaned &lt;- nodes %&gt;%\n  mutate(id = as.character(id)) %&gt;%\n  filter(!is.na(id)) %&gt;%\n  distinct(id, .keep_all = TRUE)\n\ncomms_data &lt;- comms_data %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver\" = \"id\"))\n\n# Count Nadia's messages\nnadia_counts &lt;- comms_data %&gt;%\n  summarise(\n    Sent = sum(sender_label == \"Nadia Conti\", na.rm = TRUE),\n    Received = sum(receiver_label == \"Nadia Conti\", na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Type\", values_to = \"Count\") %&gt;%\n  mutate(\n    Percent = Count / sum(Count),\n    Label = paste0(round(Percent * 100), \"%\\n(\", Count, \" msgs)\")\n  )\n\n\n\n\n2. Message count of Nadia\n\n\nShow code\nggplot(nadia_counts, aes(x = Count, y = reorder(Type, Count), fill = Type)) +\n  geom_col(color = \"white\") +\n  geom_text(aes(label = paste0(Count, \" msgs (\", round(Percent * 100), \"%)\")),\n            hjust = -0.1, size = 4) +\n  scale_fill_manual(values = c(\"Sent\" = \"deepskyblue3\", \"Received\" = \"cyan\")) +\n  labs(title = paste0(\"Nadia Conti's Messages (Total: \", sum(nadia_counts$Count), \")\"),\n       x = \"Message Count\", y = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\")) +\n  xlim(0, max(nadia_counts$Count) * 1.2)\n\n\n\n\n\n\n\n\n\n\n\n3. Message frequency of Nadia\n\n\nShow code\n# Make sure nadia_data is created\nnadia_data &lt;- comms_data %&gt;%\n  filter(sender_label == \"Nadia Conti\" | receiver_label == \"Nadia Conti\") %&gt;%\n  left_join(nodes %&gt;% select(id, timestamp), by = c(\"id\" = \"id\")) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(!is.na(timestamp)) %&gt;%\n  mutate(date = as.Date(timestamp), hour = hour(timestamp))\n\n# Create daily_freq\ndaily_freq &lt;- nadia_data %&gt;%\n  group_by(date) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n# Create hourly_freq\nhourly_freq &lt;- nadia_data %&gt;%\n  group_by(date, hour) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n\n\n3.1 Daily\n\n\nShow code\nggplot(daily_freq, aes(x = date, y = count)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = count), vjust = -0.5, size = 3) +\n  labs(\n    title = \"Nadia Conti's Daily Message Frequency\",\n    x = \"Date\",\n    y = \"Message Count\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n3.2 Hourly\n\n\nShow code\nlibrary(plotly)\n\nplot_ly(\n  data = hourly_freq,\n  x = ~hour,\n  y = ~count,\n  color = ~as.factor(date),\n  type = 'bar',\n  text = ~paste(\"Date:\", date, \"&lt;br&gt;Hour:\", hour, \"&lt;br&gt;Messages:\", count),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    barmode = 'dodge',  # use 'stack' if you prefer stacked bars\n    title = \"Nadia Conti's Hourly Message Frequency\",\n    xaxis = list(title = \"Hour of Day\"),\n    yaxis = list(title = \"Message Count\"),\n    legend = list(title = list(text = \"Date\"))\n  )\n\n\n\n\n\n\n\n\n\n4. Nadia’s relationship pattern\n\n\nShow code\nlibrary(ggplot2)\n\n# Count relationships by type\nrelationship_counts &lt;- mc3_edges_cleaned %&gt;%\n  filter(type != \"sent\", type != \"received\") %&gt;%  # Focus on relationships, not communication\n  count(type, sort = TRUE)\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(visNetwork)\n\n# Summarise Nadia's communication edges\nnadia_edges &lt;- nadia_data %&gt;%\n  count(sender_label, receiver_label) %&gt;%\n  filter(!is.na(sender_label), !is.na(receiver_label)) %&gt;%\n  rename(from = sender_label, to = receiver_label, value = n)\n\n# Get sender + receiver entity info\n# Get type info for sender and receiver\nentity_info &lt;- bind_rows(\n  nadia_data %&gt;%\n    left_join(mc3_nodes_cleaned %&gt;% select(id, name = label, type = sub_type),\n              by = c(\"sender\" = \"id\")) %&gt;%\n    select(name, type),\n  nadia_data %&gt;%\n    left_join(mc3_nodes_cleaned %&gt;% select(id, name = label, type = sub_type),\n              by = c(\"receiver\" = \"id\")) %&gt;%\n    select(name, type)\n) %&gt;%\n  distinct()\n\n# Build node table\nnadia_nodes &lt;- tibble(name = unique(c(nadia_edges$from, nadia_edges$to))) %&gt;%\n  left_join(entity_info, by = \"name\") %&gt;%\n  mutate(\n    group = ifelse(name == \"Nadia Conti\", \"Nadia Conti\", type),\n    id = name,\n    label = name,\n    color = case_when(\n      group == \"Person\" ~ \"#fc8d62\",       \n      group == \"Organization\" ~ \"#6baed6\",\n      group == \"Vessel\" ~ \"#66c2a2\",      \n      group == \"Location\" ~ \"#c6dbef\",    \n      group == \"Nadia Conti\" ~ \"#ffd92f\", \n      TRUE ~ \"#d9d9d9\"\n    ),\n    shape = case_when(\n      group == \"Person\" ~ \"dot\",\n      group == \"Organization\" ~ \"square\",\n      group == \"Vessel\" ~ \"triangle\",\n      group == \"Location\" ~ \"diamond\",\n      group == \"Nadia Conti\" ~ \"star\",\n      TRUE ~ \"dot\"\n    )\n  )\n\n# Render network\nvisNetwork(nodes = nadia_nodes, edges = nadia_edges) %&gt;%\n  visEdges(arrows = \"to\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visLayout(randomSeed = 123) %&gt;%\n  visPhysics(\n    solver = \"forceAtlas2Based\",\n    forceAtlas2Based = list(gravitationalConstant = -25, centralGravity = 0.01, springLength = 50, springConstant = 0.02),\n    stabilization = list(enabled = TRUE, iterations = 100)\n  ) %&gt;%\n  visInteraction(navigationButtons = TRUE) %&gt;%\n  visLegend(\n    useGroups = FALSE,\n    addNodes = list(\n      list(label = \"Person\", shape = \"dot\", color = \"#fc8d62\"),\n      list(label = \"Organization\", shape = \"square\", color = \"#6baed6\"),\n      list(label = \"Vessel\", shape = \"triangle\", color = \"#66c2a2\"),\n      list(label = \"Location\", shape = \"diamond\", color = \"#c6dbef\"),\n      list(label = \"Nadia Conti\", shape = \"star\", color = \"#ffd92f\")\n    ),\n    width = 0.2,\n    position = \"left\",\n    stepY = 80,\n    ncol = 1\n  )\n\n\n\n\n\n\n\n\n5. Nadia’s most frequent commuter\n\n\nShow code\n# Get communication events linked to Nadia\nnadia_comm_ids &lt;- edges %&gt;%\n  filter(type == \"sent\" | type == \"received\") %&gt;%\n  filter(source == mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"] |\n         target == mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"]) %&gt;%\n  mutate(comm_id = ifelse(type == \"sent\", target, source)) %&gt;%\n  pull(comm_id) %&gt;%\n  unique()\n\n# Get edges related to these communications\nnadia_related_edges &lt;- edges %&gt;%\n  filter(source %in% nadia_comm_ids | target %in% nadia_comm_ids)\n\n# Get people connected (excluding comm events + Nadia herself)\nnadia_id &lt;- mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"]\n\nnadia_contacts_ids &lt;- nadia_related_edges %&gt;%\n  mutate(person_id = ifelse(source %in% nadia_comm_ids, target, source)) %&gt;%\n  filter(!person_id %in% nadia_comm_ids, person_id != nadia_id) %&gt;%\n  count(person_id, sort = TRUE)\n\n# Join with node labels\ntop_contacts_named &lt;- nadia_contacts_ids %&gt;%\n  left_join(nodes %&gt;% filter(sub_type == \"Person\") %&gt;% select(id, name = label),\n            by = c(\"person_id\" = \"id\")) %&gt;%\n  filter(!is.na(name))\n\n\n\n\nShow code\ntop_contacts_named %&gt;%\n  slice_max(n, n = 3) %&gt;%\n  ggplot(aes(x = reorder(name, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 3 Contacts Communicating with Nadia Conti\",\n    x = \"Contact Person\",\n    y = \"Number of Messages\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(DT)\n\nnadia_id &lt;- mc3_nodes_cleaned$id[mc3_nodes_cleaned$label == \"Nadia Conti\"]\n\n# Nadia's communication event IDs\nnadia_comm_ids &lt;- edges %&gt;%\n  filter(type == \"sent\" | type == \"received\") %&gt;%\n  filter(source == nadia_id | target == nadia_id) %&gt;%\n  mutate(comm_id = ifelse(type == \"sent\", target, source)) %&gt;%\n  pull(comm_id) %&gt;%\n  unique()\n\n# Top contact comm IDs\ntop_contact_comm_ids &lt;- edges %&gt;%\n  filter(\n    (source %in% nadia_comm_ids & target %in% top_contacts_named$person_id) |\n    (target %in% nadia_comm_ids & source %in% top_contacts_named$person_id)\n  ) %&gt;%\n  mutate(comm_id = ifelse(source %in% nadia_comm_ids, source, target)) %&gt;%\n  pull(comm_id) %&gt;%\n  unique()\n\n# Get comm event details\nnadia_messages &lt;- nodes %&gt;%\n  filter(id %in% top_contact_comm_ids) %&gt;%\n  filter(type == \"Event\", sub_type == \"Communication\") %&gt;%\n  select(id, timestamp, content) %&gt;%\n  left_join(edges %&gt;% filter(type == \"sent\") %&gt;% select(id = target, sender = source),\n            by = \"id\") %&gt;%\n  left_join(edges %&gt;% filter(type == \"received\") %&gt;% select(id = source, receiver = target),\n            by = \"id\") %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_name = label), by = c(\"sender\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_name = label), by = c(\"receiver\" = \"id\")) %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    sender_receiver = paste(sender_name, \"→\", receiver_name)\n  ) %&gt;%\n  arrange(timestamp) %&gt;%\n  select(timestamp, sender_receiver, content)\n\n# Display\nDT::datatable(\n  nadia_messages,\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    scrollX = TRUE,\n    initComplete = htmlwidgets::JS(\n      \"function(settings, json) {\",\n      \"$(this.api().table().header()).css({'background-color': '#f8f9fa', 'color': '#333'});\",\n      \"}\"\n    )\n  ),\n  rownames = FALSE,\n  class = 'stripe hover compact',\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-size:16px; color:#444;',\n    'Messages'\n  )\n)\n\n\n\n\n\n\n\n\n6. Temporal + suspicious event alignment\n\n6.1 Showing Nadia’s unusually active days\n\n\nShow code\n# Compute mean + SD of daily messages\ndaily_summary &lt;- daily_freq %&gt;%\n  summarise(mean_count = mean(count), sd_count = sd(count))\n\n# Flag days with unusually high message counts\nspike_days &lt;- daily_freq %&gt;%\n  filter(count &gt; daily_summary$mean_count + 2 * daily_summary$sd_count)\n\n# Show spike days\nprint(spike_days)\n\n\n# A tibble: 1 × 2\n  date       count\n  &lt;date&gt;     &lt;int&gt;\n1 2040-10-08     9\n\n\n\n\n6.2 Suspicious dates\n\n\nShow code\nsuspicious_dates &lt;- as.Date(c(\"2040-10-05\", \"2040-10-08\", \"2040-10-11\")) # example reef closure, approvals, etc.\n\n\n\n\nShow code\nspike_days %&gt;%\n  mutate(suspicious = ifelse(date %in% suspicious_dates, \"YES\", \"NO\"))\n\n\n# A tibble: 1 × 3\n  date       count suspicious\n  &lt;date&gt;     &lt;int&gt; &lt;chr&gt;     \n1 2040-10-08     9 YES       \n\n\n\n\nShow code\nlibrary(plotly)\nlibrary(dplyr)\n\n# Suppose suspicious dates (replace with real ones)\nsuspicious_dates &lt;- as.Date(c(\"2040-10-05\", \"2040-10-08\", \"2040-10-11\"))\n\n# Compute threshold\ndaily_summary &lt;- daily_freq %&gt;%\n  summarise(mean_count = mean(count), sd_count = sd(count))\n\nthreshold &lt;- daily_summary$mean_count + 2 * daily_summary$sd_count\n\n# Add status column\ndaily_freq_plot &lt;- daily_freq %&gt;%\n  mutate(\n    status = case_when(\n      date %in% suspicious_dates ~ \"Suspicious Date\",\n      count &gt; threshold ~ \"Spike\",\n      TRUE ~ \"Normal\"\n    )\n  )\n\n# Assign colors\nstatus_colors &lt;- c(\n  \"Normal\" = \"steelblue\",\n  \"Spike\" = \"red\",\n  \"Suspicious Date\" = \"orange\"\n)\n\n# Build Plotly bar chart\nplot_ly(\n  data = daily_freq_plot,\n  x = ~date,\n  y = ~count,\n  type = 'bar',\n  color = ~status,\n  colors = status_colors,\n  text = ~paste(\"Date:\", date, \"&lt;br&gt;Messages:\", count, \"&lt;br&gt;Status:\", status),\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = \"Nadia Conti's Daily Communication\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Message Count\"),\n    barmode = 'group',\n    legend = list(title = list(text = \"Status\"))\n  ) %&gt;%\n  add_lines(\n    x = ~date,\n    y = rep(threshold, nrow(daily_freq_plot)),\n    line = list(dash = 'dash', color = 'red'),\n    name = 'Spike Threshold',\n    inherit = FALSE\n  )\n\n\n\n\n\n\n\n\n\n7. Drilling down on spike + flagged date\n\n7.1 Extract Nadia’s message from Oct 8\n\n\nShow code\n# Build fresh nadia_data with content included at the start\nnadia_data &lt;- comms %&gt;%\n  left_join(sent_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(sender = source) %&gt;%\n  left_join(recv_edges, by = c(\"id\" = \"comm_id\")) %&gt;%\n  rename(receiver = target) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, sender_label = label), by = c(\"sender\" = \"id\")) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(id, receiver_label = label), by = c(\"receiver\" = \"id\")) %&gt;%\n  left_join(nodes %&gt;% select(id, timestamp), by = \"id\") %&gt;%\n  mutate(\n    timestamp = ymd_hms(timestamp),\n    date = as.Date(timestamp),\n    hour = hour(timestamp)\n  ) %&gt;%\n  filter(sender_label == \"Nadia Conti\" | receiver_label == \"Nadia Conti\") %&gt;%\n  filter(!is.na(timestamp))\n\n\n\n\nShow code\noct8_msgs &lt;- nadia_data %&gt;%\n  filter(date == as.Date(\"2040-10-08\")) %&gt;%\n  select(timestamp, sender_label, receiver_label, content) %&gt;%\n  arrange(timestamp)\n\nDT::datatable(\n  oct8_msgs,\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    scrollX = TRUE,\n    columnDefs = list(\n      list(\n        targets = 3,  # adjust if content is not 3rd col\n        render = JS(\n          \"function(data, type, row, meta) {\",\n          \"return type === 'display' && data.length &gt; 50 ?\",\n          \"'&lt;span title=\\\"' + data + '\\\"&gt;' + data.substr(0, 50) + '...&lt;/span&gt;' : data;\",\n          \"}\"\n        )\n      )\n    )\n  ),\n  rownames = FALSE,\n  class = 'stripe hover compact',\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-size:14px; color:#444;',\n    '📌 Nadia Conti Messages on Oct 8, 2040'\n  )\n)\n\n\n\n\n\n\n\n\n7.2 Keywords of Oct 8\nShowing messages on Oct 8 mentioning suspicious terms of:\n\npermit\napproval\nreef\ncargo\nshipment\nillegal\n\n\n\nShow code\n# Define suspicious keywords\nkeywords &lt;- c(\"permit\", \"approval\", \"reef\", \"cargo\", \"shipment\", \"dock\", \"illegal\")\n\n# Filter messages on Oct 8 with suspicious terms\noct8_flagged_msgs &lt;- nadia_data %&gt;%\n  filter(date == as.Date(\"2040-10-08\")) %&gt;%\n  filter(!is.na(content)) %&gt;%\n  filter(grepl(paste(keywords, collapse = \"|\"), content, ignore.case = TRUE)) %&gt;%\n  select(timestamp, sender_label, receiver_label, content) %&gt;%\n  arrange(timestamp)\n\n# Display in interactive table\nDT::datatable(\n  oct8_flagged_msgs,\n  options = list(pageLength = 5, autoWidth = TRUE),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-size:16px; color:#444;',\n    '📌 Oct 8 Messages with Suspicious Keywords'\n  )\n)\n\n\n\n\n\n\n\n\n7.3 Network of Oct 8 communication\n\n\nShow code\nlibrary(visNetwork)\n\n# Summarize comms on Oct 8\noct8_edges &lt;- nadia_data %&gt;%\n  filter(date == as.Date(\"2040-10-08\")) %&gt;%\n  count(sender_label, receiver_label) %&gt;%\n  filter(!is.na(sender_label), !is.na(receiver_label)) %&gt;%\n  rename(from = sender_label, to = receiver_label, value = n)\n\n# Build node list\noct8_nodes &lt;- tibble(name = unique(c(oct8_edges$from, oct8_edges$to))) %&gt;%\n  left_join(mc3_nodes_cleaned %&gt;% select(label, sub_type), by = c(\"name\" = \"label\")) %&gt;%\n  mutate(\n    group = ifelse(name == \"Nadia Conti\", \"Nadia Conti\", sub_type),\n    id = name,\n    label = name\n  )\n\n# Render network\nvisNetwork(oct8_nodes, oct8_edges) %&gt;%\n  visEdges(arrows = \"to\") %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visLayout(randomSeed = 456) %&gt;%\n  visPhysics(stabilization = TRUE) %&gt;%\n  visLegend()\n\n\n\n\n\n\nNadia is heavily involved in:\n\nDiscussion of Nemo Reef, permits, foundation work\nCoordinating payments, doubling fees, Harbor Master cooperation\nAdjusting schedules to avoid council suspicion\n\nHighly suspicious tone: manipulation, concealment, operational coordination beyond scope.\n\n\n\n8. Linking Oct 8 comms to permits, approvals, or vessel activity\n\n\nShow code\nsuspicious_events_alt &lt;- mc3_nodes_cleaned %&gt;%\n  filter(type == \"Event\", sub_type %in% c(\"VesselMovement\", \"Monitoring\", \"HarborReport\", \"Fishing\", \"Enforcement\")) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp)) %&gt;%\n  filter(timestamp &gt;= as.POSIXct(\"2040-10-08\"))\n\nDT::datatable(\n  suspicious_events_alt %&gt;%\n    select(type, label, sub_type, id, timestamp, monitoring_type, findings),\n  options = list(\n    pageLength = 5,\n    autoWidth = TRUE,\n    scrollX = TRUE\n  ),\n  rownames = FALSE\n)\n\n\n\n\n\n\n\n\nShow code\nlibrary(dplyr)\nlibrary(plotly)\n\n# 1️⃣ Prepare entity-related vessel/harbor events\nentity_events &lt;- suspicious_events_alt %&gt;%\n  filter(str_detect(findings, regex(\"Neptune|Miesel|Mako\", ignore_case = TRUE))) %&gt;%\n  mutate(entity = case_when(\n    str_detect(findings, regex(\"Neptune\", ignore_case = TRUE)) ~ \"Neptune\",\n    str_detect(findings, regex(\"Miesel\", ignore_case = TRUE)) ~ \"Miesel\",\n    str_detect(findings, regex(\"Mako\", ignore_case = TRUE)) ~ \"Mako\",\n    TRUE ~ \"Other\"\n  ))\n\n# 2️⃣ Build interactive plot\nplot_ly() %&gt;%\n  # Nadia comms\n  add_markers(\n    data = nadia_data,\n    x = ~timestamp,\n    y = ~\"Nadia Message\",\n    marker = list(color = \"red\", size = 10),\n    text = ~paste0(\"Nadia Message&lt;br&gt;\", timestamp),\n    hoverinfo = \"text\",\n    name = \"Nadia Message\"\n  ) %&gt;%\n  # Neptune events\n  add_markers(\n    data = entity_events %&gt;% filter(entity == \"Neptune\"),\n    x = ~timestamp,\n    y = ~entity,\n    marker = list(color = \"#1f77b4\", size = 10),\n    text = ~paste0(entity, \" Event&lt;br&gt;\", findings),\n    hoverinfo = \"text\",\n    name = \"Neptune Event\"\n  ) %&gt;%\n  # Miesel events\n  add_markers(\n    data = entity_events %&gt;% filter(entity == \"Miesel\"),\n    x = ~timestamp,\n    y = ~entity,\n    marker = list(color = \"#17becf\", size = 10),\n    text = ~paste0(entity, \" Event&lt;br&gt;\", findings),\n    hoverinfo = \"text\",\n    name = \"Miesel Event\"\n  ) %&gt;%\n  # Mako events\n  add_markers(\n    data = entity_events %&gt;% filter(entity == \"Mako\"),\n    x = ~timestamp,\n    y = ~entity,\n    marker = list(color = \"#7f7f7f\", size = 10),\n    text = ~paste0(entity, \" Event&lt;br&gt;\", findings),\n    hoverinfo = \"text\",\n    name = \"Mako Event\"\n  ) %&gt;%\n  layout(\n    title = \"Nadia Comms + Vessel/Harbor Events\",\n    xaxis = list(title = \"Time\"),\n    yaxis = list(title = \"\"),\n    legend = list(orientation = \"h\", x = 0.1, y = -0.3)\n  )\n\n\n\n\n\n\nThe interactive timeline highlights that Nadia Conti’s communications were closely followed by vessel/harbor events involving Neptune, V. Miesel Shipping, and Mako. Notably:\n•   On **Oct 8**, Nadia’s messages spiked, coinciding with planned operations at Nemo Reef.\n\n•   Shortly afterward, vessel activities linked to **Neptune, Miesel, and Mako** were logged.\n\n•   This temporal proximity strongly suggests coordination between Nadia and these entities.\nThere is no evidence of formal approvals or permits linked to these activities, pointing to potential covert operations.",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  },
  {
    "objectID": "testing_qmd/main.html#question-4b---are-cleppers-suspicions-justified",
    "href": "testing_qmd/main.html#question-4b---are-cleppers-suspicions-justified",
    "title": "VAST Challenge 2025 - Mini-Challenge 3",
    "section": "Question 4B - Are Clepper’s suspicions justified?",
    "text": "Question 4B - Are Clepper’s suspicions justified?\n\n1. Findings\n1.1 Communication Activity\n\nNadia exchanged a total of 26 messages, of which 31% were sent and 69% received.\nAn unusually high volume of messages was recorded on 2040-10-08 (9 messages), exceeding the normal daily message count and crossing the defined spike threshold.\nMessaging patterns were concentrated between 08:00 and 12:00, suggesting focused coordination during operational hours.\n\n1.2 Relationship Network\n\nNetwork visualizations indicate Nadia as a central figure in communications with key individuals (Davis, Liam Thorne, Elise) and entities (Neptune, Marlin, V. Miesel Shipping).\nThe strongest links were observed between Nadia and Davis, Neptune, and Liam Thorne, with frequent exchanges regarding sensitive operational matters.\n\n1.3 Content of Communication\n\nThematic analysis identified frequent mentions of permits, approvals, reef, foundation work, shipment, and cargo.\nSeveral messages contained concerning elements, including discussions of doubling fees for cooperation, adjustments to patrol schedules, and concealing operations from the council.\n\n1.4 Temporal and Event Alignment\n\nTimeline analysis shows that Nadia’s communication spikes closely preceded vessel activities involving Neptune, V. Miesel Shipping, and Mako.\nThis temporal alignment strongly suggests a coordinated effort linked to unauthorized operations at Nemo Reef.\n\n\n\nConclusion\nThe evidence supports Clepper’s suspicions. Nadia’s communication patterns, network centrality, message content, and the alignment with vessel activity point to her active involvement in potentially covert and unauthorized operations. There is no indication of formal approvals associated with these activities, raising further concerns about compliance and legality.",
    "crumbs": [
      "Home",
      "Testing Section",
      "For project"
    ]
  }
]